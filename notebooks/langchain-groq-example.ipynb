{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-groq in /Users/greatmaster/miniconda3/envs/oreilly-agents/lib/python3.11/site-packages (0.0.1)\n",
      "Requirement already satisfied: groq<1,>=0.4.1 in /Users/greatmaster/miniconda3/envs/oreilly-agents/lib/python3.11/site-packages (from langchain-groq) (0.4.2)\n",
      "Requirement already satisfied: langchain-core<0.2,>=0.1 in /Users/greatmaster/miniconda3/envs/oreilly-agents/lib/python3.11/site-packages (from langchain-groq) (0.1.30)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/greatmaster/miniconda3/envs/oreilly-agents/lib/python3.11/site-packages (from groq<1,>=0.4.1->langchain-groq) (3.5.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/greatmaster/miniconda3/envs/oreilly-agents/lib/python3.11/site-packages (from groq<1,>=0.4.1->langchain-groq) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/greatmaster/miniconda3/envs/oreilly-agents/lib/python3.11/site-packages (from groq<1,>=0.4.1->langchain-groq) (0.25.2)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/greatmaster/miniconda3/envs/oreilly-agents/lib/python3.11/site-packages (from groq<1,>=0.4.1->langchain-groq) (2.5.3)\n",
      "Requirement already satisfied: sniffio in /Users/greatmaster/miniconda3/envs/oreilly-agents/lib/python3.11/site-packages (from groq<1,>=0.4.1->langchain-groq) (1.2.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /Users/greatmaster/miniconda3/envs/oreilly-agents/lib/python3.11/site-packages (from groq<1,>=0.4.1->langchain-groq) (4.9.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/greatmaster/miniconda3/envs/oreilly-agents/lib/python3.11/site-packages (from langchain-core<0.2,>=0.1->langchain-groq) (6.0.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/greatmaster/miniconda3/envs/oreilly-agents/lib/python3.11/site-packages (from langchain-core<0.2,>=0.1->langchain-groq) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /Users/greatmaster/miniconda3/envs/oreilly-agents/lib/python3.11/site-packages (from langchain-core<0.2,>=0.1->langchain-groq) (0.1.23)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in /Users/greatmaster/miniconda3/envs/oreilly-agents/lib/python3.11/site-packages (from langchain-core<0.2,>=0.1->langchain-groq) (23.2)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/greatmaster/miniconda3/envs/oreilly-agents/lib/python3.11/site-packages (from langchain-core<0.2,>=0.1->langchain-groq) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /Users/greatmaster/miniconda3/envs/oreilly-agents/lib/python3.11/site-packages (from langchain-core<0.2,>=0.1->langchain-groq) (8.2.3)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/greatmaster/miniconda3/envs/oreilly-agents/lib/python3.11/site-packages (from anyio<5,>=3.5.0->groq<1,>=0.4.1->langchain-groq) (3.4)\n",
      "Requirement already satisfied: certifi in /Users/greatmaster/miniconda3/envs/oreilly-agents/lib/python3.11/site-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq) (2023.11.17)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/greatmaster/miniconda3/envs/oreilly-agents/lib/python3.11/site-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/greatmaster/miniconda3/envs/oreilly-agents/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/greatmaster/miniconda3/envs/oreilly-agents/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2,>=0.1->langchain-groq) (2.4)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /Users/greatmaster/miniconda3/envs/oreilly-agents/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.0->langchain-core<0.2,>=0.1->langchain-groq) (3.9.15)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/greatmaster/miniconda3/envs/oreilly-agents/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain-groq) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in /Users/greatmaster/miniconda3/envs/oreilly-agents/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain-groq) (2.14.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/greatmaster/miniconda3/envs/oreilly-agents/lib/python3.11/site-packages (from requests<3,>=2->langchain-core<0.2,>=0.1->langchain-groq) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/greatmaster/miniconda3/envs/oreilly-agents/lib/python3.11/site-packages (from requests<3,>=2->langchain-core<0.2,>=0.1->langchain-groq) (2.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain-groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_groq import ChatGroq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatGroq(temperature=0, model_name=\"mixtral-8x7b-32768\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello! How can I help you today? If you have any questions about a particular topic or just need some guidance on a problem, feel free to ask. I'm here to help!\")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.invoke(\"HI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_openai = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I assist you today?')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_openai.invoke(\"HI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "system = \"You are a helpful assistant.\"\n",
    "human = \"{text}\"\n",
    "prompt = ChatPromptTemplate.from_messages([(\"system\", system), (\"human\", human)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='DEMONSTRATE –SEARCH –PREDICT :\\nComposing retrieval and language models for knowledge-intensive NLP\\nOmar Khattab1Keshav Santhanam1Xiang Lisa Li1David Hall1\\nPercy Liang1Christopher Potts1Matei Zaharia1\\nAbstract\\nRetrieval-augmented in-context learning has\\nemerged as a powerful approach for addressing\\nknowledge-intensive tasks using frozen language\\nmodels (LM) and retrieval models (RM). Exist-\\ning work has combined these in simple “retrieve-\\nthen-read” pipelines in which the RM retrieves\\npassages that are inserted into the LM prompt.\\nTo begin to fully realize the potential of frozen\\nLMs and RMs, we propose DEMONSTRATE –\\nSEARCH –PREDICT (DSP ), a framework that re-\\nlies on passing natural language texts in sophisti-\\ncated pipelines between an LM and an RM. DSP\\ncan express high-level programs that bootstrap\\npipeline-aware demonstrations, search for rele-\\nvant passages, and generate grounded predictions,\\nsystematically breaking down problems into small\\ntransformations that the LM and RM can handle\\nmore reliably. We have written novel DSP pro-\\ngrams for answering questions in open-domain,\\nmulti-hop, and conversational settings, establish-\\ning in early evaluations new state-of-the-art in-\\ncontext learning results and delivering 37–120%,\\n8–39%, and 80–290% relative gains against the\\nvanilla LM (GPT-3.5), a standard retrieve-then-\\nread pipeline, and a contemporaneous self-ask\\npipeline, respectively. We release DSP athttps:\\n//github.com/stanfordnlp/dsp .\\n1. Introduction\\nIn-context learning adapts a frozen language model (LM) to\\ntasks by conditioning the LM on a textual prompt including\\ntask instructions and a few demonstrating examples (Mc-\\nCann et al., 2018; Radford et al., 2019; Brown et al., 2020).\\nFor knowledge-intensive tasks such as question answering,\\nfact checking, and information-seeking dialogue, retrieval\\nmodels (RM) are increasingly used to augment prompts\\n1Stanford University . Correspondence to:\\nOmar Khattab <okhattab@cs.stanford.edu >.\\nPreprint .\\nHow many storeys are in the castle David Gregory inherited?\\nLM:Castle Gregory has three storeys.❌Hallucinates \\na fictitious castle\\nRM: “St. Gregory Hotel is a nine-floor boutique hotel in D.C...”\\nLM: St. Gregory Hotel has nine storeys.❌Retrieves a \\ndifferent building\\nLM: “Which castle did David Gregory inherit?”\\nRM: “David Gregory inherited Kinnairdy Castle in 1664...”\\nLM: “How many storyes does Kinnairdy Castle have?”\\nRM: “Kinnairdy Castle is a tower house, having five storeys…”\\nLM: Kinnairdy Castle has fivestoreys.Vanilla LM\\nRetrieve-\\nthen-Read\\nMulti-Hop\\nDSP ProgramFigure 1. A comparison between three systems based on GPT-\\n3.5 (text-davinci-002 ). On its own, the LM often makes false\\nassertions. An increasingly popular retrieve-then-read pipeline\\nfails when simple search can’t ﬁnd an answer. In contrast, a task-\\naware DSP program successfully decomposes the problem and\\nproduces a correct response. Texts edited for presentation.\\nwith relevant information from a large corpus (Lazaridou\\net al., 2022; Press et al., 2022; Khot et al., 2022).\\nRecent work has shown such retrieval-augmented in-context\\nlearning to be effective in simple “retrieve-then-read”\\npipelines: a query is fed to the RM and the retrieved pas-\\nsages become part of a prompt that provides context for\\nthe LM to use in its response. In this work, we argue that\\nthe fact that both LMs and RMs consume (and generate or\\nretrieve) natural language texts creates an opportunity for\\nmuch more sophisticated interactions between them. Fully\\nrealizing this would be transformative: frozen LMs and\\nRMs could serve as infrastructure across tasks, enabling\\nML- and domain-experts alike to rapidly build grounded\\nAI systems at a high level of abstraction and with lower\\ndeployment overheads and annotation costs.\\nFigure 1 begins to illustrate the power of retrieval-\\naugmented in-context learning, but also the limitations of\\n“retrieve-then-read” (Lazaridou et al., 2022; Izacard et al.,\\n2022). Our query is “How many storeys are in the castle', metadata={'source': './returning-structured-output-langchain/rag-intro-paper.pdf', 'page': 0}),\n",
       " Document(page_content='augmented in-context learning, but also the limitations of\\n“retrieve-then-read” (Lazaridou et al., 2022; Izacard et al.,\\n2022). Our query is “How many storeys are in the castle\\nDavid Gregory inherited?” When prompted to answer this,\\nGPT-3.5 ( text-davinci-002 ; Ouyang et al. 2022) makes\\nup a ﬁctitious castle with incorrect attributes, highlighting\\nthe common observation that knowledge stored in LM pa-\\nrameters is often unreliable (Shuster et al., 2021; Ishii et al.,\\n2022). Introducing an RM component helps, as the LM\\ncan ground its responses in retrieved passages, but a rigidarXiv:2212.14024v2  [cs.CL]  23 Jan 2023', metadata={'source': './returning-structured-output-langchain/rag-intro-paper.pdf', 'page': 0}),\n",
       " Document(page_content='DEMONSTRATE –SEARCH –PREDICT : Composing retrieval and language models\\nQHow many storeys are in...\\nQ In which city did Akeem \\nEllis play in 2017?\\nA Ellesmere PortQ When was the discoverer of \\nPalomar 4 born?\\nA 1889Train\\nDemonstrate\\ndefdemonstrate (x:Example ) -> Example :\\nx.demos = annotate (x.train, attempt )\\nreturn x\\ndefattempt (d:Example ):\\nd= search(d)\\nd= predict (d)\\nif d.pred == d.answer: return d1QHow many storeys are in the castle...\\nQ When was the discoverer of Palomar 4 born?\\nA 1889\\nHop1 Who discovered Palomar 4?\\nPsg1 Edwin Hubble discovered Palomar 4...\\nHop2 When was Edwin Powell born?\\nPsg2 Edwin Powell Hubble (1889–1953) was...\\nPred 1889\\nx : ExampleQ In which city did Akeem Ellis play...\\nA Ellesmere Port\\n... ...\\nPred Waterloo❌Demos“How many storeys are in the \\ncastle David Gregory inherited?”QHow many storeys are in the...\\nDemos . . .\\nHop1 Which castle did David Gregory inherit?\\nPsg1 David Gregory inherited Kinnairdy Castle...\\nHop2 How many storeys are in Kinnairdy Castle?\\nPsg2 Kinnairdy Castle […] having five storeys...\\nQHow many storeys does the...\\n. . . . . .\\nPred Five storeysSearch\\ndefsearch(x:Example ) -> Example :\\nx.hop1 =generate (hop_template)( x).pred\\nx.psg1 =retrieve (x.hop1, k=1)[0]\\nx.hop2 =generate (hop_template)( x).pred\\nx.psg2 =retrieve (x.hop2, k=1)[0]\\nreturn x2Predict\\ndefpredict (x:Example ) -> Example :\\nx.context = [x.psg1, x.psg2]\\nx.pred =generate (qa_template)( x).pred\\nreturn x3\\n“Five storeys”\\nFigure 2. A toy example of a DSP program for multi-hop question answering. Given an input question and a 2-shot training set, the\\nDEMONSTRATE stage programmatically annotates intermediate transformations on the training examples using a form of weak supervision.\\nLearning from a resulting demonstration , the SEARCH stage decomposes the complex input question and retrieves supporting information\\nover two retrieval hops. Finally, the P REDICT stage uses the demonstration and retrieved passages to answer the question.\\nretrieve-then-read strategy fails because the RM cannot ﬁnd\\npassages that directly answer the question.\\nWe introduce the DEMONSTRATE –SEARCH –PREDICT\\n(DSP ) framework for in-context learning, which relies en-\\ntirely on passing natural language text (and scores) be-\\ntween a frozen RM and LM.DSP introduces a num-\\nber of composable functions that bootstrap training exam-\\nples ( DEMONSTRATE ), gather information from a knowl-\\nedge corpus ( SEARCH ), and generate grounded outputs\\n(PREDICT ), using them to systematically unify techniques\\nfrom the retrieval-augmented NLP and the in-context learn-\\ning literatures (Lee et al., 2019; Khattab et al., 2021a; Anan-\\ntha et al., 2020; Gao et al., 2022; Izacard et al., 2022; Dohan\\net al., 2022; Zelikman et al., 2022; Zhang et al., 2022).\\nWe use DSP to suggest powerful strategies for knowledge-\\nintensive tasks with compositions of these techniques. This\\nreveals new conceptual possibilities for in-context learning\\nin general (§2), and it allows us to present rich programs\\nthat set new state-of-the-art results (§3).\\nFigure 1 shows the path that a DSP program might take to\\narrive at an answer, and Figure 2 illustrates how a deliberate\\nprogram achieves this. Instead of asking the LMto answer\\nthis complex question, the program’s SEARCH stage uses the\\nLMto generate a query “Which castle did David Gregory\\ninherit?” The RM retrieves a passage saying Gregory inher-\\nited the Kinnairdy Castle. After a second search “hop” ﬁnds\\nthe castle’s number of storeys, the PREDICT stage queries\\ntheLM with these passages to answer the original question.\\nAlthough this program implements behaviors such as query\\ngeneration, it requires no hand-labeled examples of these\\nintermediate transformations (i.e., of the queries and pas-\\nsages of both retrieval hops). Instead, the DEMONSTRATEstage uses labeled question–answer pairs to implement a\\nform of weak supervision that programmatically annotates\\nthe transformations invoked within SEARCH andPREDICT .', metadata={'source': './returning-structured-output-langchain/rag-intro-paper.pdf', 'page': 1}),\n",
       " Document(page_content='form of weak supervision that programmatically annotates\\nthe transformations invoked within SEARCH andPREDICT .\\nWe evaluate several DSP programs on answering questions\\nin open-domain, multi-hop, and conversational settings. In\\nthem, we implement novel and reusable transformations\\nsuch as bootstrapping annotations for all of our pipelines\\nwith weak supervision (§2.3), reliably rewriting questions to\\nresolve conversational dependencies and iteratively decom-\\npose complex queries with summarization of intermediate\\nhops (§2.4), and generating grounded responses from mul-\\ntiple passages with self-consistency (§2.5). We report pre-\\nliminary results on Open-SQuAD, HotPotQA, and QReCC\\nusing the frozen LMGPT-3.5 and RM ColBERTv2 (Khat-\\ntab & Zaharia, 2020; Santhanam et al., 2022b) with no\\nﬁne-tuning. Our DSP programs deliver 37–120%, 8–39%,\\nand 80–290% relative gains against corresponding vanilla\\nLMs, a standard retrieve-then-read pipeline, and a contem-\\nporaneous self-ask pipeline (Press et al., 2022), respectively.\\nFuture versions of this report will include additional test\\ntasks and LMchoices.\\nIn summary, this work makes the following contributions.\\nFirst, we argue that simple task-agnostic pipelines for in-\\ncontext learning should give way to deliberate, task-aware\\nstrategies. Second, we show that this shift need not be a\\nburden: with DSP , such strategies can be easily expressed\\nas short programs using composable operators. Third, this\\ncomposability spawns powerful capacities, like automati-\\ncally annotating demonstrations for complex pipelines from\\nend-task labels. Fourth, for three knowledge-intensive tasks,\\nwe implement rich programs that establish state-of-the-art\\nresults for in-context learning.', metadata={'source': './returning-structured-output-langchain/rag-intro-paper.pdf', 'page': 1}),\n",
       " Document(page_content='DEMONSTRATE –SEARCH –PREDICT : Composing retrieval and language models\\n2. D EMONSTRATE –SEARCH –PREDICT\\nWe now introduce the DSP framework and show its expres-\\nsive power by suggesting a number of strategies in which\\ntheLM andRM can come together to tackle complex prob-\\nlems effectively. We show in §3 that such strategies out-\\nperform existing in-context learning methods. We begin by\\ndiscussing the LMandRM foundation modules on which\\nDSP is built (§2.1) and then the datatypes and control ﬂow\\nwithin DSP (§2.2). Subsequently, we discuss each of the\\nthree inference stages: DEMONSTRATE (§2.3), SEARCH\\n(§2.4), and P REDICT (§2.5).\\n2.1. Pretrained Modules: LM and RM\\nADSP program deﬁnes the communication between the\\nlanguage model LMand the retrieval model RM.\\nLanguage Model We invoke a frozen language model\\nLM to conditionally generate (orscore ) text. For each\\ninvocation, the program prepares a prompt that adapts the\\nLM to a speciﬁc function (e.g., answering questions or\\ngenerating queries). A prompt often includes instructions,\\na few demonstrations of the desired behavior, and an input\\nquery to be answered.\\nAs in Figure 2, the LM generates not only: (i)the ﬁnal\\nanswer to the input question (in the PREDICT stage), but also\\n(ii)intermediate “hop” queries to ﬁnd useful information\\nfor the input question ( SEARCH ) as well as (iii)exemplar\\nqueries that illustrate how to produce queries for questions\\nin the training set ( DEMONSTRATE ). This systematic use of\\ntheLMis a hallmark of DSP programs.\\nRetrieval Model DSP programs also invoke a frozen re-\\ntrieval model RM toretrieve the top- kmost “relevant”\\ntext sequences for a given query . The RM canindex a\\nmassive set of pre-deﬁned passages for scalable search, and\\nthose passages can be updated without changing the retrieval\\nparameters. The RM accepts free-form textual inputs and\\nspecializes in estimating the relevance (or similarity) of a\\ntext sequence to a query.\\nAs in Figure 2, the RM is responsible for retrieving (i)\\npassages for each query generated by the LM(during the\\nSEARCH stage), but also (ii)passages that are used within\\ndemonstrations ( DEMONSTRATE ). In the latter case, the\\nRM’s contributions are less about providing directly rel-\\nevant information to the input question and more about\\nhelping the LMadapt to the domain and task.\\nThough not utilized in this example, the RM is also used in\\nDSP for functions like retrieving “nearest-neighbor” demon-\\nstrations from task training data ( DEMONSTRATE ) and se-\\nlecting well-grounded generated sequences from the LM\\n(PREDICT ).2.2. Datatypes and Control Flow\\nWe have implemented the DSP framework in Python. The\\npresent section introduces the core data types and compos-\\nable functions provided by the framework. We use illustra-\\ntive code snippets to ground the examples, and to convey\\nthe power that comes from being able to express complex\\ninteractions between the LMandRM in simple programs.\\nThe Example Datatype To conduct a task, a DSP pro-\\ngram manipulates one or more instances of the Example\\ndatatype. An Example behaves like a Python dictionary\\nwith multiple ﬁelds. The program is typically provided with\\na few training examples. The code snippet below illustrates\\nthis for multi-hop question answering.\\n1from dsp import Example\\n2\\n3train = [ Example ( question =\" When was the discoverer\\nof Palomar 4 born ?\", answer =\" 1889 \"),\\n4 Example ( question =\"In which city did Akeem\\nEllis play in 2017? \", answer =\" Ellesmere Port \")]\\nThis snippet contains two labeled examples, each with a\\nmulti-hop question (e.g., “In which city did Akeem Ellis\\nplay in 2017?”) and its short answer (“Ellesmere Port”).\\nArbitrary keys and values are allowed within an Example ,\\nthough typical values are strings or lists of strings.\\nIn this task, we are unlikely to ﬁnd an individual passage\\nthat provides the answer to any question. For example, the\\nﬁrst training example can probably be resolved only by ﬁrst\\nanswering the question of who discovered Palomar (“Edwin', metadata={'source': './returning-structured-output-langchain/rag-intro-paper.pdf', 'page': 2}),\n",
       " Document(page_content='that provides the answer to any question. For example, the\\nﬁrst training example can probably be resolved only by ﬁrst\\nanswering the question of who discovered Palomar (“Edwin\\nHubble”) and then addressing the question of Hubble’s birth\\ndate using different evidence passages. We typically assume\\nthat the human-labeled training data do notinclude labels\\nfor intermediate transformations (e.g., queries for individual\\nhops) that would be useful for following these steps, and so\\nit is the job of the DSP program to discover these strategies\\nvia in-context learning.\\nA DSP Program The following code snippet is a com-\\nplete program for resolving multi-hop questions like those\\nin Figure 1, with help from train examples like those above.\\n1def multihop_program ( question : str ) -> str :\\n2 x = Example ( question = question , train = train )\\n3 x = multihop_demonstrate (x)\\n4 x = multihop_search (x)\\n5 x = multihop_predict (x)\\n6 return x. answer\\n7\\n8multihop_program (\" How many storeys does the castle\\nDavid Gregory inherited have ?\")\\n9# => \" five storeys \"\\nThe program takes the input (here, a question) and outputs\\nthe system output (its short answer). It starts by creating\\nanExample for the input question and assigning the train\\nﬁeld to the training set from the previous snippet. Programs', metadata={'source': './returning-structured-output-langchain/rag-intro-paper.pdf', 'page': 2}),\n",
       " Document(page_content='DEMONSTRATE –SEARCH –PREDICT : Composing retrieval and language models\\ninvoke and compose DSP primitives (i.e., built-in functions)\\nto build the DEMONSTRATE ,SEARCH , and PREDICT trans-\\nformations that deﬁne the program.\\nTransformations A transformation is a function that\\ntakes an Example as input and returns an Example , pop-\\nulating new ﬁelds (or modifying existing ﬁelds) in it. This\\nprogram invokes three developer-deﬁned transformations,\\nnamely, multihop_demonstrate ,multihop_search , and\\nmultihop_predict . Transformations may themselves in-\\nvoke other transformations, and they act analogously to\\nlayers in standard deep neural network (DNN) program-\\nming frameworks such as PyTorch, except that they pass\\ntext data instead of tensors between each other and do not\\ninvolve backpropagation.\\nWe categorize transformations according to their behavior\\n(or purpose) under one of the DEMONSTRATE ,SEARCH ,\\nandPREDICT stages. That said, DSP does not impose this\\ncategorization and allows us to deﬁne functions that may\\nblend these stages. We will discuss each of the three stages\\nnext.\\n2.3. D EMONSTRATE\\nIt is known that including examples of the desired behavior\\nfrom the LMin its prompt typically leads to better perfor-\\nmance (Brown et al., 2020). In DSP , ademonstration is a\\ntraining example that has been prepared to illustrate speciﬁc\\ndesired behaviors from the LM. ADEMONSTRATE transfor-\\nmation takes as input xof type Example and prepares a list\\nof demonstrations in x.demos , typically by selecting a sub-\\nset of the training examples in x.train andbootstrapping\\nnew ﬁelds in them.\\nBootstrapping Demonstrations Examples in the train-\\ning set typically consist of the input text and the target\\noutput of the task. The DEMONSTRATE stage can aug-\\nment a training example by programmatically bootstrapping\\nannotations for intermediate transformations. In our run-\\nning “multi-hop” example, the demonstrations illustrate\\nthree LM-based transformations: (i)how to break down the\\ninput question in order to gather information for answer-\\ning it (i.e., ﬁrst-hop retrieval), (ii)how to use information\\ngathered in an earlier “hop” to ask follow-up questions, and\\n(iii)how to use the information gathered to answer complex\\nquestions.\\n1Examples = list [ Example ]\\n2Transformation = Callable [[ Example ],\\n3 Optional [ Example ]]\\n4\\n5annotate ( train : Examples , fn: Transformation )\\n6 -> Examples\\nAkin to a specialized map, the annotate primitive accepts\\na user-deﬁned transformation fnand applies it over a listof training examples. Whenever fnreturns an example\\n(rather than None ),annotate caches the intermediate pre-\\ndictions (i.e., the generated queries and retrieved passages).\\nThese predictions serve as successful demonstrations for the\\npipeline transformations. In simple uses, fnmay attempt\\nto answer the example “zero-shot” one or more times. This\\nis typically done by invoking the SEARCH andPREDICT\\nstages of the program. When an answer is produced, if\\nfnassesses it as correct, it returns a populated example in\\nwhich the intermediate predictions are present.\\nCase Study The snippet below deﬁnes the func-\\ntion multihop_demonstrate , called in Line 3 of\\nmultihop_program , and illustrates the usage of annotate .\\n1from dsp import sample , annotate\\n2\\n3def attempt_example (d: Example ):\\n4 d = d. copy ( demos =[])\\n5 d = multihop_search (d)\\n6 d = multihop_predict (d)\\n7 return d if d. pred == d. answer else None\\n8\\n9def multihop_demonstrate (x: Example ):\\n10 demos = annotate (x.train , attempt_example )\\n11 return Example (x, demos = demos )\\nIn Line 10, multihop_demonstrate invokes annotate ,\\nwhich bootstraps missing ﬁelds in training examples by\\ncaching annotations from attempt_example . The transfor-\\nmation attempt_example takes a training example dand\\nattempts to answer it in a zero-shot fashion: it creates a copy\\nofdwith no demonstrations (Line 4; i.e., zero-shot) and\\ninvokes the multi-hop search and predict pipeline (Lines 5', metadata={'source': './returning-structured-output-langchain/rag-intro-paper.pdf', 'page': 3}),\n",
       " Document(page_content='attempts to answer it in a zero-shot fashion: it creates a copy\\nofdwith no demonstrations (Line 4; i.e., zero-shot) and\\ninvokes the multi-hop search and predict pipeline (Lines 5\\nand 6). Each transformation returns an updated version of\\ndwith additional ﬁelds populated. If the pipeline answers\\ncorrectly (Line 7), the updated dis returned.\\nFigure 2 illustrates this behavior. DEMONSTRATE trans-\\nforms a training question–answer pair to a fully-populated\\ndemonstration, including ﬁelds such as hop1 andhop2 (i.e.,\\nqueries for multi-hop search) as well as psg1 andpsg2 .\\nWhen the LMis later invoked to conduct a transformation,\\nsay, generating a “second-hop” query during SEARCH , the\\npsg1 ﬁeld serves as context and the hop2 ﬁeld serves as a\\nlabel for this particular training example.\\nDiscussion This simple case study illustrates the power of\\ncomposition in the DSP abstraction. Because the pipeline\\nis a well-deﬁned program in which transformations com-\\nmunicate by passing text attached to Example s, a simple\\nmap-and-ﬁlter strategy can leverage the LM andRM to\\nbootstrap annotations for a full pipeline from end-task la-\\nbels. This is an extensible strategy, but even in its simplest\\nform it generalizes the approaches explored recently by Ze-\\nlikman et al. (2022), Wei et al. (2022), Zhang et al. (2022),\\nand Huang et al. (2022) in which an LM self-generates\\nchain-of-thought rationales for an individual prompt.', metadata={'source': './returning-structured-output-langchain/rag-intro-paper.pdf', 'page': 3}),\n",
       " Document(page_content='DEMONSTRATE –SEARCH –PREDICT : Composing retrieval and language models\\nBy bootstrapping pipelines, DEMONSTRATE makes it easy\\nto explore complex strategies in SEARCH andPREDICT\\nwithout writing examples for every transformation. This\\nincludes strategies that are challenging to explore without\\ncustom annotations in traditional retrieval-augmented NLP.\\nFor instance, Khattab et al. (2021a) introduces a pipeline\\nfor multi-hop reasoning that is trained with weak supervi-\\nsion, extending work by Lee et al. (2019) and Khattab et al.\\n(2021b). In it, the target 3 or 4 passages that need to re-\\ntrieved must be labeled but the system discovers the best\\norder of “hops” automatically.\\nIn contrast, DSP allows us to build complex pipelines with-\\nout labels for intermediate steps, because we can compose\\nprograms out of small transformations. If LMandRM can\\naccurately process such transformations “zero-shot” (i.e.,\\nwithout demonstrations) on at least one or two examples,\\nthese examples can be discovered with end-task labels and\\nused as demonstrations.\\nTo draw on our earlier analogy with DNN frameworks like\\nPyTorch, DEMONSTRATE aims to replace the function of\\nbackpropagation in extensible ways by simulating the be-\\nhavior of the program (corresponding to a “forward” pass)\\nand programmatically learning from errors. In doing this\\nwith frozen models and with only end-task labels, DEMON -\\nSTRATE introduces a high degree of modularity. In partic-\\nular, without hand-labeling intermediate transformations,\\ndevelopers may swap the training domain, update the train-\\ning examples, or modify the program’s strategy, and use\\nannotate to automatically populate all of the intermediate\\nﬁelds for demonstrations.\\nSelecting Demonstrations It is not always possible to ﬁt\\nall of the training examples in the context window of the\\nLM.DSP provides three primitives for selecting a subset\\nof training examples, namely, sample ,knn, and crossval .\\n1sample ( train : Examples , k: int )\\n2 -> Examples\\n3\\n4knn ( train : Examples , cast : Callable [[ Example ], str ])\\n5 -> fn( example : Example , k: int ) # currying\\n6 -> Examples\\n7\\n8crossval ( train : Examples , n: int , k: int )\\n9 -> fn( evaluate : Transformation )\\n10 -> Examples\\nAs a baseline choice, kdemonstrations can be randomly\\nsampled from train using the sample primitive, an ap-\\nproach used by Brown et al. (2020) and much subsequent\\nwork. We can also leverage the RM’s representations and se-\\nlect from the training set the knearest neighbors to the input\\ntext, a strategy explored by Liu et al. (2021). Another strat-\\negy is to apply cross-validation to select among a number of\\nsampled sets of demonstrations (Perez et al., 2021). For ex-\\nample, given |train |= 100 training examples, crossvalwould select nsubsets of k= 5examples each, and return\\nthe set with which a transformation evaluate performs best\\non the remaining 95examples.\\nCompositions & Extensions By manipulating demon-\\nstrations and higher-order transformations, these simple\\nselection and bootstrapping primitives can be combined to\\nconduct larger novel strategies. If the training set is very\\nlarge (e.g., |train |= 100 ,000), we can conduct knnto\\nﬁnd the nearest k= 16 examples and only annotate these,\\narriving at a system that learns incrementally in real-time. If\\nthe training set is moderately large (e.g., |train |= 1000 ),\\nwe can conduct crossval and cache the performance of all\\nprompts it evaluates on each training example. At test time,\\nwe can use knnto ﬁnd k= 50 similar examples to the test\\ninput and select the prompt that performs best on these k\\nexamples, producing an adaptive system that is informed by\\nthe quality of its pipeline on different types of examples.\\n2.4. S EARCH\\nTheSEARCH stage gathers passages to support transforma-\\ntions conducted by the LM. We assume a large knowledge\\ncorpus—e.g., a snippet of Web, Wikipedia, or arXiv—that\\nis divided into text passages . Providing passages to the LM\\nfacilitates factual responses, enables updating the knowl-', metadata={'source': './returning-structured-output-langchain/rag-intro-paper.pdf', 'page': 4}),\n",
       " Document(page_content='corpus—e.g., a snippet of Web, Wikipedia, or arXiv—that\\nis divided into text passages . Providing passages to the LM\\nfacilitates factual responses, enables updating the knowl-\\nedge store without retraining, and presents a transparency\\ncontract: when in doubt, users can check whether the system\\nhas faithfully used a reliable source in making a prediction.\\nIn the simplest scenarios, SEARCH can directly query the\\nRM, requesting the top- kpassages (from a pre-deﬁned in-\\ndex) that match an input question. This baseline instantia-\\ntion of SEARCH simulates retrieval in most open-domain\\nquestion answering systems, which implement a “retrieve-\\nthen-read” pipeline, like Lee et al. (2019), Khattab et al.\\n(2021b), Lazaridou et al. (2022), and many others.\\n1from dsp import retrieve\\n2\\n3def simple_search (x):\\n4 passages = retrieve ( query =x. question , k =2)\\n5 return passages\\nSEARCH Strategies In many scenarios, the complexity\\nof the task demands more sophisticated SEARCH strategies\\nthat empower the RM to ﬁnd relevant passages. Our run-\\nning example (Figure 2) is one such scenario, in which we\\nsuspect examples are likely to require multi-hop reasoning\\nin particular. Other settings, for instance, pose conversa-\\ntional challenges, in which the information need expressed\\nby a user can only be resolved by taking into account pre-\\nvious turns in the conversation, or demand more extensive\\nplanning (Zhong et al., 2022).\\nIn the retrieval-augmented NLP literature, multi-hop\\nsearch (Xiong et al., 2020; Khattab et al., 2021a) and con-', metadata={'source': './returning-structured-output-langchain/rag-intro-paper.pdf', 'page': 4}),\n",
       " Document(page_content='DEMONSTRATE –SEARCH –PREDICT : Composing retrieval and language models\\nversational search (Del Tredici et al., 2021; Raposo et al.,\\n2022) pipelines have received much attention. These sys-\\ntems are typically ﬁne-tuned with many hand-labeled query\\n“rewrites” (Anantha et al., 2020), “decompositions” (Geva\\net al., 2021; Min et al., 2019), or target hops (Yang et al.,\\n2018; Jiang et al., 2020). Supported with automatic anno-\\ntations from D EMONSTRATE , the S EARCH stage allows us\\nto simulate many such strategies and many others in terms\\nof passing queries, passages, and demonstrations between\\ntheRM andLM. More importantly, SEARCH facilitates our\\nvision of advanced strategies in which the LMandRM co-\\noperate to incrementally plan a research path for which the\\nRM gathers information and the LMidentiﬁes next steps.\\nCase Study Let us build on our running multi-hop exam-\\nple as a case study. We can deﬁne multihop_search_v2\\n(Line 4 in our core program), a slightly more advanced ver-\\nsion of the SEARCH transformation from Figure 2. This\\ntransformation simulates the iterative retrieval component\\nof ﬁne-tuned retrieval-augmented systems like IRRR (Qi\\net al., 2020), which reads a retrieved passage in every hop\\nand generates a search query (or a termination condition to\\nstop hopping), and Baleen (Khattab et al., 2021a), which\\nsummarizes the information from many passages in each\\nhop for inclusion in subsequent hops.\\n1from dsp import generate\\n2\\n3def multihop_search_v2 (x, max_hops =3) :\\n4 x. hops = []\\n5\\n6 for hop in range ( max_hops ):\\n7 summary , query = generate ( hop_template )(x)\\n8 x. hops . append (( summary , query ))\\n9\\n10 if query == /quotesingle.VarN/A/quotesingle.Var: break\\n11\\n12 passages = retrieve (query , k =5)\\n13 x. context = [ summary ] + passages\\n14\\n15 return x\\nInmultihop_search_v2 , Line 7 calls the generate prim-\\nitive, which invokes the LM to produce a query for each\\nretrieval hop. The LM is conditioned on a prompt that is\\nprepared using the hop_template template. (We discuss\\nprompt templates and the generate primitive in §2.5.) Here,\\nthis template may be designed to generate a prompt that has\\nthe following format (e.g., for the second hop).\\n1My task is to write a simple query that gathers\\ninformation for answering a complex question . I\\nwrite N/A if the context contains all\\ninformation required .\\n2\\n3{ Task demonstrations from x.demos , if any }\\n4\\n5Context : {x. context }\\n6Question : {x. question }\\n7Summary : Let /quotesingle.Vars summarize the above context .\\n__{ summary }__\\n8Search Query : __{ query }__As shown, the LM is instructed to read the context re-\\ntrieved in earlier hops and a complex question. It is then\\nprompted to write: (i)a summary of the supplied con-\\ntext and (ii)a search query that gathers information for\\nanswering that question. The generated text will be ex-\\ntracted and assigned to the summary andquery variables in\\n(multihop_search_v2 ; Line 7). On Line 10, we terminate\\nthe hops if the query is “N/A”. Otherwise, Line 12 retrieves\\nk= 5 passages using the query and Line 13 assigns the\\ncontext for the subsequent hop (or for PREDICT ), setting\\nthat to include the summary of all previous hops as well as\\nthe passages retrieved in the ﬁnal hop so far.\\nComparison with self-ask It may be instructive to con-\\ntrast this multi-hop DSP program with the recent “self-\\nask” (Press et al., 2022) prompting technique, which we\\ncompare against in §3. Self-ask can be thought of as a sim-\\nple instantiation of DSP ’sSEARCH stage. In it, the LMasks\\none or more “follow-up questions”, which are intercepted\\nand sent to a search engine. The search engine’s answers\\nare concatenated into the prompt and are used to answer\\nthe question. This is essentially a simpliﬁed simulation of\\nIRRR (Qi et al., 2020).\\nAs a general framework, DSP can express ideas like self-ask\\nand many other, more sophisticated pipelines as we discuss\\nin the present section. More importantly, DSP offers a num-', metadata={'source': './returning-structured-output-langchain/rag-intro-paper.pdf', 'page': 5}),\n",
       " Document(page_content='As a general framework, DSP can express ideas like self-ask\\nand many other, more sophisticated pipelines as we discuss\\nin the present section. More importantly, DSP offers a num-\\nber of intrinsic advantages that lead to large empirical gains:\\n80%–290% over self-ask. For instance, DSP programs are\\ndeeply modular, which among other things means that DSP\\nprograms will annotate and construct their own demonstra-\\ntions. Thus, they can be developed without labeling any\\nof the intermediate transformations (e.g., the queries gener-\\nated). In addition, the LM prompts constructed by DSP get\\nautomatically updated to align with the training data and re-\\ntrieval corpus provided. In contrast, approaches like self-ask\\nrely on a hand-written prompt with hard-coded examples.\\nMoreover, DSP assigns the control ﬂow to an explicit pro-\\ngram and facilitates design patterns that invoke the LM(or\\nRM) to conduct small transformations. This allows us to\\nbuild steps that are dedicated to generating one or more re-\\ntrieval queries, summarizing multiple passages per hop, and\\nanswering questions. These steps are individually simpler\\nthan the self-ask prompt, yet our multi-hop DSP program\\ndeliberately composes them to build richer pipelines that are\\nthus more reliable. In contrast, self-ask delegates the con-\\ntrol ﬂow to the LMcompletions, maintaining state within\\nthe prompt itself and intercepting follow-up questions to\\nconduct search. We ﬁnd that this paradigm leads to a “self-\\ndistraction” problem (§3.5) that DSP programs avoid.\\nFusing Retrieval Results For improved recall and robust-\\nness, we can also fuse the retrieval across multiple gen-\\nerated queries. Fusion has a long history in information', metadata={'source': './returning-structured-output-langchain/rag-intro-paper.pdf', 'page': 5}),\n",
       " Document(page_content='DEMONSTRATE –SEARCH –PREDICT : Composing retrieval and language models\\nretrieval (Fox & Shaw, 1994; Xue & Croft, 2013; Kur-\\nland & Culpepper, 2018) and sequentially processing multi-\\nple queries was explored recently by Gao et al. (2022) for\\nretroactively attributing text generated by LMs to citations.\\nInspired by these, we include a fused_retrieval primitive\\ntoDSP to offer a versatile mechanism for interacting with\\nfrozen retrievers. It accepts an optional fusion function that\\nmaps multiple retrieval lists into one. By default, DSP uses\\na variant of CombSUM (Fox & Shaw, 1994), assigning each\\npassage the sum of its probabilities across retrieval lists.\\nTo illustrate, the modiﬁcation below generates n= 10\\nqueries for the transformation multihop_search_v2 .\\nc = generate ( hop_template , n =10) (x)\\npassages = fused_retrieval (c. queries , k =5)\\nsummary = c. summaries [0] # highest - scoring summary\\nCompositions & Extensions To illustrate a simple com-\\nposition, we can equip a chatbot with the capacity for con-\\nversational multi-hop search by combining a query rewriting\\nstep, which produces a query that encompasses all of the\\nrelevant conversational context, with the multi-hop transfor-\\nmation, as follows.\\n1def conversational_multihop_search (x):\\n2 x. question = generate ( conv_rewriting_template )(x)\\n3 return multihop_search_v2 (x)\\nSimilar approaches can be used for correcting spelling mis-\\ntakes or implementing pseudo-relevance feedback (Cao\\net al., 2008; Wang et al., 2022a), in which retrieved passages\\nare used to inform a better search query, though this has not\\nbeen attempted with pretrained LMs to our knowledge.\\n2.5. P REDICT\\nThe PREDICT stage generates the system output using\\ndemonstrations (e.g., in x.demos ) and passages (e.g., in\\nx.context ).PREDICT tackles the challenges of reliably\\nsolving the downstream task, which integrates much of the\\nwork on in-context learning in general. Within DSP , it also\\nhas the more specialized function of systematically aggre-\\ngating information across a large number of demonstrations,\\npassages, and candidate predictions.\\nGenerating Candidates Generally, PREDICT has to pro-\\nduce one or more candidate predictions for the end-task.\\nTo this end, the basic primitive in PREDICT isgenerate ,\\nwhich accepts a Template and (via currying) an Example\\nand queries the LM to produce one or more completions,\\nas explored earlier in §2.4. A corresponding primitive that\\nuses the RM in this stage is rank , which accepts a query\\nand one or more passages and returns their relevance scores.1Template # template : an object that can produce\\nprompts and parse completions\\n2\\n3generate ( template : Template )\\n4 -> fn( example : Example )\\n5 -> Completions # object with keys to access\\nextracted preds and scores\\n6\\n7rank ( query : str , passages : List [ str ])\\n8 -> List [ float ] # object with keys to access\\npassage texts and scores\\nATemplate is an object that can produce prompts, that is,\\nmap an Example to a string, and extract ﬁelds out of com-\\npletions. For instance, we can map an example xthat has a\\nquestion and retrieved passages to the following prompt:\\n1My task is to answer questions using Web documents .\\n2\\n3{ Task demonstrations from x.demos , if any }\\n4\\n5Context : {x. passage }\\n6Question : {x. question }\\n7Rationale : Let /quotesingle.Vars think step by step . __{ rationale }__\\n8Answer : __{ answer }__\\nAs this illustrates, the LMwill be asked to generate a chain-\\nof-thought rationale (CoT; Wei et al. 2022; Kojima et al.\\n2022) and an answer, and the generated text will be ex-\\ntracted back into the rationale andanswer keys of each\\ncompletion.\\nEach invocation to the LMcan sample multiple candidate\\npredictions. Selecting a “best” prediction is the subject of\\nmuch work on decoding (Wiher et al., 2022; Li et al., 2022),\\nbut a frozen and general-purpose LMmay not support cus-\\ntom modiﬁcations to decoding. Within these constraints, we\\npresent several high-level strategies for selecting predictions', metadata={'source': './returning-structured-output-langchain/rag-intro-paper.pdf', 'page': 6}),\n",
       " Document(page_content='but a frozen and general-purpose LMmay not support cus-\\ntom modiﬁcations to decoding. Within these constraints, we\\npresent several high-level strategies for selecting predictions\\nand aggregating information in DSP via the LMandRM.\\nSelecting Predictions Among multiple candidates, we\\ncan simply extract the most popular prediction. When a CoT\\nis used to arrive at the answer, this is the self-consistency\\nmethod of Wang et al. (2022c), which seeks to identify\\npredictions at which multiple distinct rationales arrive.\\n1from dsp import generate , majority\\n2\\n3def multihop_predict (x):\\n4 candidates = generate ( template_qa )(x)\\n5 return x. copy ( answer = majority ( candidates ). answer )\\nDSP generalizes this in two ways. First, we can sample\\nmultiple “pipelines of transformations” (PoT) within the pro-\\ngram, rather than locally with “chains of thought” (CoT) in\\none transformation. These chains may even invoke different\\npaths in the program, as illustrated below.', metadata={'source': './returning-structured-output-langchain/rag-intro-paper.pdf', 'page': 6}),\n",
       " Document(page_content='DEMONSTRATE –SEARCH –PREDICT : Composing retrieval and language models\\n1from dsp import branch\\n2\\n3def pipeline (x):\\n4 return multihop_predict ( multihop_search_v2 (x))\\n5\\n6def PoT_program ( question : str ) -> str :\\n7 x = Example ( question = question , train = train )\\n8 x = multihop_demonstrate (x)\\n9\\n10 candidates = branch ( pipeline , n=5, t =0.7) (x)\\n11 return x. copy ( answer = majority ( candidates ). answer )\\nIn the snippet above, Line 10 invokes the primitive branch\\nwhich samples ndifferent PoTs with a high temperature\\n(e.g., t= 0.7) and accumulates their intermediate and\\nﬁnal predictions. In this example, our pipeline invokes\\nmultihop_search_v2 (§2.4), which applies a variable num-\\nber of retrieval hops depending on the questions generated,\\nbefore doing PREDICT . That is, PoT_program potentially\\ninvokes multiple distinct paths in the program (i.e., with dif-\\nferent multi-hop queries and number of hops in each) across\\nbranches. It then selects the majority answer overall.\\nDSP generalizes self-consistency in a second way. When\\nsampling our CoTs or PoTs provides multiple candidates,\\nwe can select the top- k(e.g., top-4) predictions and then\\ncompare them directly. For instance, we may prompt the\\nLMto compare these choices as MCQ candidates, a trans-\\nformation for which DEMONSTRATE can automatically pre-\\npare exemplars. This effectively simulates the LM recursion\\nof Levine et al. (2022), though unlike their approach it does\\nnot require a large training set or updating any (prompt-\\ntuning) weights. One such implementation is illustrated in\\nopenqa_predict below.\\n1def openqa_predict (x):\\n2 preds = generate ( template_qa , n =20) (x). answers\\n3 x. choices = most_common (preds , k =4)\\n4\\n5 queries = [f\"{x. question } {c}\"\\n6 for c in x. choices ]\\n7\\n8 x. passages = fused_retrieval ( queries )\\n9 x. answer = generate ( TemplateMCQ )(x). answer\\n10 return x\\nAs an alternative comparison approach, we can invoke the\\nRM viarank to ﬁnd the prediction that is most grounded in\\na retrieved contexts (i.e., most similar to the concatenation\\nof the retrieved passages) or, given an RM that can score\\ncompletions (Krishna et al., 2022), simply the prediction\\nthat has the highest score given the prompt.\\nAggregating Information When only a few demonstra-\\ntions or passages are selected, we can simply concate-\\nnate them all into the prompt. For instance, GPT-3.5\\ntext-davinci-002 has a context window of 4097 tokens,\\nwhich we ﬁnd to be reasonably large for accommodating\\nseveral (e.g., 3–5) demonstrations, which individually in-\\nclude their own passages and rationales.To deal with a larger number of demonstrations or passages,\\nwe can branch in parallel to process individual subsets\\nof the passages or demonstrations and then aggregate the\\nindividual answers using one of the scoring methods pre-\\nsented earlier. Indeed, Lewis et al. (2020) and Lazaridou\\net al. (2022) have explored marginalization as a way to com-\\nbine scores across passages and Le et al. (2022) ensemble\\nprompts across demonstrations, which can be expressed in\\nthis way.\\nAn alternative aggregation strategy is to accumulate informa-\\ntion across passages sequentially, rather than independently.\\nThis is effectively how our multi-hop approach works (§2.4).\\nSuch a strategy has also been employed recently by Gao\\net al. (2022) for retroactively attributing text generated by\\nLMs to citations. They generate many queries but instead\\nof fusion (§2.4), they run their pipeline on each query and\\nuse its outputs to alter the input to subsequent queries.1\\n3. Evaluation\\nWe now consider how to implement DSP programs for three\\ndiverse knowledge-intensive NLP tasks: open-domain ques-\\ntion answering (QA), multi-hop QA, and conversational QA.\\nAll of these tasks are “open-domain”, in the sense that sys-\\ntems are given a short question or participate in a multi-turn\\nconversation without being granted access to context that\\nanswers these questions.\\nWe build and evaluate intuitive compositions of the func-', metadata={'source': './returning-structured-output-langchain/rag-intro-paper.pdf', 'page': 7}),\n",
       " Document(page_content='conversation without being granted access to context that\\nanswers these questions.\\nWe build and evaluate intuitive compositions of the func-\\ntions explored in §2 for each task. We show that, despite\\nlow development effort, the resulting DSP programs exhibit\\nstrong quality and deliver considerable empirical gains over\\nvanilla in-context learning and a standard retrieve-then-read\\npipeline with in-context learning.\\n3.1. Evaluation Methodology\\nIn this report, we consider one development dataset for each\\nof the tasks we consider, namely, the open-domain version\\nof SQuAD (Rajpurkar et al., 2016; Lee et al., 2019), the\\nmulti-hop HotPotQA (Yang et al., 2018) dataset in the open-\\ndomain “fullwiki” setting, and the conversational question\\nanswering QReCC (Anantha et al., 2020; Vakulenko et al.,\\n2022) dataset, which we used for developing the DSP ab-\\nstractions. We report the validation set accuracy on all three\\ndatasets and discuss them in detail §3.5.\\nUnless otherwise stated, systems are given access to 16-\\nshot training examples, that is, each DSP program can use\\n(up to) 16 questions—or conversations, where applicable—\\nrandomly sampled from the respective training set. We\\n1Though most of the functionality in this section is imple-\\nmented, the primitives branch ,knn, and crossval are currently\\nwork-in-progress.', metadata={'source': './returning-structured-output-langchain/rag-intro-paper.pdf', 'page': 7}),\n",
       " Document(page_content='DEMONSTRATE –SEARCH –PREDICT : Composing retrieval and language models\\nsubsample the validation and test sets to 1000 questions\\n(or 400 conversations, where applicable) and report average\\nquality across ﬁve seeds where each seed ﬁxes a single k-\\nshot training set of examples. To control the language model\\nAPI spending budget, each seed processes one ﬁfth of the\\nevaluation examples (e.g., 200 questions per seed, for a total\\nof 1000 unique questions).\\nWe also dedicate held-out test datasets (e.g., Open-\\nNaturalQuestions; Kwiatkowski et al. 2019) and test tasks\\n(e.g., claim veriﬁcation, as in FEVER; Thorne et al. 2018)\\nthat we only use for evaluating pre-deﬁned DSP programs\\nrather than development. We will include these results in a\\nfuture version of this report.\\n3.2. Pretrained Modules\\nRM We use ColBERTv2 (Santhanam et al., 2022b), a\\nstate-of-the-art retriever based on late interaction (Khattab\\n& Zaharia, 2020). We choose ColBERTv2 for its highly\\neffective zero-shot search quality and efﬁcient search (San-\\nthanam et al., 2022a). However, our DSP programs are\\nagnostic to how the retriever represents examples or scores\\npassages, so essentially any retriever can be used.\\nIn addition, by making retrieval a ﬁrst-class construct, DSP\\nallows us to change or update the search index over time.\\nWe simulate this in our experiments by aligning each of our\\ndatasets with the nearest Wikipedia corpus among the Dec\\n2016 Wikipedia dump from Chen et al. 2017, the Nov 2017\\nWikipedia “abstracts” dump from Yang et al. 2018, and the\\nDec 2018 Wikipedia dump from Karpukhin et al. 2020.\\nLM We use the GPT-3.5 ( text-davinci-002 ; Brown\\net al. 2020; Ouyang et al. 2022) language model. Unless\\notherwise stated, we use greedy decoding when generating\\nn= 1 prediction. We sample with temperature t= 0.7\\nwhen n > 1, like related work (Wang et al., 2022c).\\n3.3. Baselines\\nVanilla LM The vanilla LM baselines represent the few-\\nshot in-context learning paradigm used by Brown et al.\\n(2020). The open-domain QA and multi-hop QA base-\\nlines randomly sample 16 demonstrations (i.e., all of the\\nexamples available to each program in our evaluation) from\\nthe training set and do not augment these demonstrations\\nwith evidence. Similarly, the conversational QA baseline\\nsamples four conversations. The vanilla baselines do not\\nsearch for passages relevant to the input query.\\n1def vanilla_LM_QA ( question : str ) -> str :\\n2 demos = sample (train , k =16)\\n3 x = Example ( question = question , demos = demos )\\n4 return generate ( qa_template )(x). predRetrieve-then-Read The “retrieve-then-read” baselines\\nuse the RM to support each example with a potentially rele-\\nvant passage before submitting the prompt to the LM. This\\nemulates the pipelines used by state-of-the-art open-domain\\nquestion answering systems (Khattab et al., 2021b; Izacard\\n& Grave, 2020; Hofstätter et al., 2022). In conversational\\nQA, we concatenate the ﬁrst turn and the ﬁnal question, an\\napproach that we found to perform much better than simply\\nusing the ﬁnal turn. For multi-hop QA, we retrieve and\\nconcatenate two passages per question.\\n1def retrieve_then_read_QA ( question : str ) -> str :\\n2 demos = sample (train , k =16)\\n3 passages = retrieve ( question , k =1)\\n4 x = Example ( question = question ,\\n5 passages = passages ,\\n6 demos = demos )\\n7 return generate ( qa_template )(x). pred\\nSelf-ask We also compare against self-ask (Press et al.,\\n2022), a contemporaneous pipeline that can be thought of\\nas a speciﬁc instantiation of DSP ’sSEARCH stage followed\\nby a simple PREDICT step. For direct comparison with\\nour methods, we modify the self-ask control ﬂow to query\\nthe same ColBERTv2 index used in our DSP experiments\\ninstead of Google Search. We evaluate two conﬁgurations of\\nself-ask. The ﬁrst uses the original self-ask prompt template,\\nwhich contains four hand-written demonstrations. In the\\nsecond conﬁguration, we modify the prompt template to\\napply a number of changes that we ﬁnd are empirically\\nuseful for HotPotQA.2', metadata={'source': './returning-structured-output-langchain/rag-intro-paper.pdf', 'page': 8}),\n",
       " Document(page_content='which contains four hand-written demonstrations. In the\\nsecond conﬁguration, we modify the prompt template to\\napply a number of changes that we ﬁnd are empirically\\nuseful for HotPotQA.2\\n3.4. Proposed DSP Programs\\nWe build on transformations presented in §2. Our programs\\nfor all three tasks have the following structure, illustrated\\nfor open-domain QA.\\n1def openqa_program ( question : str ) -> str :\\n2 x = Example ( question = question , train = train )\\n3 x = openqa_demonstrate (x)\\n4 x = openqa_search (x)\\n5 x = openqa_predict (x)\\n6 return x. answer\\nThe exception is that the conversational QA program,\\n2In particular: (i)use ColBERTv2-style passages in the hand-\\ncrafted demonstrations of self-ask (i.e., instead of the original\\nGoogle-style snippets), (ii)concatenate 16-shot training examples\\nfrom the task (i.e., question–answer pairs) as a preﬁx of the prompt,\\n(iii)ask the model to generate a short intermediate answer per\\nretrieval step, and (iv)explicitly ask the model to generate a follow-\\nup “search query” at each step. We found the ﬁnal item to be\\nimportant because self-ask’s default prompt often produces follow-\\nup questions that are not self-contained (e.g., “what is the name of\\nthe national park?”, which is not an informative search query). We\\nalso ﬁx the casing in the prompt to be consistent.', metadata={'source': './returning-structured-output-langchain/rag-intro-paper.pdf', 'page': 8}),\n",
       " Document(page_content='DEMONSTRATE –SEARCH –PREDICT : Composing retrieval and language models\\nTable 1. Development results comparing a task-aware DSP program against baseline vanilla LM and retrieve-then-read LM as well as\\nrecent and contemporaneous in-context learning approaches with and without retrieval. All of our runs use GPT-3.5 and our retrieval-based\\nrows use ColBERTv2. The results marked with¶are collected from related work as of mid-December 2022, and attributed to their\\nindividual sources in the main text. As we discuss in the main text, the marked results are not generally apples-to-apples comparisons,\\nsince they span a variety of evaluation settings. Nonetheless, we report them here as qualitative reference points.\\nOpen-SQuAD HotPotQA QReCC\\nEM F1 EM F1 F1 nF1\\nVanilla LM 16.2 25.6 28.3 36.4 29.8 18.4\\nNo-retrieval LM SoTA 20.2¶– 33.8¶44.6¶– –\\nRetrieve-then-Read 33.8 46.1 36.9 46.1 31.6 22.2\\nSelf-ask w/ ColBERTv2 Search 9.3 17.2 25.2 33.2 – –\\n+ Reﬁned Prompt 9.0 15.7 28.6 37.3 – –\\nRetrieval-augmented LM SoTA 34.0¶– 35.1¶– – –\\nTask-aware DSP Program 36.6 49.0 51.4 62.9 35.0 25.3\\nconvqa_program , accepts turns (i.e., a list of strings, rep-\\nresenting the conversational history) instead of a single\\nquestion . Unless otherwise stated, our programs default to\\ngreedy decoding during the D EMONSTRATE stage.\\nForSEARCH , our open-domain QA program uses the ques-\\ntion directly for retrieving k= 7 passages and concate-\\nnates these passages into our QA prompt with CoT. For\\nPREDICT , it generates n= 20 reasoning chains and uses\\nself-consistency (SC; Wang et al. 2022c) to select its ﬁnal\\nprediction. For DEMONSTRATE , our open-domain QA pro-\\ngram uses the following approach, slightly simpliﬁed for\\npresentation. In it, the parameter k= 3passed to annotate\\nrequests annotating only three demonstrations, which will\\nthen be used in the prompts.\\n1def openqa_demonstrate (x: Example ) -> Example :\\n2 demos = sample (x.train , k =16)\\n3\\n4 def openqa_attempt (d: Example ) -> Example :\\n5 d. demos = all_but (demos , d) # all ( raw )\\nexamples different from d\\n6\\n7 d = openqa_search (d, k =2)\\n8 if not passage_match (d): return None # skip\\nexamples where search fails\\n9\\n10 d = openqa_predict (d, sc= False )\\n11 if not answer_match (d): return None # skip\\nexamples where predict fails\\n12\\n13 return d\\n14\\n15 x. demos = annotate (demos , openqa_attempt , k =3)\\n16 return x\\nOur multi-hop program adopts a very similar approach for\\nDEMONSTRATE andPREDICT . For SEARCH , it uses the\\napproach described in §2.4, with the following adjustments.\\nIt uses result fusion across n= 10 queries per hop and,\\namong the npredictions, uses the summary corresponding\\nto the largest average log-probability. It uses a ﬁxed number\\nof hops for HotPotQA, i.e., two hops. In each prompt (i.e.,each hop and QA), it concatenates the summaries of all\\nprevious hops (i.e., hop 1 onwards) and a total of k= 5\\npassages divided between the hops (i.e., ﬁve passages from\\nthe ﬁrst hop or two passages from the ﬁrst and three from\\nthe second).\\nFor conversational QA, we use a simple PREDICT which\\ngenerates a response with greedy decoding, conditioned\\non all of the previous turns of the conversation and ﬁve\\nretrieved passages. For SEARCH , our conversational QA\\npipeline generates n= 10 re-written queries (and also uses\\nthe simple query as the retrieve-and-read baseline; §3.3) and\\nfuses them as in §2.4. We implement DEMONSTRATE simi-\\nlar to openqa_demonstrate , but sample only four examples\\n(i.e., four conversational turns; instead of 16 questions as in\\nopen-domain QA) for demonstrating the task for the higher-\\norder transformation convqa_attempt , which is passed to\\nannotate (not shown for brevity).\\n1def convqa_attempt (d: Example ) -> Example :\\n2 d. demos = all_but (demos , d) # all ( raw )\\nexamples that don /quotesingle.Vart intersect with the\\nconversation of d\\n3\\n4 d = convqa_search (d, k =2)\\n5 if max ( precision (d. answer , p) for p in\\nd. passages ) < .8: return None # skip examples\\nwhere search fails\\n6', metadata={'source': './returning-structured-output-langchain/rag-intro-paper.pdf', 'page': 9}),\n",
       " Document(page_content='conversation of d\\n3\\n4 d = convqa_search (d, k =2)\\n5 if max ( precision (d. answer , p) for p in\\nd. passages ) < .8: return None # skip examples\\nwhere search fails\\n6\\n7 d = convqa_predict (d, n =20)\\n8 if max (F1(c.pred , d. answer ) for c in\\nd. candidates ) < .75: return None # skip\\nexamples where predict fails out of n =20\\nattempts\\n9\\n10 return d\\n3.5. Development Datasets & Results\\nOpen-SQuAD We conduct the open-domain version of\\nSQuAD over the Wikipedia 2016 corpus from Chen et al.\\n(2017), as processed by Khattab et al. (2021b). We use the', metadata={'source': './returning-structured-output-langchain/rag-intro-paper.pdf', 'page': 9}),\n",
       " Document(page_content='DEMONSTRATE –SEARCH –PREDICT : Composing retrieval and language models\\nsame train/validation/test splits as in Karpukhin et al. (2020)\\nand Khattab et al. (2021b).\\nTable 1 reports the answer EM and F1. The task-aware DSP\\nprogram achieves 36.6% EM, outperforming the vanilla LM\\nbaseline by 126% EM relative gains. This indicates the im-\\nportance of grounding the LM’s predictions in retrieval, and\\nit shows that state-of-the-art retrievers like ColBERTv2 have\\nthe capacity to do so off-the-shelf. The proposed DSP pro-\\ngram also achieves relative gains of 8% in EM and 6% in F1\\nover the retrieve-then-read pipeline, highlighting that non-\\ntrivial gains are possible by aggregating information across\\nseveral retrieved passages as we do with self-consistency.\\nThese in-context learning results are competitive with a\\nnumber of popular ﬁne-tuned systems. For instance, on\\nthe Open-SQuAD test set, DPR achieves 29.8% EM, well\\nbelow our 16-shot DSP program. On the Open-SQuAD\\ndev set, the powerful Fusion-in-Decoder (Izacard & Grave,\\n2020) “base” approach achieves approximately 36% (i.e.,\\nvery similar quality to our system) when invoked with ﬁve\\nretrieved passages. Nonetheless, with the default setting\\nof reading 100 passages, their system reaches 48% EM in\\nthis evaluation. This may indicate that similar gains are\\npossible for our DSP program if the PREDICT stage is made\\nto aggregate information across many more passages.\\nFor comparison, we also evaluate the self-ask pipeline,\\nwhich achieves 9.3% EM, suggesting that its ﬁxed pipeline\\nis ineffective outside its default multi-hop setting. Study-\\ning a few examples of its errors reveals that it often de-\\ncomposes questions in tangential ways and answers these\\nquestions instead. We refer to this behavior of the LMas\\n“self-distraction”, and we believe it adds evidence in favor of\\nour design decisions in DSP . To illustrate self-distraction,\\nwhen self-ask is prompted with “When does The Kidnap-\\nping of Edgardo Mortara take place?”, it asks “What is The\\nKidnapping of Edgardo Mortara“ and then asks when it was\\npublished, a tangential question. Thus, self-ask answers\\n“1997”, instead of the time The Kidnapping of Edgardo\\nMortara takes place (1858).\\nFor reference, Table 1 also reports (as No-retrieval LM\\nSoTA) the concurrent in-context learning results from Si\\net al. (2022) using code-davinci-002 , who achieve 20.2%\\nEM without retrieval and 34.0% EM with retrieval, albeit\\non a different sample and split of the SQuAD data. Overall,\\ntheir approaches are very similar to the baselines we im-\\nplement (vanilla LM and retrieve-then-read), though their\\nretrieval-augmented approach retrieves (and concatenates\\ninto the prompt) 10 passages from a Wikipedia dump.\\nHotPotQA We use the open-domain “fullwiki” setting\\nof HotPotQA using its ofﬁcial Wikipedia 2017 “abstracts”\\ncorpus. The HotPotQA test set is hidden, so we reserve\\nthe ofﬁcial validation set for our testing. We sub-dividethe training set into 90%/10% train/validation splits. In the\\ntraining (and thus validation) split, we keep only examples\\nmarked as “hard” in the original dataset, which matches the\\ndesignation of the ofﬁcial validation and test sets.\\nWe report the ﬁnal answer EM and F1 in Table 1. On\\nHotPotQA, the task-aware DSP program outperforms the\\nbaselines and existing work by very wide margins, exceed-\\ning the vanilla LM, the retrieve-then-read baseline, and the\\nself-ask pipeline by 82%, 39%, and 80%, respectively, in\\nEM. This highlights the effectiveness of building up more\\nsophisticated programs that coordinate the LM andRM for\\nthe S EARCH step.\\nThese results may be pegged against the evaluation on Hot-\\nPotQA in a number of concurrent papers. We ﬁrst compare\\nwith non-retrieval approaches, though our comparisons must\\nbe tentative due to variation in evaluation methodologies. Si\\net al. (2022) achieve 25.2% EM with CoT prompting. With\\na “recite-and-answer” technique for PaLM-62B (Chowdh-\\nery et al., 2022), Sun et al. (2022) achieve 26.5% EM. Wang', metadata={'source': './returning-structured-output-langchain/rag-intro-paper.pdf', 'page': 10}),\n",
       " Document(page_content='et al. (2022) achieve 25.2% EM with CoT prompting. With\\na “recite-and-answer” technique for PaLM-62B (Chowdh-\\nery et al., 2022), Sun et al. (2022) achieve 26.5% EM. Wang\\net al. (2022b) achieve 33.8% EM and 44.6 F1 when apply-\\ning a self-consistency prompt for PaLM-540B. Next, we\\ncompare with a contemporaneous retrieval-based approach:\\nYao et al. (2022) achieve 35.1% EM using a system capable\\nof searching using a Wikipedia API. All of these approaches\\ntrail our task-aware DSP program, which achieves 51.4%\\nEM, by large margins.\\nQReCC We use QReCC (Anantha et al., 2020) in an open-\\ndomain setting over Wikipedia 2018. QReCC does not have\\nan ofﬁcial development set, so we sub-divide the training\\nset into 90%/10% train/validation splits. For the ﬁrst ques-\\ntion in every conversation, we use the rewritten question\\nas the original question often assumes access to a ground-\\ntruth document. We also ﬁlter low-quality examples from\\nQReCC.3\\nWe conduct the QReCC conversations in an auto-regressive\\nmanner. At turn t > 1of a particular conversation, the\\nsystem sees its own responses (i.e., not the ground truth\\nresponses) to previous turns of the conversation. We report\\nthe novel-F1 metric (nF1; Paranjape et al. 2022), which\\ncomputes the F1 overlap between the system response and\\nthe ground truth while discounting common stopwords and\\nterms present in the question (or earlier questions). The\\nresults are shown in Table 1, and follow the same general\\npattern as SQuAD and HotPotQA.\\n3We remove conversations that have one or more empty ground-\\ntruth answers and conversations that have only one or two ques-\\ntions. We also ﬁnd many conversations that include “what other\\ninteresting facts are in this article?”, which conﬂict with the open-\\ndomain formulation and have no well-deﬁned answer. Hence, we\\nremove any conversation that includes the keywords “other inter-\\nesting” or “else”, which we found to be markers of low quality.', metadata={'source': './returning-structured-output-langchain/rag-intro-paper.pdf', 'page': 10}),\n",
       " Document(page_content='DEMONSTRATE –SEARCH –PREDICT : Composing retrieval and language models\\n4. Conclusion\\nFor a long time, the dominant paradigm for building models\\nin AI has centered around multiplication of tensor repre-\\nsentations, and in the deep learning era this has given rise\\nto highly modular (layer-wise) designs that allow for fast\\ndevelopment and wide exploration. However, these design\\nparadigms require extensive domain expertise, and even\\nexperts face substantial challenges when it comes to com-\\nbining different pretrained components into larger systems.\\nThe promise of in-context learning is that we can build com-\\nplex systems from pretrained components using only natural\\nlanguage as the medium for giving systems instructions and,\\nas we argue for, allowing components to communicate with\\neach other. In this new paradigm, the building blocks are\\npretrained models and the core operations are natural lan-\\nguage instructions and operations on natural language texts.\\nIf we can realize this potential, then we can broaden partici-\\npation in AI system development, rapidly prototype systems\\nfor new domains, and maximize the value of specialized\\npretrained components.\\nIn the current paper, we introduced the DEMONSTRATE –\\nSEARCH –PREDICT (DSP ) framework for retrieval aug-\\nmented in-context learning. DSP consists of a number of\\nsimple, composable functions for implementing in-context\\nlearning systems as deliberate programs —instead of end-\\ntask prompts—for solving knowledge intensive tasks. We\\nimplemented DSP as a Python library and used it to write\\nprograms for Open-SQuAD, HotPotQA, and QReCC. These\\nprograms deliver substantial gains over previous in-context\\nlearning approaches. However, beyond any particular per-\\nformance number, we argue that the central contribution of\\nDSP is in helping to reveal a very large space of conceptual\\npossibilities for in-context learning in general.\\nAcknowledgements\\nWe thank Ashwin Paranjape, Amir Ziai, and Rick Battle for\\nvaluable discussions and feedback. This work was partially\\nsupported by IBM as a founding member of the Stanford\\nInstitute for Human-Centered Artiﬁcial Intelligence (HAI).\\nThis research was supported in part by afﬁliate members and\\nother supporters of the Stanford DAWN project—Ant Fi-\\nnancial, Facebook, Google, and VMware—as well as Cisco,\\nSAP, and the NSF under CAREER grant CNS-1651570.\\nAny opinions, ﬁndings, and conclusions or recommenda-\\ntions expressed in this material are those of the authors and\\ndo not necessarily reﬂect the views of the National Science\\nFoundation. We thank Giuseppe Attanasio for his public\\nLATEX GitHub-style Python code formatting gist.4We also\\nthank Riley Goodside for his public tips on formatting LM\\n4https://gist.github.com/g8a9/\\n07c2be12ae02cfad4aa430d77dc940cbprompts (at @goodside on Twitter).\\nReferences\\nAnantha, R., Vakulenko, S., Tu, Z., Longpre, S., Pulman,\\nS., and Chappidi, S. Open-domain question answering\\ngoes conversational via question rewriting. arXiv preprint\\narXiv:2010.04898 , 2020.\\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D.,\\nDhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,\\nAskell, A., et al. Language models are few-shot learners.\\nAdvances in neural information processing systems , 33:\\n1877–1901, 2020.\\nCao, G., Nie, J.-Y ., Gao, J., and Robertson, S. Selecting\\ngood expansion terms for pseudo-relevance feedback. In\\nProceedings of the 31st annual international ACM SIGIR\\nconference on Research and development in information\\nretrieval , pp. 243–250, 2008.\\nChen, D., Fisch, A., Weston, J., and Bordes, A. Reading\\nWikipedia to answer open-domain questions. In Proceed-\\nings of the 55th Annual Meeting of the Association for\\nComputational Linguistics (Volume 1: Long Papers) , pp.\\n1870–1879, Vancouver, Canada, 2017. Association for\\nComputational Linguistics. doi: 10.18653/v1/P17-1171.\\nURL https://aclanthology.org/P17-1171 .\\nChowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra,\\nG., Roberts, A., Barham, P., Chung, H. W., Sutton, C.,', metadata={'source': './returning-structured-output-langchain/rag-intro-paper.pdf', 'page': 11}),\n",
       " Document(page_content='URL https://aclanthology.org/P17-1171 .\\nChowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra,\\nG., Roberts, A., Barham, P., Chung, H. W., Sutton, C.,\\nGehrmann, S., et al. Palm: Scaling language modeling\\nwith pathways. arXiv preprint arXiv:2204.02311 , 2022.\\nDel Tredici, M., Barlacchi, G., Shen, X., Cheng, W., and\\nde Gispert, A. Question rewriting for open-domain con-\\nversational qa: Best practices and limitations. In Pro-\\nceedings of the 30th ACM International Conference on\\nInformation & Knowledge Management , pp. 2974–2978,\\n2021.\\nDohan, D., Xu, W., Lewkowycz, A., Austin, J., Bieber, D.,\\nLopes, R. G., Wu, Y ., Michalewski, H., Saurous, R. A.,\\nSohl-Dickstein, J., et al. Language model cascades. arXiv\\npreprint arXiv:2207.10342 , 2022.\\nFox, E. A. and Shaw, J. A. Combination of multiple searches.\\nNIST special publication SP , 243, 1994.\\nGao, L., Dai, Z., Pasupat, P., Chen, A., Chaganty, A. T., Fan,\\nY ., Zhao, V . Y ., Lao, N., Lee, H., Juan, D.-C., et al. At-\\ntributed text generation via post-hoc research and revision.\\narXiv preprint arXiv:2210.08726 , 2022.\\nGeva, M., Khashabi, D., Segal, E., Khot, T., Roth, D., and\\nBerant, J. Did aristotle use a laptop? a question answering', metadata={'source': './returning-structured-output-langchain/rag-intro-paper.pdf', 'page': 11}),\n",
       " Document(page_content='DEMONSTRATE –SEARCH –PREDICT : Composing retrieval and language models\\nbenchmark with implicit reasoning strategies. Transac-\\ntions of the Association for Computational Linguistics , 9:\\n346–361, 2021.\\nHofstätter, S., Chen, J., Raman, K., and Zamani, H. Fid-\\nlight: Efﬁcient and effective retrieval-augmented text\\ngeneration. arXiv preprint arXiv:2209.14290 , 2022.\\nHuang, J., Gu, S. S., Hou, L., Wu, Y ., Wang, X., Yu, H., and\\nHan, J. Large language models can self-improve. arXiv\\npreprint arXiv:2210.11610 , 2022.\\nIshii, Y ., Madotto, A., and Fung, P. Survey of hallucination\\nin natural language generation. ACM Comput. Surv , 1(1),\\n2022.\\nIzacard, G. and Grave, E. Leveraging passage retrieval with\\ngenerative models for open domain question answering.\\narXiv preprint arXiv:2007.01282 , 2020.\\nIzacard, G., Lewis, P., Lomeli, M., Hosseini, L., Petroni,\\nF., Schick, T., Dwivedi-Yu, J., Joulin, A., Riedel, S., and\\nGrave, E. Few-shot learning with retrieval augmented lan-\\nguage models. arXiv preprint arXiv:2208.03299 , 2022.\\nJiang, Y ., Bordia, S., Zhong, Z., Dognin, C., Singh, M.,\\nand Bansal, M. HoVer: A dataset for many-hop fact\\nextraction and claim veriﬁcation. In Findings of the\\nAssociation for Computational Linguistics: EMNLP\\n2020 , pp. 3441–3460, Online, 2020. Association for\\nComputational Linguistics. doi: 10.18653/v1/2020.\\nﬁndings-emnlp.309. URL https://aclanthology.\\norg/2020.findings-emnlp.309 .\\nKarpukhin, V ., Oguz, B., Min, S., Lewis, P., Wu, L., Edunov,\\nS., Chen, D., and Yih, W.-t. Dense passage retrieval\\nfor open-domain question answering. In Proceedings\\nof the 2020 Conference on Empirical Methods in Nat-\\nural Language Processing (EMNLP) , pp. 6769–6781,\\nOnline, 2020. Association for Computational Linguis-\\ntics. doi: 10.18653/v1/2020.emnlp-main.550. URL\\nhttps://aclanthology.org/2020.emnlp-main.550 .\\nKhattab, O. and Zaharia, M. Colbert: Efﬁcient and effective\\npassage search via contextualized late interaction over\\nBERT. In Huang, J., Chang, Y ., Cheng, X., Kamps, J.,\\nMurdock, V ., Wen, J., and Liu, Y . (eds.), Proceedings\\nof the 43rd International ACM SIGIR conference on re-\\nsearch and development in Information Retrieval, SIGIR\\n2020, Virtual Event, China, July 25-30, 2020 , pp. 39–\\n48. ACM, 2020. doi: 10.1145/3397271.3401075. URL\\nhttps://doi.org/10.1145/3397271.3401075 .\\nKhattab, O., Potts, C., and Zaharia, M. Baleen: Robust\\nMulti-Hop Reasoning at Scale via Condensed Retrieval.\\nInThirty-Fifth Conference on Neural Information Pro-\\ncessing Systems , 2021a.Khattab, O., Potts, C., and Zaharia, M. Relevance-guided\\nsupervision for openqa with ColBERT. Transactions of\\nthe Association for Computational Linguistics , 9:929–\\n944, 2021b.\\nKhot, T., Trivedi, H., Finlayson, M., Fu, Y ., Richardson, K.,\\nClark, P., and Sabharwal, A. Decomposed prompting:\\nA modular approach for solving complex tasks. arXiv\\npreprint arXiv:2210.02406 , 2022.\\nKojima, T., Gu, S. S., Reid, M., Matsuo, Y ., and Iwasawa,\\nY . Large language models are zero-shot reasoners. arXiv\\npreprint arXiv:2205.11916 , 2022.\\nKrishna, K., Chang, Y ., Wieting, J., and Iyyer, M. Rankgen:\\nImproving text generation with large ranking models.\\narXiv preprint arXiv:2205.09726 , 2022.\\nKurland, O. and Culpepper, J. S. Fusion in information\\nretrieval: Sigir 2018 half-day tutorial. In The 41st Inter-\\nnational ACM SIGIR Conference on Research & Devel-\\nopment in Information Retrieval , pp. 1383–1386, 2018.\\nKwiatkowski, T., Palomaki, J., Redﬁeld, O., Collins, M.,\\nParikh, A., Alberti, C., Epstein, D., Polosukhin, I., Devlin,\\nJ., Lee, K., Toutanova, K., Jones, L., Kelcey, M., Chang,\\nM.-W., Dai, A. M., Uszkoreit, J., Le, Q., and Petrov, S.\\nNatural questions: A benchmark for question answering\\nresearch. Transactions of the Association for Computa-\\ntional Linguistics , 7:452–466, 2019. doi: 10.1162/tacl_a_\\n00276. URL https://aclanthology.org/Q19-1026 .\\nLazaridou, A., Gribovskaya, E., Stokowiec, W., and Grig-\\norev, N. Internet-augmented language models through', metadata={'source': './returning-structured-output-langchain/rag-intro-paper.pdf', 'page': 12}),\n",
       " Document(page_content='00276. URL https://aclanthology.org/Q19-1026 .\\nLazaridou, A., Gribovskaya, E., Stokowiec, W., and Grig-\\norev, N. Internet-augmented language models through\\nfew-shot prompting for open-domain question answering.\\narXiv preprint arXiv:2203.05115 , 2022.\\nLe, N. T., Bai, F., and Ritter, A. Few-shot anaphora reso-\\nlution in scientiﬁc protocols via mixtures of in-context\\nexperts. arXiv preprint arXiv:2210.03690 , 2022.\\nLee, K., Chang, M.-W., and Toutanova, K. Latent re-\\ntrieval for weakly supervised open domain question an-\\nswering. In Proceedings of the 57th Annual Meeting of\\nthe Association for Computational Linguistics , pp. 6086–\\n6096, Florence, Italy, 2019. Association for Computa-\\ntional Linguistics. doi: 10.18653/v1/P19-1612. URL\\nhttps://aclanthology.org/P19-1612 .\\nLevine, Y ., Dalmedigos, I., Ram, O., Zeldes, Y ., Jan-\\nnai, D., Muhlgay, D., Osin, Y ., Lieber, O., Lenz, B.,\\nShalev-Shwartz, S., et al. Standing on the shoul-\\nders of giant frozen language models. arXiv preprint\\narXiv:2204.10019 , 2022.\\nLewis, P. S. H., Perez, E., Piktus, A., Petroni, F., Karpukhin,\\nV ., Goyal, N., Küttler, H., Lewis, M., Yih, W., Rock-\\ntäschel, T., Riedel, S., and Kiela, D. Retrieval-Augmented', metadata={'source': './returning-structured-output-langchain/rag-intro-paper.pdf', 'page': 12}),\n",
       " Document(page_content='DEMONSTRATE –SEARCH –PREDICT : Composing retrieval and language models\\nGeneration for Knowledge-Intensive NLP Tasks. In\\nLarochelle, H., Ranzato, M., Hadsell, R., Balcan, M.,\\nand Lin, H. (eds.), Advances in Neural Information\\nProcessing Systems 33: Annual Conference on Neural\\nInformation Processing Systems 2020, NeurIPS 2020,\\nDecember 6-12, 2020, virtual , 2020. URL https:\\n//proceedings.neurips.cc/paper/2020/hash/\\n6b493230205f780e1bc26945df7481e5-Abstract.\\nhtml .\\nLi, X. L., Holtzman, A., Fried, D., Liang, P., Eisner, J.,\\nHashimoto, T., Zettlemoyer, L., and Lewis, M. Con-\\ntrastive decoding: Open-ended text generation as opti-\\nmization. arXiv preprint arXiv:2210.15097 , 2022.\\nLiu, J., Shen, D., Zhang, Y ., Dolan, B., Carin, L., and Chen,\\nW. What makes good in-context examples for gpt- 3?\\narXiv preprint arXiv:2101.06804 , 2021.\\nMcCann, B., Keskar, N. S., Xiong, C., and Socher, R.\\nThe natural language decathlon: Multitask learning as\\nquestion answering. arXiv:1806.08730, 2018. URL\\nhttps://arxiv.org/abs/1806.08730 .\\nMin, S., Zhong, V ., Zettlemoyer, L., and Hajishirzi,\\nH. Multi-hop reading comprehension through ques-\\ntion decomposition and rescoring. arXiv preprint\\narXiv:1906.02916 , 2019.\\nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright,\\nC. L., Mishkin, P., Zhang, C., Agarwal, S., Slama,\\nK., Ray, A., et al. Training language models to fol-\\nlow instructions with human feedback. arXiv preprint\\narXiv:2203.02155 , 2022.\\nParanjape, A., Khattab, O., Potts, C., Zaharia, M., and\\nManning, C. D. Hindsight: Posterior-guided Training\\nof Retrievers for Improved Open-ended Generation. In\\nInternational Conference on Learning Representations ,\\n2022. URL https://openreview.net/forum?id=Vr_\\nBTpw3wz .\\nPerez, E., Kiela, D., and Cho, K. True few-shot learning\\nwith language models. Advances in Neural Information\\nProcessing Systems , 34:11054–11070, 2021.\\nPress, O., Zhang, M., Min, S., Schmidt, L., Smith, N. A.,\\nand Lewis, M. Measuring and narrowing the com-\\npositionality gap in language models. arXiv preprint\\narXiv:2210.03350 , 2022.\\nQi, P., Lee, H., Sido, O., Manning, C. D., et al. Retrieve,\\nrerank, read, then iterate: Answering open-domain ques-\\ntions of arbitrary complexity from text. arXiv preprint\\narXiv:2010.12527 , 2020. URL https://arxiv.org/\\nabs/2010.12527 .Radford, A., Wu, J., Child, R., Luan, D., Amodei, D.,\\nSutskever, I., et al. Language models are unsupervised\\nmultitask learners. OpenAI blog , 1(8):9, 2019.\\nRajpurkar, P., Zhang, J., Lopyrev, K., and Liang, P. SQuAD:\\n100,000+ questions for machine comprehension of text.\\nInProceedings of the 2016 Conference on Empirical\\nMethods in Natural Language Processing , pp. 2383–\\n2392, Austin, Texas, 2016. Association for Computa-\\ntional Linguistics. doi: 10.18653/v1/D16-1264. URL\\nhttps://aclanthology.org/D16-1264 .\\nRaposo, G., Ribeiro, R., Martins, B., and Coheur, L. Ques-\\ntion rewriting? assessing its importance for conversa-\\ntional question answering. In European Conference on\\nInformation Retrieval , pp. 199–206. Springer, 2022.\\nSanthanam, K., Khattab, O., Potts, C., and Zaharia, M.\\nPLAID: An Efﬁcient Engine for Late Interaction Re-\\ntrieval. arXiv preprint arXiv:2205.09707 , 2022a.\\nSanthanam, K., Khattab, O., Saad-Falcon, J., Potts, C.,\\nand Zaharia, M. ColBERTv2: Effective and efﬁcient\\nretrieval via lightweight late interaction. In Proceedings\\nof the 2022 Conference of the North American Chapter of\\nthe Association for Computational Linguistics: Human\\nLanguage Technologies , pp. 3715–3734, Seattle, United\\nStates, July 2022b. Association for Computational Lin-\\nguistics. doi: 10.18653/v1/2022.naacl-main.272. URL\\nhttps://aclanthology.org/2022.naacl-main.272 .\\nShuster, K., Poff, S., Chen, M., Kiela, D., and Weston, J.\\nRetrieval augmentation reduces hallucination in conver-\\nsation. arXiv preprint arXiv:2104.07567 , 2021.\\nSi, C., Gan, Z., Yang, Z., Wang, S., Wang, J., Boyd-Graber,\\nJ., and Wang, L. Prompting gpt-3 to be reliable. arXiv\\npreprint arXiv:2210.09150 , 2022.', metadata={'source': './returning-structured-output-langchain/rag-intro-paper.pdf', 'page': 13}),\n",
       " Document(page_content='sation. arXiv preprint arXiv:2104.07567 , 2021.\\nSi, C., Gan, Z., Yang, Z., Wang, S., Wang, J., Boyd-Graber,\\nJ., and Wang, L. Prompting gpt-3 to be reliable. arXiv\\npreprint arXiv:2210.09150 , 2022.\\nSun, Z., Wang, X., Tay, Y ., Yang, Y ., and Zhou, D.\\nRecitation-augmented language models. arXiv preprint\\narXiv:2210.01296 , 2022.\\nThorne, J., Vlachos, A., Christodoulopoulos, C., and Mit-\\ntal, A. FEVER: a large-scale dataset for fact extrac-\\ntion and VERiﬁcation. In Proceedings of the 2018 Con-\\nference of the North American Chapter of the Associa-\\ntion for Computational Linguistics: Human Language\\nTechnologies, Volume 1 (Long Papers) , pp. 809–819,\\nNew Orleans, Louisiana, 2018. Association for Compu-\\ntational Linguistics. doi: 10.18653/v1/N18-1074. URL\\nhttps://aclanthology.org/N18-1074 .\\nVakulenko, S., Kiesel, J., and Fröbe, M. SCAI-QReCC\\nshared task on conversational question answering. In Pro-\\nceedings of the Thirteenth Language Resources and Eval-\\nuation Conference , pp. 4913–4922, Marseille, France,', metadata={'source': './returning-structured-output-langchain/rag-intro-paper.pdf', 'page': 13}),\n",
       " Document(page_content='DEMONSTRATE –SEARCH –PREDICT : Composing retrieval and language models\\nJune 2022. European Language Resources Association.\\nURL https://aclanthology.org/2022.lrec-1.525 .\\nWang, X., Macdonald, C., Tonellotto, N., and Ounis, I.\\nColbert-prf: Semantic pseudo-relevance feedback for\\ndense passage and document retrieval. ACM Transac-\\ntions on the Web , 2022a.\\nWang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., and\\nZhou, D. Rationale-augmented ensembles in language\\nmodels. arXiv preprint arXiv:2207.00747 , 2022b.\\nWang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E.,\\nand Zhou, D. Self-consistency improves chain of\\nthought reasoning in language models. arXiv preprint\\narXiv:2203.11171 , 2022c.\\nWei, J., Wang, X., Schuurmans, D., Bosma, M., Chi, E.,\\nLe, Q., and Zhou, D. Chain of thought prompting elic-\\nits reasoning in large language models. arXiv preprint\\narXiv:2201.11903 , 2022.\\nWiher, G., Meister, C., and Cotterell, R. On decoding\\nstrategies for neural text generators. arXiv preprint\\narXiv:2203.15721 , 2022.\\nXiong, W., Li, X. L., Iyer, S., Du, J., Lewis, P., Wang,\\nW. Y ., Mehdad, Y ., Yih, W.-t., Riedel, S., Kiela, D., et al.\\nAnswering complex open-domain questions with multi-\\nhop dense retrieval. arXiv preprint arXiv:2009.12756 ,\\n2020. URL https://arxiv.org/abs/2009.12756 .\\nXue, X. and Croft, W. B. Modeling reformulation using\\nquery distributions. ACM Transactions on Information\\nSystems (TOIS) , 31(2):1–34, 2013.\\nYang, Z., Qi, P., Zhang, S., Bengio, Y ., Cohen, W. W.,\\nSalakhutdinov, R., and Manning, C. D. Hotpotqa: A\\ndataset for diverse, explainable multi-hop question an-\\nswering. arXiv preprint arXiv:1809.09600 , 2018.\\nYao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan,\\nK., and Cao, Y . React: Synergizing reasoning and acting\\nin language models. arXiv preprint arXiv:2210.03629 ,\\n2022.\\nZelikman, E., Wu, Y ., and Goodman, N. D. Star: Boot-\\nstrapping reasoning with reasoning. arXiv preprint\\narXiv:2203.14465 , 2022.\\nZhang, Z., Zhang, A., Li, M., and Smola, A. Automatic\\nchain of thought prompting in large language models.\\narXiv preprint arXiv:2210.03493 , 2022.\\nZhong, V ., Shi, W., Yih, W.-t., and Zettlemoyer, L. Romqa:\\nA benchmark for robust, multi-evidence, multi-answer\\nquestion answering. arXiv preprint arXiv:2210.14353 ,\\n2022.', metadata={'source': './returning-structured-output-langchain/rag-intro-paper.pdf', 'page': 14})]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "pdf_path = \"./returning-structured-output-langchain/rag-intro-paper.pdf\"\n",
    "loader = PyPDFLoader(pdf_path) # LOAD\n",
    "pdf_docs = loader.load_and_split() # SPLIT\n",
    "pdf_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OpenAIEmbeddings(client=<openai.resources.embeddings.Embeddings object at 0x127088750>, async_client=<openai.resources.embeddings.AsyncEmbeddings object at 0x1270594d0>, model='text-embedding-ada-002', dimensions=None, deployment='text-embedding-ada-002', openai_api_version='', openai_api_base='https://api.openai.com/v1', openai_api_type='', openai_proxy='', embedding_ctx_length=8191, openai_api_key=SecretStr('**********'), openai_organization=None, allowed_special=set(), disallowed_special='all', chunk_size=1000, max_retries=2, request_timeout=None, headers=None, tiktoken_enabled=True, tiktoken_model_name=None, show_progress_bar=False, model_kwargs={}, skip_empty=False, default_headers=None, default_query=None, retry_min_seconds=4, retry_max_seconds=20, http_client=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings() # EMBED\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['Chroma', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x12707fc50>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "vectordb = Chroma.from_documents(pdf_docs, embedding=embeddings) # STORE\n",
    "vectordb\n",
    "retriever = vectordb.as_retriever()\n",
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RetrievalQA(combine_documents_chain=StuffDocumentsChain(llm_chain=LLMChain(prompt=ChatPromptTemplate(input_variables=['context', 'question'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], template=\"Use the following pieces of context to answer the user's question. \\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\\n----------------\\n{context}\")), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], template='{question}'))]), llm=ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x127daca10>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x147770050>, temperature=1e-08, groq_api_key=SecretStr('**********'))), document_prompt=PromptTemplate(input_variables=['page_content'], template='Context:\\n{page_content}'), document_variable_name='context'), retriever=VectorStoreRetriever(tags=['Chroma', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x12707fc50>))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "llm = ChatGroq(model=\"mixtral-8x7b-32768\", temperature=0.0)\n",
    "pdf_qa = RetrievalQA.from_llm(llm=llm, retriever=retriever) # RETRIEVE\n",
    "pdf_qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'What is a RAG?',\n",
       " 'result': 'A RAG (Retrieval-Augmented Generation) model is a type of language model that combines a retriever module and a generator module to perform a task. The retriever module retrieves relevant documents or passages from a large corpus of text, and the generator module then uses this context to generate a response. This approach allows the model to access a much larger amount of information than its internal parameters alone, which can improve its ability to perform knowledge-intensive tasks. RAG models have been shown to be effective for a variety of tasks, including question answering and dialogue systems.'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_qa.invoke(\"What is a RAG?\") # ANSWER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'What are the steps described in the paper for using RAG Write it in bullet points?',\n",
       " 'result': 'Based on the provided context, the paper does not provide detailed steps for using RAG (Retrieval-augmented generation) in a bullet point format. However, I can provide you with a general idea of how RAG works based on the information given in the context:\\n1. Given a question or a prompt, the Retrieval Model (RM) retrieves relevant passages from a large corpus.\\n2. The Retrieved passages are then passed as context to the Language Model (LM).\\n3. The LM generates a response based on the given question and the context provided by the retrieved passages.\\n4. The generated response is then returned as the final answer.\\n\\nIt\\'s important to note that the context mainly discusses the limitations of the \"retrieve-then-read\" approach and the potential benefits of more sophisticated interactions between the LM and RM. The paper \"DEMONSTRATE –SEARCH –PREDICT : Composing retrieval and language models for knowledge-intensive NLP\" proposes a new framework called DSP (DEMONSTRATE –SEARCH –PREDICT) that goes beyond the \"retrieve-then-read\" approach. However, it does not provide a detailed step-by-step guide on using RAG.'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_qa.invoke(\"What are the steps described in the paper for using RAG Write it in bullet points?\") # ANSWER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'What is the DSP framework?',\n",
       " 'result': 'The DSP (DEMONSTRATE –SEARCH –PREDICT) framework is a general approach for tackling complex problems effectively by combining the use of a language model (LM) and a retrieval model (RM) in a deliberate, task-aware manner. It is designed to deliver large empirical gains over existing in-context learning methods.\\n\\nThe DSP framework consists of three main stages:\\n\\n1. DEMONSTRATE: In this stage, the LM is used to generate exemplar queries that illustrate how to produce queries for questions in the training set. The RM is used to retrieve relevant passages that help the LM adapt to the domain and task.\\n\\n2. SEARCH: In this stage, the LM generates intermediate \"hop\" queries to find useful information for the input question. The RM retrieves the top-k most relevant text sequences for a given query.\\n\\n3. PREDICT: In this final stage, the LM generates the final answer to the input question based on the information gathered in the previous stages.\\n\\nThe DSP framework is implemented in Python and provides core data types and composable functions that allow for the expression of complex interactions between the LM and RM in simple programs. It has been shown to deliver strong quality and considerable empirical gains over vanilla in-context learning and other pipelines for open-domain, multi-hop, and conversational question answering tasks.'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_qa.invoke(\"What is the DSP framework?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oreilly-agents",
   "language": "python",
   "name": "oreilly-agents"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
