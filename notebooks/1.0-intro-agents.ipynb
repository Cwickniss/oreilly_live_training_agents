{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T12:08:35.796831Z",
     "iopub.status.busy": "2024-07-18T12:08:35.796449Z",
     "iopub.status.idle": "2024-07-18T12:08:35.800239Z",
     "shell.execute_reply": "2024-07-18T12:08:35.799812Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why Agents?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is an agent?\n",
    "\n",
    "An agent is a reference to combining models that can perform some kind of reasoning, like large language models (e.g ChatGPT, Llama2, Mistral, etc...) with tools to give it access to the real world,\n",
    "so they can do things like browsing the internet, buying stuff, etc..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so, why is there so much hype around agents right now?\n",
    "\n",
    "Because Agents are cool! Recently with the advance of LLMs, we've seen them become an amazing tool to do all sorts of things like building apps, browse the internet and more.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SOme neat examples of these kinds of agents can be found in here:\n",
    "\n",
    "- [AutoGPT](https://github.com/Significant-Gravitas/AutoGPT)\n",
    "- [GPT-Engineer](https://github.com/gpt-engineer-org/gpt-engineer)\n",
    "- [BabyAGI](https://github.com/yoheinakajima/babyagi)\n",
    "\n",
    "Now, today although they seem extremely powerful, agents are still at a very early stage in terms of readiness to deploy as products, something you can atest by listening to Andrej Karpathy talk about agents in this talk here:\n",
    "\n",
    "- [Karpathy on Agents](https://www.youtube.com/watch?v=fqVLjtvWgq8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This live-training is all about! Getting you excited about this amazing new technology, understanding it from the ground up but with a focus on practical applications and fun stuff you can do with them! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is an Agent?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An agent is nothing more than some entity that can _think_ and _act_, that's right, in a way you're an agent! \n",
    "\n",
    "After all you can think and act on those thoughts like in the case of coming to this live-training:\n",
    "\n",
    "- Thought: \"I want to learn about agents\"\n",
    "\n",
    "- Action: \"Go to the internet and research cool platforms where I can learn about agents\"\n",
    "\n",
    "- Thought: \"O'Reilly has some awesome courses and live-trainings\"\n",
    "\n",
    "- Action: \"Look up O'Reilly courses\"\n",
    "\n",
    "- Thought: \"Live-trainings by instructor Lucas are awesome\"\n",
    "\n",
    "- Action: \"Schedule live-training about agents with instructor Lucas Soares\" (lol)\n",
    "\n",
    "In a way this is a simplified rendition of what brought you here, obviously not necessarily in this particular order nor these particular sets of thought and action pairs. This particular way of thinking about how to structure thoughts and actions is well represented in the paper: [ReACT](https://arxiv.org/pdf/2210.03629.pdf). \n",
    "\n",
    "With regards to LLMs, how can bring this idea to fruition thinking about the LLM model as the reasoning and thinking engine?\n",
    "\n",
    "We can start simple and just call the openai API to start:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T12:08:35.803091Z",
     "iopub.status.busy": "2024-07-18T12:08:35.802947Z",
     "iopub.status.idle": "2024-07-18T12:08:35.804996Z",
     "shell.execute_reply": "2024-07-18T12:08:35.804702Z"
    }
   },
   "outputs": [],
   "source": [
    "# uncomment this if running locally\n",
    "#!pip install python-dotenv\n",
    "# from dotenv import load_dotenv\n",
    "\n",
    "# load_dotenv()\n",
    "\n",
    "# Or if you are in Colab, uncoment below and add your api key\n",
    "# import os\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"your-api-key\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T12:08:35.806946Z",
     "iopub.status.busy": "2024-07-18T12:08:35.806803Z",
     "iopub.status.idle": "2024-07-18T12:08:39.443941Z",
     "shell.execute_reply": "2024-07-18T12:08:39.443376Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Sure! Here are three simple desktop tasks that you can do on the terminal:\n",
       "\n",
       "1. Check available system resources: Open the terminal and type in the command \"top\" to see a real-time display of system resource usage. This will show you CPU usage, memory usage, and other system statistics.\n",
       "\n",
       "2. Create a new folder: To create a new folder on your desktop, navigate to the desktop directory using the \"cd\" command (e.g., \"cd Desktop\") and then use the \"mkdir\" command followed by the name of the folder you want to create (e.g., \"mkdir new_folder\").\n",
       "\n",
       "3. List files and folders on your desktop: Use the \"ls\" command to list the files and folders on your desktop. Simply navigate to the desktop directory (e.g., \"cd Desktop\") and then run the command \"ls\" to see a list of all the files and folders on your desktop.\n",
       "\n",
       "These tasks are simple yet useful for managing your desktop environment using the terminal."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from IPython.display import Markdown\n",
    "client = OpenAI()\n",
    "\n",
    "def get_response(prompt_question):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo-16k\",\n",
    "        messages=[{\"role\": \"system\", \"content\": \"You are a helpful research and programming assistant\"},\n",
    "                  {\"role\": \"user\", \"content\": prompt_question}]\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "output = get_response(\"Create a simple task list of 3 desktop things I can do on the terminal.\")\n",
    "Markdown(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok cool, so here we have three ideas of actions to perform:\n",
    "\n",
    "- Creating directories\n",
    "- Listing files\n",
    "- Removing files\n",
    "\n",
    "Let's transform them into functions that we could call just like in any type of Python-based application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T12:08:39.470329Z",
     "iopub.status.busy": "2024-07-18T12:08:39.470185Z",
     "iopub.status.idle": "2024-07-18T12:08:39.472679Z",
     "shell.execute_reply": "2024-07-18T12:08:39.472378Z"
    }
   },
   "outputs": [],
   "source": [
    "# NOTE: I changed the functions in these examples from the slides (which use subprocess), but they do the same thing\n",
    "# I am just using the os module here so people can test this on google colab\n",
    "\n",
    "import os\n",
    "\n",
    "def create_directory():\n",
    "    os.makedirs('test', exist_ok=True)\n",
    "\n",
    "def create_file():\n",
    "    with open('test.txt', 'w'):\n",
    "        pass\n",
    "\n",
    "def list_files():\n",
    "    files = os.listdir()\n",
    "    for file in files:\n",
    "        print(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that here we are not concerned with giving these functions arguments to avoid complicating things, the only thing we should focus on is to have the ability to perform the actions outside the narrow scope of the python script out there in the real-world.\n",
    "\n",
    "Let's test these functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T12:08:39.474251Z",
     "iopub.status.busy": "2024-07-18T12:08:39.474159Z",
     "iopub.status.idle": "2024-07-18T12:08:39.593464Z",
     "shell.execute_reply": "2024-07-18T12:08:39.593138Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0-intro-agents.ipynb\r\n",
      "1.1-intro-agents-openai-functions.ipynb\r\n",
      "1.2-intro-langchain.ipynb\r\n",
      "1.3-intro-agents-with-langchain.ipynb\r\n",
      "2.0-building-llm-agents-with-langchain.ipynb\r\n",
      "2024-03-14-19-26-00.png\r\n",
      "3.0-langchain-github-agent-prototype.ipynb\r\n",
      "4.0-building-a-simple-research-agent.ipynb\r\n",
      "5.0-langchain-deploy-agent.ipynb\r\n",
      "5.1-langchain-deploy-chat-with-website.ipynb\r\n",
      "6.0-demo-research-reporting-agent.ipynb\r\n",
      "6.1-simple-personal-assistant-agent.ipynb\r\n",
      "7.0-langgraph-walkthrough.ipynb\r\n",
      "\u001b[1m\u001b[36magent-deploy\u001b[m\u001b[m\r\n",
      "\u001b[1m\u001b[36massets-resources\u001b[m\u001b[m\r\n",
      "file.py\r\n",
      "function_calls_chatgpt.py\r\n",
      "jira-agent.py\r\n",
      "langgraph-simple-example.html\r\n",
      "\u001b[1m\u001b[36mlucas-agent-007\u001b[m\u001b[m\r\n",
      "\u001b[1m\u001b[36mlucas-agent-007-the-return-of-pancake-man\u001b[m\u001b[m\r\n",
      "\u001b[1m\u001b[36mlucas-loves-pancakes\u001b[m\u001b[m\r\n",
      "\u001b[1m\u001b[36mlucas-still-loves-pancakes\u001b[m\u001b[m\r\n",
      "\u001b[1m\u001b[36mlucas-test\u001b[m\u001b[m\r\n",
      "\u001b[1m\u001b[36mlucas-the-agent-master\u001b[m\u001b[m\r\n",
      "mdpdf.log\r\n",
      "\u001b[1m\u001b[36mpancakes-are-better-than-waffles\u001b[m\u001b[m\r\n",
      "\u001b[1m\u001b[36mpancakes-rule\u001b[m\u001b[m\r\n",
      "pancakes_for_the_win.md\r\n",
      "paper.pdf\r\n",
      "print_file.py\r\n",
      "research-assistant.py\r\n",
      "research_report.pdf\r\n",
      "tasks-for-lucas.txt\r\n",
      "\u001b[1m\u001b[36mtest\u001b[m\u001b[m\r\n",
      "test.txt\r\n",
      "token-limits.ipynb\r\n",
      "waffles_for_the_win.md\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, we start with this folder containing a few notebooks, scripts and sub-directories.\n",
    "\n",
    "Let's test each function and inspect its output behavior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T12:08:39.595243Z",
     "iopub.status.busy": "2024-07-18T12:08:39.595156Z",
     "iopub.status.idle": "2024-07-18T12:08:39.718593Z",
     "shell.execute_reply": "2024-07-18T12:08:39.714786Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0-intro-agents.ipynb\r\n",
      "1.1-intro-agents-openai-functions.ipynb\r\n",
      "1.2-intro-langchain.ipynb\r\n",
      "1.3-intro-agents-with-langchain.ipynb\r\n",
      "2.0-building-llm-agents-with-langchain.ipynb\r\n",
      "2024-03-14-19-26-00.png\r\n",
      "3.0-langchain-github-agent-prototype.ipynb\r\n",
      "4.0-building-a-simple-research-agent.ipynb\r\n",
      "5.0-langchain-deploy-agent.ipynb\r\n",
      "5.1-langchain-deploy-chat-with-website.ipynb\r\n",
      "6.0-demo-research-reporting-agent.ipynb\r\n",
      "6.1-simple-personal-assistant-agent.ipynb\r\n",
      "7.0-langgraph-walkthrough.ipynb\r\n",
      "\u001b[1m\u001b[36magent-deploy\u001b[m\u001b[m\r\n",
      "\u001b[1m\u001b[36massets-resources\u001b[m\u001b[m\r\n",
      "file.py\r\n",
      "function_calls_chatgpt.py\r\n",
      "jira-agent.py\r\n",
      "langgraph-simple-example.html\r\n",
      "\u001b[1m\u001b[36mlucas-agent-007\u001b[m\u001b[m\r\n",
      "\u001b[1m\u001b[36mlucas-agent-007-the-return-of-pancake-man\u001b[m\u001b[m\r\n",
      "\u001b[1m\u001b[36mlucas-loves-pancakes\u001b[m\u001b[m\r\n",
      "\u001b[1m\u001b[36mlucas-still-loves-pancakes\u001b[m\u001b[m\r\n",
      "\u001b[1m\u001b[36mlucas-test\u001b[m\u001b[m\r\n",
      "\u001b[1m\u001b[36mlucas-the-agent-master\u001b[m\u001b[m\r\n",
      "mdpdf.log\r\n",
      "\u001b[1m\u001b[36mpancakes-are-better-than-waffles\u001b[m\u001b[m\r\n",
      "\u001b[1m\u001b[36mpancakes-rule\u001b[m\u001b[m\r\n",
      "pancakes_for_the_win.md\r\n",
      "paper.pdf\r\n",
      "print_file.py\r\n",
      "research-assistant.py\r\n",
      "research_report.pdf\r\n",
      "tasks-for-lucas.txt\r\n",
      "\u001b[1m\u001b[36mtest\u001b[m\u001b[m\r\n",
      "test.txt\r\n",
      "token-limits.ipynb\r\n",
      "waffles_for_the_win.md\r\n"
     ]
    }
   ],
   "source": [
    "create_directory()\n",
    "\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T12:08:39.726418Z",
     "iopub.status.busy": "2024-07-18T12:08:39.725452Z",
     "iopub.status.idle": "2024-07-18T12:08:39.853912Z",
     "shell.execute_reply": "2024-07-18T12:08:39.852473Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0-intro-agents.ipynb\r\n",
      "1.1-intro-agents-openai-functions.ipynb\r\n",
      "1.2-intro-langchain.ipynb\r\n",
      "1.3-intro-agents-with-langchain.ipynb\r\n",
      "2.0-building-llm-agents-with-langchain.ipynb\r\n",
      "2024-03-14-19-26-00.png\r\n",
      "3.0-langchain-github-agent-prototype.ipynb\r\n",
      "4.0-building-a-simple-research-agent.ipynb\r\n",
      "5.0-langchain-deploy-agent.ipynb\r\n",
      "5.1-langchain-deploy-chat-with-website.ipynb\r\n",
      "6.0-demo-research-reporting-agent.ipynb\r\n",
      "6.1-simple-personal-assistant-agent.ipynb\r\n",
      "7.0-langgraph-walkthrough.ipynb\r\n",
      "\u001b[1m\u001b[36magent-deploy\u001b[m\u001b[m\r\n",
      "\u001b[1m\u001b[36massets-resources\u001b[m\u001b[m\r\n",
      "file.py\r\n",
      "function_calls_chatgpt.py\r\n",
      "jira-agent.py\r\n",
      "langgraph-simple-example.html\r\n",
      "\u001b[1m\u001b[36mlucas-agent-007\u001b[m\u001b[m\r\n",
      "\u001b[1m\u001b[36mlucas-agent-007-the-return-of-pancake-man\u001b[m\u001b[m\r\n",
      "\u001b[1m\u001b[36mlucas-loves-pancakes\u001b[m\u001b[m\r\n",
      "\u001b[1m\u001b[36mlucas-still-loves-pancakes\u001b[m\u001b[m\r\n",
      "\u001b[1m\u001b[36mlucas-test\u001b[m\u001b[m\r\n",
      "\u001b[1m\u001b[36mlucas-the-agent-master\u001b[m\u001b[m\r\n",
      "mdpdf.log\r\n",
      "\u001b[1m\u001b[36mpancakes-are-better-than-waffles\u001b[m\u001b[m\r\n",
      "\u001b[1m\u001b[36mpancakes-rule\u001b[m\u001b[m\r\n",
      "pancakes_for_the_win.md\r\n",
      "paper.pdf\r\n",
      "print_file.py\r\n",
      "research-assistant.py\r\n",
      "research_report.pdf\r\n",
      "tasks-for-lucas.txt\r\n",
      "\u001b[1m\u001b[36mtest\u001b[m\u001b[m\r\n",
      "test.txt\r\n",
      "token-limits.ipynb\r\n",
      "waffles_for_the_win.md\r\n"
     ]
    }
   ],
   "source": [
    "create_file()\n",
    "\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T12:08:39.861742Z",
     "iopub.status.busy": "2024-07-18T12:08:39.861440Z",
     "iopub.status.idle": "2024-07-18T12:08:39.865145Z",
     "shell.execute_reply": "2024-07-18T12:08:39.864760Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pancakes-rule\n",
      "5.0-langchain-deploy-agent.ipynb\n",
      "research-assistant.py\n",
      "print_file.py\n",
      "3.0-langchain-github-agent-prototype.ipynb\n",
      "paper.pdf\n",
      ".DS_Store\n",
      "test\n",
      "mdpdf.log\n",
      "6.1-simple-personal-assistant-agent.ipynb\n",
      "pancakes_for_the_win.md\n",
      "lucas-loves-pancakes\n",
      "function_calls_chatgpt.py\n",
      "token-limits.ipynb\n",
      "file.py\n",
      "2.0-building-llm-agents-with-langchain.ipynb\n",
      ".gitignore\n",
      "pancakes-are-better-than-waffles\n",
      "6.0-demo-research-reporting-agent.ipynb\n",
      "lucas-the-agent-master\n",
      "lucas-test\n",
      "langgraph-simple-example.html\n",
      "7.0-langgraph-walkthrough.ipynb\n",
      "test.txt\n",
      "1.2-intro-langchain.ipynb\n",
      "lucas-agent-007\n",
      "1.1-intro-agents-openai-functions.ipynb\n",
      "assets-resources\n",
      "1.3-intro-agents-with-langchain.ipynb\n",
      "lucas-agent-007-the-return-of-pancake-man\n",
      "1.0-intro-agents.ipynb\n",
      "agent-deploy\n",
      "4.0-building-a-simple-research-agent.ipynb\n",
      "jira-agent.py\n",
      "research_report.pdf\n",
      "waffles_for_the_win.md\n",
      "tasks-for-lucas.txt\n",
      "lucas-still-loves-pancakes\n",
      "2024-03-14-19-26-00.png\n",
      "5.1-langchain-deploy-chat-with-website.ipynb\n"
     ]
    }
   ],
   "source": [
    "list_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok great all the functions are working!\n",
    "\n",
    "Now, let's imagine that we wanted to create an agent that would perform these actions for us based on some input that we give it, how can we connect models that we know and can use today like ChatGPT, with these tools that do stuff in the real world?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To answer this question, how about we give a task to the model, and for that task we ask it to list the steps that it needs to perform to complete the task, and then for each of those steps we would ask the model to decide whether or not a function should be called to execute that task? \n",
    "\n",
    "This is what the now famous paper ['Toolformer'](https://arxiv.org/pdf/2302.04761.pdf) demonstrated!\n",
    "\n",
    "They showed that today's advanced LLMs like the gpt-series could teacha themselves how to properly call and use external tools!\n",
    "\n",
    "Isn't that awesome???\n",
    "\n",
    "So, let's see if we can hack our way into connecting the llm response with the functions that we want that llm to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T12:08:39.867318Z",
     "iopub.status.busy": "2024-07-18T12:08:39.867186Z",
     "iopub.status.idle": "2024-07-18T12:08:39.870755Z",
     "shell.execute_reply": "2024-07-18T12:08:39.870429Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_response(prompt_question):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo-1106\",\n",
    "        messages=[{\"role\": \"system\", \"content\": \"You are a helpful research and programming assistant\"},\n",
    "                {\"role\": \"user\", \"content\": prompt_question}]\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "def create_directory(dir_name):\n",
    "    os.makedirs(dir_name, exist_ok=True)\n",
    "\n",
    "def create_file(filename):\n",
    "    with open(filename, 'w'):\n",
    "        pass\n",
    "\n",
    "def list_files():\n",
    "    files = os.listdir()\n",
    "    for file in files:\n",
    "        print(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, cool! now Notice that, here we added single parameters to the functions: `create_directory(), create_file()`, and we did this so\n",
    "that we can actually do real things instead of just always creating the same folders over and over.\n",
    "\n",
    "Now, how can we actually put it all together so that given a task, a model can:\n",
    "\n",
    "- Plan the task\n",
    "- Execute actions to complete the task\n",
    "- Know when to call a function\n",
    "\n",
    "????\n",
    "\n",
    "This is actually an interesting problem, let's understand why is that the case by trying to hack our way into putting all of these together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T12:08:39.873018Z",
     "iopub.status.busy": "2024-07-18T12:08:39.872898Z",
     "iopub.status.idle": "2024-07-18T12:08:40.945824Z",
     "shell.execute_reply": "2024-07-18T12:08:40.944916Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "create_directory('lucas-the-agent-master')"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_response(prompt_question, model=\"gpt-3.5-turbo-16k\"):\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"system\", \"content\": \"You are a helpful research and programming assistant\"},\n",
    "                {\"role\": \"user\", \"content\": prompt_question}]\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def create_directory(dir_name):\n",
    "    os.makedirs(dir_name, exist_ok=True)\n",
    "\n",
    "def create_file(filename):\n",
    "    with open(filename, 'w'):\n",
    "        pass\n",
    "\n",
    "def list_files():\n",
    "    files = os.listdir()\n",
    "    for file in files:\n",
    "        print(file)\n",
    "\n",
    "    \n",
    "\n",
    "task_description = \"Create a folder called 'lucas-the-agent-master'. Inside that folder, create a file called 'the-10-master-rules.md\"\n",
    "output = get_response(f\"\"\"Given this task: {task_description}, \\n\n",
    "                            Consider you have access to the following functions:\n",
    "                            \n",
    "    def create_directory(dir_name):\n",
    "        os.makedirs(dir_name, exist_ok=True)\n",
    "\n",
    "    def create_file():\n",
    "        with open('test.txt', 'w'):\n",
    "            pass\n",
    "\n",
    "    def list_files():\n",
    "        files = os.listdir()\n",
    "        for file in files:\n",
    "            print(file)\n",
    "    \n",
    "    Your output should be the first function to be executed to complete the task containing the necessary arguments.\n",
    "    The OUTPUT SHOULD ONLY BE THE PYTHON FUNCTION CALL and NOTHING ELSE.\n",
    "    \"\"\")\n",
    "\n",
    "Markdown(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hey! Look at that the output is that function! Now, all we need is find a way to execute this function. We can use Python's built in `exec` method for that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T12:08:40.950229Z",
     "iopub.status.busy": "2024-07-18T12:08:40.950056Z",
     "iopub.status.idle": "2024-07-18T12:08:40.952787Z",
     "shell.execute_reply": "2024-07-18T12:08:40.952335Z"
    }
   },
   "outputs": [],
   "source": [
    "exec(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T12:08:40.954864Z",
     "iopub.status.busy": "2024-07-18T12:08:40.954701Z",
     "iopub.status.idle": "2024-07-18T12:08:41.088247Z",
     "shell.execute_reply": "2024-07-18T12:08:41.083799Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[36mlucas-agent-007-the-return-of-pancake-man/\u001b[m\u001b[m\r\n",
      "\u001b[1m\u001b[36mlucas-agent-007/\u001b[m\u001b[m\r\n",
      "\u001b[1m\u001b[36mlucas-loves-pancakes/\u001b[m\u001b[m\r\n",
      "\u001b[1m\u001b[36mlucas-still-loves-pancakes/\u001b[m\u001b[m\r\n",
      "\u001b[1m\u001b[36mlucas-test/\u001b[m\u001b[m\r\n",
      "\u001b[1m\u001b[36mlucas-the-agent-master/\u001b[m\u001b[m\r\n"
     ]
    }
   ],
   "source": [
    "!ls -d */ | grep lucas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yessss! We did it! All we had to do is to use the Python builtin method `exec` connected with the function call we got from the model's response!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is great, but what if we wanted to perform multiple actions?\n",
    "\n",
    "How about changing our prompt so that our output is a python list of function calls which we can later programmatically call?\n",
    "\n",
    "Let's try that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T12:08:41.091992Z",
     "iopub.status.busy": "2024-07-18T12:08:41.091777Z",
     "iopub.status.idle": "2024-07-18T12:08:42.385588Z",
     "shell.execute_reply": "2024-07-18T12:08:42.384291Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "[create_directory('lucas-the-agent-master'), create_file('lucas-the-agent-master/the-10-master-rules.md')]"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_description = \"Create a folder called 'lucas-the-agent-master'. Inside that folder create a file called 'the-10-master-rules.md'.\"\n",
    "output = get_response(f\"\"\"Given a task that will be fed as input, and consider you have access to the following functions:\n",
    "                            \n",
    "    def create_directory(dir_name):\n",
    "        os.makedirs(dir_name, exist_ok=True)\n",
    "\n",
    "    def create_file(filename):\n",
    "        with open(filename, 'w'):\n",
    "            pass\n",
    "\n",
    "    def list_files():\n",
    "        files = os.listdir()\n",
    "        for file in files:\n",
    "            print(file)\n",
    "    .\n",
    "    Your output should be the a list of function calls to be executed to complete the task containing the necessary arguments.\n",
    "    For example:\n",
    "    \n",
    "    task: 'create a folder named test-dir'\n",
    "    output_list: [create_directory('test-dir')]\n",
    "    \n",
    "    task: 'create a file named file.txt'\n",
    "    output_list: [create_file('file.txt')]\n",
    "    \n",
    "    task: 'Create a folder named lucas-dir and inside that folder create a file named lucas-file.txt'\n",
    "    output_list: [create_directory('lucas-dir'), create_file('lucas-dir/lucas-file.txt')]\n",
    "    \n",
    "    The OUTPUT SHOULD ONLY BE A PYTHON LIST WITH THE FUNCTION CALLS INSIDE and NOTHING ELSE.\n",
    "    task: {task_description}\n",
    "    output_list:\\n\n",
    "    \"\"\")\n",
    "\n",
    "Markdown(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T12:08:42.389999Z",
     "iopub.status.busy": "2024-07-18T12:08:42.389383Z",
     "iopub.status.idle": "2024-07-18T12:08:42.392802Z",
     "shell.execute_reply": "2024-07-18T12:08:42.392293Z"
    }
   },
   "outputs": [],
   "source": [
    "exec(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T12:08:42.394963Z",
     "iopub.status.busy": "2024-07-18T12:08:42.394823Z",
     "iopub.status.idle": "2024-07-18T12:08:42.518684Z",
     "shell.execute_reply": "2024-07-18T12:08:42.517282Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the-10-master-rules 2.md the-10-master-rules.md\r\n"
     ]
    }
   ],
   "source": [
    "!ls lucas-the-agent-master/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we can start identifying a lot of issues with this approach despite our early sucess:\n",
    "\n",
    "- Uncertainty of model's outputs can affect our ability to reliably call the functions\n",
    "- We need more structured ways to prepare the inputs of the function calls\n",
    "- We need better ways to put everything together (just feeding the entire functions like this makes it a very clunky and non-scalable framework for more complex cases)\n",
    "\n",
    "There are many more issues but starting with these, we can now look at frameworks and see how they fix these issues and with that in mind understand what is behind their implementations!\n",
    "\n",
    "I personally think this is a much better way to understand what is going on behind agents in practice rather than just use the more higher level frameworks right of the bat!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "- [HuggingGPT](https://github.com/microsoft/JARVIS)\n",
    "- [Gen Agents](https://arxiv.org/pdf/2304.03442.pdf)\n",
    "- [WebGPT](https://www.semanticscholar.org/paper/WebGPT%3A-Browser-assisted-question-answering-with-Nakano-Hilton/2f3efe44083af91cef562c1a3451eee2f8601d22)\n",
    "- [LangChain](https://python.langchain.com/docs/get_started/introduction)\n",
    "- [OpenAI](https://openai.com/)\n",
    "- [OpenAI Function Calling](https://platform.openai.com/docs/guides/function-calling)\n",
    "- [AutoGPT](https://github.com/Significant-Gravitas/AutoGPT)\n",
    "- [GPT-Engineer](https://github.com/gpt-engineer-org/gpt-engineer)\n",
    "- [BabyAGI](https://github.com/yoheinakajima/babyagi)\n",
    "- [Karpathy on Agents](https://www.youtube.com/watch?v=fqVLjtvWgq8)\n",
    "- [ReACT Paper](https://arxiv.org/abs/2210.03629)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oreilly-agents",
   "language": "python",
   "name": "oreilly-agents"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
