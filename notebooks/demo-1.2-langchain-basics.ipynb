{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi folks!\n"
     ]
    }
   ],
   "source": [
    "print(\"Hi folks!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Sure, here's a joke for you:\\n\\nWhy don't scientists trust atoms?\\n\\nBecause they make up everything!\", response_metadata={'token_usage': {'completion_tokens': 20, 'prompt_tokens': 13, 'total_tokens': 33}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_c4e5b6fa31', 'finish_reason': 'stop', 'logprobs': None}, id='run-67cf5474-ed34-46d2-8fc1-5515f4003c31-0', usage_metadata={'input_tokens': 13, 'output_tokens': 20, 'total_tokens': 33})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0) # super precise best performing model\n",
    "\n",
    "llm.invoke(\"Hi! Tell me a joke\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\"Hi! Tell me a joke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Sure, here's a joke for you:\\n\\nWhy don't scientists trust atoms?\\n\\nBecause they make up everything!\", response_metadata={'token_usage': {'completion_tokens': 20, 'prompt_tokens': 13, 'total_tokens': 33}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_c4e5b6fa31', 'finish_reason': 'stop', 'logprobs': None}, id='run-fd8cda35-bf24-49b1-9af6-b3dd49658cba-0', usage_metadata={'input_tokens': 13, 'output_tokens': 20, 'total_tokens': 33})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | llm\n",
    "\n",
    "chain.invoke({\"input\": \"\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human: Give me 5 ideas of names for a company that makes cars'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\"Give me 5 ideas of names for a company that makes {product}\")\n",
    "\n",
    "prompt.format(product=\"cars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Sure! Here are five creative names for a company that makes mustard:\\n\\n1. **Golden Grain Mustard Co.**\\n2. **Zesty Harvest Mustards**\\n3. **Mustard Maven**\\n4. **Savory Seed Mustard**\\n5. **Tangy Tradition Mustards**\\n\\nEach of these names aims to evoke the quality, flavor, and heritage associated with mustard production.', response_metadata={'token_usage': {'completion_tokens': 77, 'prompt_tokens': 21, 'total_tokens': 98}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_c4e5b6fa31', 'finish_reason': 'stop', 'logprobs': None}, id='run-f6d54b0a-97c0-45c1-9ec5-259e8f03b518-0', usage_metadata={'input_tokens': 21, 'output_tokens': 77, 'total_tokens': 98})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_naming_companies = prompt | llm\n",
    "\n",
    "\n",
    "chain_naming_companies.invoke({\"product\": \"Mustard\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The capital of France is Paris.', response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 23, 'total_tokens': 30}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_c4e5b6fa31', 'finish_reason': 'stop', 'logprobs': None}, id='run-204585ea-349d-4615-b4e5-98a8c56d4ab5-0', usage_metadata={'input_tokens': 23, 'output_tokens': 7, 'total_tokens': 30})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a brilliant researcher\"),\n",
    "        (\"human\", \"{input}\")\n",
    "    ])\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "chain.invoke({\"input\": \"What is the capital of France?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The capital of France is Paris.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | llm | output_parser\n",
    "\n",
    "chain.invoke({\"input\": \"What is the capital of France?\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oreilly-agents",
   "language": "python",
   "name": "oreilly-agents"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
