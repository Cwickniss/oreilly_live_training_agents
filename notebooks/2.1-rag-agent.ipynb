{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This example was taken from the langchain docs:\n",
    "# https://python.langchain.com/v0.2/docs/tutorials/rag/\n",
    "# %pip install --upgrade --quiet  langchain langchain-community langchainhub langchain-openai faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"./assets-resources/papers/tree-of-thoughts.pdf\")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree of Thoughts: Deliberate Problem Solving\n",
      "with Large Language Models\n",
      "Shunyu Yao\n",
      "Princeton UniversityDian Yu\n",
      "Google DeepMindJeffrey Zhao\n",
      "Google DeepMindIzhak Shafran\n",
      "Google DeepMind\n",
      "Thomas L. Grifﬁths\n",
      "Princeton UniversityYuan Cao\n",
      "Google DeepMindKarthik Narasimhan\n",
      "Princeton University\n",
      "Abstract\n",
      "Language models are increasingly being deployed for general problem solving\n",
      "across a wide range of tasks\n",
      "GĮŔũƜ\n",
      "jũƜŔũƜGĮŔũƜ\n",
      "jũƜŔũƜʱÊʲˤGjʱæʲˤ\u001dĵÉGĮŔũƜ\n",
      "ˤjũƜŔũƜʱçʲˤ\u001dĵÉˁ\u001dʟʟʟʟaÊĠĵŗƓŤƆˤſĵŤòGĮŔũƜ\n",
      "ˤjũƜŔũƜʱíʲˤÉĵÉˤʱĵũŗŝʲʟʟʟʟʟʟˤˤʝˤƛĎĵũĈĎƜ)L[\u0003FRORU\u0003\u000bE\\\u0003<XTLDQ\f0DUN\u0003GLIIHUHQFH\u0003E\\\u0003FRORU\n",
      "GĮŔũƜ\n",
      "jũƜŔũƜGĮŔũƜ\n",
      "jũƜŔũƜGĮŔũƜ\n",
      "ˤjũƜŔũƜʱçʲˤòĦƙˤ\u001dĵĮŝƓŝŤòĮçƆˤƀƓƜĎˤ\u001dĵÉˤʱ\u001dĵÉˁ\u001dʲaÊĠĵŗƓŤƆˤſĵŤòGĮŔũƜ\n",
      "ˤjũƜŔũƜʱíʲˤÉŗòòˤĵƙˤĎĵũĈĎŤŝˤʱÉĵÉʲʟʟʟʟʟʟʟʟʟʟˤˤƛĎĵũĈĎƜ\n",
      "ʱçʲˤ\u001dĎÊđĮˤĵƙˤĎĵũĈĎƜˤŗĵĭŔƜđĮĈˤʱ\u001dĵÉʲʱÊʲˤGĮŔũƜˁjũƜŔũƜˤŗĵĭŔƜđĮĈˤʱGjʲFigure 1: Schematic\n",
      "Chain-of-thought (CoT) prompting [35] was proposed to address cases where the mapping of\n",
      "inputxto outputyis non-trivial (e.g. when xis a math question and yis the ﬁnal numerical answer).\n",
      "The key idea is to introduce a chain of thoughtsz1,···,znto bridgexandy, where each ziis a\n",
      "coherent language sequence that serves as a meaningful intermediate step toward problem solving\n",
      "(e.g.zicould be an interme\n",
      "learned (e.g. AlphaGo [ 26]). We propose a third alternative, by using the LM to deliberately reason\n",
      "about states. When applicable, such a deliberate heuristic can be more ﬂexible than programmed\n",
      "rules, and more sample-efﬁcient than learned models. Similar to the thought generator, we consider\n",
      "two strategies to evaluate states either independently or together:\n",
      "(a)Value each state independently: V(\n",
      "Game of 24 Creative Writing 5x5 Crosswords\n",
      "Input 4 numbers (4 9 10 13) 4 random sentences 10 clues (h1. presented;..)\n",
      "Output An equation to reach 24\n",
      "(13-9)*(10-4)=24A passage of 4 paragraphs\n",
      "ending in the 4 sentences5x5 letters: SHOWN;\n",
      "WIRRA; A V AIL; ...\n",
      "Thoughts 3 intermediate equations\n",
      "(13-9=4 (left 4,4,10); 10-\n",
      "4=6 (left 4,6); 4*6=24)A short writing plan\n",
      "(1. Introduce a book that\n",
      "connects...)W\n",
      "Method Success\n",
      "IO prompt 7.3%\n",
      "CoT prompt 4.0%\n",
      "CoT-SC (k=100) 9.0%\n",
      "ToT (ours) (b=1) 45%\n",
      "ToT (ours) (b=5) 74%\n",
      "IO + Reﬁne (k=10) 27%\n",
      "IO(best of 100) 33%\n",
      "CoT (best of 100) 49%\n",
      "Table 2: Game of 24 Results.\n",
      "0 25 50 75 1000.20.40.6(a) Success rate with nodes visited\n",
      "IO (best of k)\n",
      "CoT (best of k)\n",
      "ToT (b=1...5)\n",
      "1 2 3 4Correct0.00.20.40.6(b) Samples failed at each step\n",
      "CoT\n",
      "ToT (b=5) Figure 3: Game of 24 (a\n",
      "µŗƓŤòˤÊˤçĵĎòŗòĮƜˤŔÊŝŝÊĈòˤĵƙˤʁˤŝĎĵŗƜˤŔÊŗÊĈŗÊŔĎŝʛˤĎòˤòĮíˤŝòĮŤòĮçòˤĵƙˤòÊçĎˤŔÊŗÊĈŗÊŔĎˤĭũŝƜˤæòʝˤɾʛˤGƜˤƓŝĮ˙ƛˤíđƙƙƓçũĦƜˤƘĵˤíĵˤÊˤĎÊĮíŝŤÊĮíˤđƙˤƆĵũˤĠũŝƜˤŝŤÊĮíˤĵĮˤƆĵũŗˤĎÊĮíŝʛˤɿʛˤGƜˤçÊũĈĎƜˤĎđĭˤĵƙƙˤĈũÊŗíˤƛĎÊƜˤŝŔÊçòˤŝĭòĦĦòíˤĵƙˤŝòÊŗòíˤŝŤòÊģʛˤʀʛˤµĎòĮˤŝĎòˤíƓíĮ˒ƛˤĦđģòˤÊˤĈũƆˤƀĎĵˤƀÊŝˤƛŗƆđĮĈˤƘĵˤŔƓçģˤĎòŗˤũŔʜˤŝĎòˤŝŤÊŗŤòíˤũŝđĮĈˤŝƓĈĮˤĦÊĮĈũÊĈòʛˤʁʛˤ)ÊçĎˤŔòŗŝĵĮˤƀĎĵˤģĮĵƀŝˤƆĵũˤĎÊŝˤÊˤíđƙćòŗòĮƜˤŔòŗçòŔƜƓĵĮˤĵƙˤƀĎĵˤƆĵũˤÊŗòʛˤˤɾʛˤGĮ\n",
      ">\u000b\n",
      "Y\u0016\u0011\u0003HORSH\n",
      "\u000f\u0003\u0016\u0011\u0015\f\u000f\u0003\u000b\n",
      "K\u0015\u0011\u0003YDOXH\n",
      "\u000f\u0003\u0015\u0011\u0013\f\u000f\u0003\u000b\n",
      "K\u0014\u0011\u0003SDUFK\n",
      "\u000f\u0003\u0014\u0011\u001c\f\u000f\u0003\u000b\n",
      "Y\u0018\u0011\u0003FRYHW\n",
      "\u000f\u0003\u0013\u0011\u0019\u0013\u0013\u0013\u0013\u0013\u0013\u0013\u0013\u0013\u0013\u0013\u0013\u0013\u0013\u0014\f\u000f\u0003\u000b\n",
      "K\u0015\u0011\u0003PHULW\n",
      "\u000f\u0003\u0013\u0011\u0017\f\u000f\u0003\u000b\n",
      "Y\u0014\u0011\u0003DOORZ\n",
      "\u000f\u0003\u0013\u0011\u0015\f\u000f\u0003\u000b\n",
      "Y\u0015\u0011\u0003JULQG\n",
      "\u000f\u0003\u0013\u0011\u0014\f\u000f\u0003\u000b\n",
      "K\u0017\u0011\u0003OHSHU\n",
      "\u000f\u0003\u0013\u0011\u0014\f@\n",
      "Y\u0016\u0011\u0003HORSH\n",
      "0XOWLSOH\u0003UXQV3DUVH\u000f\u0003ILOWHU\u0003RXW\u0003QRQ\u0010ILYH\u0010OHWWHU\u000f\u0003VFRUH\u000f\u0003DJJUHJDWH\n",
      "&KRRVH\u0003\u000bVRIW\u0003VHOI\u0010FRQVLVWHQF\\\"\f\u0014\u00110D[\u0015\u00110D[\u0003ZLWKRXW\u0003YLRODWH\u0016\u0011')6\n",
      "GĮŔũƜˤ\u001dĦũòŝĎɿʛĭĵŤĵŗĎɾʛƘÊŝģŝĎʁʛŝÊĦĵĮƘÊŝģŝƘÊŝģŝ\n",
      "ĎʁʛˤŝÊĦĵĮˤʱŝũŗòʲſʂʛˤŝŗíŗƆˤʱĦĵƀʲſʀʛˤŝƜŗđĮ\n",
      "Self-reﬂection. Using LLMs to assess the viability of their own predictions is becoming an in-\n",
      "creasingly important procedure in problem solving. [ 25,17,21] introduced the “self-reﬂection”\n",
      "mechanism, in which LMs provide feedback to their generation candidates. [ 4] improves LMs code\n",
      "generation accuracy by injecting feedback messages generated by the LM itself based on its code\n",
      "execution results.\n",
      "References\n",
      "[1]T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam,\n",
      "G. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in neural\n",
      "information processing systems , 33:1877–1901, 2020.\n",
      "[2]C. Browne, E. J. Powley, D. Whitehouse, S. M. M. Lucas, P. I. Cowling, P. Rohlfshagen,\n",
      "S. Tavener, D. P. Liebana, S. Samothrakis, and S. Colton. A su\n",
      "[20] OpenAI. Gpt-4 technical report. ArXiv , abs/2303.08774, 2023.\n",
      "[21] D. Paul, M. Ismayilzada, M. Peyrard, B. Borges, A. Bosselut, R. West, and B. Faltings. Reﬁner:\n",
      "Reasoning feedback on intermediate representations, 2023.\n",
      "[22] A. Radford, K. Narasimhan, T. Salimans, I. Sutskever, et al. Improving language understanding\n",
      "by generative pre-training. OpenAI blog , 2018.\n",
      "[23] A. Radford, J. Wu, R. C\n"
     ]
    }
   ],
   "source": [
    "for doc in documents:\n",
    "    print(doc.page_content[:400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "texts = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()\n",
    "db = FAISS.from_documents(texts, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever()\n",
    "\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "\n",
    "tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    \"tree_of_thoughts\",\n",
    "    \"Searches and returns excerpts from the tree of thoughts paper.\",\n",
    ")\n",
    "tools = [tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a helpful assistant')),\n",
       " MessagesPlaceholder(variable_name='chat_history', optional=True),\n",
       " HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}')),\n",
       " MessagesPlaceholder(variable_name='agent_scratchpad')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import hub\n",
    "\n",
    "prompt = hub.pull(\"hwchase17/openai-tools-agent\")\n",
    "prompt.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"As an AI, I don't have feelings or physical sensations, so I don't experience tiredness. How can I assist you today?\", response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 11, 'total_tokens': 37, 'prompt_tokens_details': {'cached_tokens': 0}, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_72bbfa6014', 'finish_reason': 'stop', 'logprobs': None}, id='run-c1c45aed-2d20-489e-9b7c-a94e3dcede82-0', usage_metadata={'input_tokens': 11, 'output_tokens': 26, 'total_tokens': 37})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-4o', temperature=0)\n",
    "\n",
    "llm.invoke(\"Are you tired?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor, create_openai_tools_agent\n",
    "\n",
    "agent = create_openai_tools_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Tree of Thoughts (ToT) technique is a framework designed to enhance the problem-solving capabilities of large language models (LLMs) by allowing them to explore multiple reasoning paths and make deliberate decisions. This approach generalizes the \"Chain of Thought\" (CoT) method by enabling exploration over coherent units of text, referred to as \"thoughts,\" which serve as intermediate steps toward solving a problem.\\n\\nKey aspects of the Tree of Thoughts technique include:\\n\\n1. **Deliberate Decision Making**: ToT allows language models to consider multiple reasoning paths, self-evaluate choices, and decide on the next course of action. It also incorporates lookahead and backtracking to make global choices.\\n\\n2. **Problem Framing as a Search**: Problems are framed as a search over a tree, where each node represents a partial solution. The branches of the tree correspond to different thought processes or reasoning paths.\\n\\n3. **Thought Decomposition**: The intermediate process is decomposed into thought steps, which can vary in size depending on the problem. These steps are designed to be small enough for the model to generate diverse samples but large enough to evaluate their potential toward problem-solving.\\n\\n4. **Thought Generation and Evaluation**: The framework includes strategies for generating potential thoughts from each state and heuristically evaluating these states to guide the search process.\\n\\n5. **Search Algorithms**: ToT can utilize different search algorithms, such as breadth-first search (BFS) or depth-first search (DFS), to explore the tree of thoughts.\\n\\nThe Tree of Thoughts technique has been shown to significantly improve the problem-solving abilities of language models on tasks that require non-trivial planning or search, such as the Game of 24, Creative Writing, and Mini Crosswords. It provides a way to translate classical insights about problem-solving into actionable methods for contemporary language models, enhancing their ability to tackle complex, multi-step problems.'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = agent_executor.invoke(\n",
    "    {\n",
    "        \"input\": \"what is the tree of thoughts technique?\"\n",
    "    }\n",
    ")\n",
    "\n",
    "result[\"output\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = agent_executor.invoke(\n",
    "    {\"input\": \"How did they evaluated the performance of the technique?\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The performance of the Tree of Thoughts (ToT) technique was evaluated through a series of experiments involving tasks that challenge existing language model inference methods, even with state-of-the-art models like GPT-4. The tasks included:\\n\\n1. **Game of 24**: This task required mathematical reasoning to reach a target number using a set of given numbers and operations.\\n\\n2. **Creative Writing**: This task involved generating coherent and creative text based on given prompts.\\n\\n3. **Mini Crosswords**: This task involved solving 5x5 mini crosswords, which required lexical reasoning and systematic planning.\\n\\nThe evaluation involved comparing the ToT approach with other methods such as Input-Output (IO) prompting and Chain-of-Thought (CoT) prompting. The performance metrics included success rates in terms of correct letters, words, and games for the Mini Crosswords task, as well as coherency scores for the Creative Writing task.\\n\\nThe ToT approach demonstrated superior results across these tasks by leveraging a tree of thoughts, where each thought is a coherent language sequence serving as an intermediate step toward problem-solving. The method involved generating and evaluating diverse thoughts using search algorithms like breadth-first search (BFS) and depth-first search (DFS), allowing systematic exploration with lookahead and backtracking.\\n\\nThe results showed that ToT significantly improved success rates and coherency scores compared to other methods, highlighting its effectiveness in handling complex, multi-step problem-solving tasks.'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"output\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oreilly-agents",
   "language": "python",
   "name": "oreilly-agents"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
