{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install openai>1.50.0 langchain>0.3.0 langgraph langchainhub langchain-openai langchain-community langchain-cli langchain_ollama tavily-python>=0.5.0 langchain_nomic nomic[local] langserve faiss-cpu tiktoken pypdf chroma jira google-search-results numexpr beautifulsoup4 scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research Agent with LangGraph and Ollama\n",
    "\n",
    "\n",
    "## This notebook implements a research agent that:\n",
    " 1. Generates search queries\n",
    " 2. Performs web research\n",
    " 3. Summarizes findings\n",
    " 4. Reflects on results to identify knowledge gaps\n",
    "# We'll use LangGraph for the agent's workflow and Ollama for local LLM inference.\n",
    "\n",
    "\n",
    "# 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "# os.environ[\"LANGCHAIN_API_KEY\"] = \"your-api-key\"\n",
    "# os.environ[\"TAVILY_API_KEY\"] = \"your-api-key\"\n",
    "# os.environ[\"OPENAI_API_KEY\"] = 'your-api-key'\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import Optional, Any, List\n",
    "from typing_extensions import Literal, Annotated\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_ollama import ChatOllama\n",
    "from langgraph.graph import START, END, StateGraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Configuration Model\n",
    "First, let's define our configuration using Pydantic instead of dataclasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Research topic:\n",
      "Write a quick report on the latest LLMs that came out in 2025.\n",
      "result\n",
      "content='{\\n  \"query\": \"latest large language models released in 2025\",\\n  \"aspect\": \"technology/artificial intelligence\",\\n  \"rationale\": \"The query is specific and focused on the topic of LLMs, which is relevant to the research question. The aspect is technology/artificial intelligence, as it is a key area related to the topic. The rationale is that searching for this query will provide information on the latest developments in LLMs, which is what the report aims to summarize.\"\\n}' additional_kwargs={} response_metadata={'model': 'llama3.1', 'created_at': '2025-01-29T14:38:43.174232Z', 'done': True, 'done_reason': 'stop', 'total_duration': 4588147041, 'load_duration': 565082541, 'prompt_eval_count': 129, 'prompt_eval_duration': 1791000000, 'eval_count': 104, 'eval_duration': 1916000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)} id='run-a9f94d53-4044-46fe-8041-f4cfce395a7b-0' usage_metadata={'input_tokens': 129, 'output_tokens': 104, 'total_tokens': 233}\n",
      "Query created in generate_query:  {'query': 'latest large language models released in 2025', 'aspect': 'technology/artificial intelligence', 'rationale': 'The query is specific and focused on the topic of LLMs, which is relevant to the research question. The aspect is technology/artificial intelligence, as it is a key area related to the topic. The rationale is that searching for this query will provide information on the latest developments in LLMs, which is what the report aims to summarize.'}\n",
      "Current search query\n",
      "latest large language models released in 2025\n",
      "Write a quick report on the latest LLMs that came out in 2025.\n",
      "Current search query\n",
      "What are the technical specifications (e.g., model architecture, optimizer used, batch size) and computational resources (e.g., number of GPUs, total memory) required to train and deploy the top 9 LLMs mentioned in the report?\n",
      "Write a quick report on the latest LLMs that came out in 2025.\n",
      "Current search query\n",
      "What are some common techniques used in large language models to handle ambiguous or uncertain input, and how do they prioritize context when dealing with conflicting information?\n",
      "Write a quick report on the latest LLMs that came out in 2025.\n",
      "Warning: No raw_content found for source https://openreview.net/forum?id=QXqlwHt2Kg\n",
      "Current search query\n",
      "What are the most effective techniques for training LLMs on datasets that include a wide range of ambiguous inputs, and how can these models be evaluated and improved to better handle ambiguity?\n",
      "Write a quick report on the latest LLMs that came out in 2025.\n",
      "Warning: No raw_content found for source https://hyperight.com/4-pillars-to-effective-training-of-large-language-models/\n"
     ]
    }
   ],
   "source": [
    "class Configuration(BaseModel):\n",
    "    \"\"\"The configurable fields for the research assistant.\"\"\"\n",
    "    max_web_research_loops: int = 3\n",
    "    local_llm: str = \"deepseek-r1\"\n",
    "\n",
    "    @classmethod\n",
    "    def from_runnable_config(\n",
    "        cls, config: Optional[RunnableConfig] = None\n",
    "    ) -> \"Configuration\":\n",
    "        \"\"\"Create a Configuration instance from a RunnableConfig.\"\"\"\n",
    "        configurable = (\n",
    "            config[\"configurable\"] if config and \"configurable\" in config else {}\n",
    "        )\n",
    "        values: dict[str, Any] = {\n",
    "            field: os.environ.get(field.upper(), configurable.get(field))\n",
    "            for field in cls.model_fields.keys()\n",
    "        }\n",
    "        return cls(**{k: v for k, v in values.items() if v is not None})\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 3. State Models\n",
    "# Now let's define our state models using Pydantic\n",
    "\n",
    "# %%\n",
    "class SummaryState(BaseModel):\n",
    "    \"\"\"Main state model for the research agent.\"\"\"\n",
    "    research_topic: Optional[str] = None\n",
    "    search_query: Optional[str] = None\n",
    "    web_research_results: List[str] = Field(default_factory=list)\n",
    "    sources_gathered: List[str] = Field(default_factory=list)\n",
    "    research_loop_count: int = 0\n",
    "    running_summary: Optional[str] = None\n",
    "\n",
    "    class Config:\n",
    "        arbitrary_types_allowed = True\n",
    "\n",
    "class SummaryStateInput(BaseModel):\n",
    "    \"\"\"Input state model.\"\"\"\n",
    "    research_topic: Optional[str] = None\n",
    "\n",
    "class SummaryStateOutput(BaseModel):\n",
    "    \"\"\"Output state model.\"\"\"\n",
    "    running_summary: Optional[str] = None\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 4. Utility Functions\n",
    "# Let's define our helper functions for web search and source formatting\n",
    "\n",
    "# %%\n",
    "from langsmith import traceable\n",
    "from tavily import TavilyClient\n",
    "\n",
    "def deduplicate_and_format_sources(search_response, max_tokens_per_source, include_raw_content=True):\n",
    "    \"\"\"\n",
    "    Takes either a single search response or list of responses from Tavily API and formats them.\n",
    "    Limits the raw_content to approximately max_tokens_per_source.\n",
    "    \"\"\"\n",
    "    # Convert input to list of results\n",
    "    if isinstance(search_response, dict):\n",
    "        sources_list = search_response['results']\n",
    "    elif isinstance(search_response, list):\n",
    "        sources_list = []\n",
    "        for response in search_response:\n",
    "            if isinstance(response, dict) and 'results' in response:\n",
    "                sources_list.extend(response['results'])\n",
    "            else:\n",
    "                sources_list.extend(response)\n",
    "    else:\n",
    "        raise ValueError(\"Input must be either a dict with 'results' or a list of search results\")\n",
    "    \n",
    "    # Deduplicate by URL\n",
    "    unique_sources = {}\n",
    "    for source in sources_list:\n",
    "        if source['url'] not in unique_sources:\n",
    "            unique_sources[source['url']] = source\n",
    "    \n",
    "    # Format output\n",
    "    formatted_text = \"Sources:\\n\\n\"\n",
    "    for i, source in enumerate(unique_sources.values(), 1):\n",
    "        formatted_text += f\"Source {source['title']}:\\n===\\n\"\n",
    "        formatted_text += f\"URL: {source['url']}\\n===\\n\"\n",
    "        formatted_text += f\"Most relevant content from source: {source['content']}\\n===\\n\"\n",
    "        if include_raw_content:\n",
    "            char_limit = max_tokens_per_source * 4\n",
    "            raw_content = source.get('raw_content', '')\n",
    "            if raw_content is None:\n",
    "                raw_content = ''\n",
    "                print(f\"Warning: No raw_content found for source {source['url']}\")\n",
    "            if len(raw_content) > char_limit:\n",
    "                raw_content = raw_content[:char_limit] + \"... [truncated]\"\n",
    "            formatted_text += f\"Full source content limited to {max_tokens_per_source} tokens: {raw_content}\\n\\n\"\n",
    "                \n",
    "    return formatted_text.strip()\n",
    "\n",
    "def format_sources(search_results):\n",
    "    \"\"\"Format search results into a bullet-point list of sources.\"\"\"\n",
    "    return '\\n'.join(\n",
    "        f\"* {source['title']} : {source['url']}\"\n",
    "        for source in search_results['results']\n",
    "    )\n",
    "\n",
    "@traceable\n",
    "def tavily_search(query, include_raw_content=True, max_results=3):\n",
    "    \"\"\"Search the web using the Tavily API.\"\"\"\n",
    "    tavily_client = TavilyClient()\n",
    "    return tavily_client.search(query, \n",
    "                         max_results=max_results, \n",
    "                         include_raw_content=include_raw_content)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 5. Agent Prompts\n",
    "# Define the instruction prompts for our agent's components\n",
    "\n",
    "# %%\n",
    "query_writer_instructions = \"\"\"Your goal is to generate targeted web search query.\n",
    "\n",
    "The query will gather information related to a specific topic.\n",
    "\n",
    "Topic:\n",
    "{research_topic}\n",
    "\n",
    "Return your query as a JSON object:\n",
    "{{\n",
    "    \"query\": \"string\",\n",
    "    \"aspect\": \"string\",\n",
    "    \"rationale\": \"string\"\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "summarizer_instructions = \"\"\"Your goal is to generate a high-quality summary of the web search results.\n",
    "\n",
    "When EXTENDING an existing summary:\n",
    "1. Seamlessly integrate new information without repeating what's already covered\n",
    "2. Maintain consistency with the existing content's style and depth\n",
    "3. Only add new, non-redundant information\n",
    "4. Ensure smooth transitions between existing and new content\n",
    "\n",
    "When creating a NEW summary:\n",
    "1. Highlight the most relevant information from each source\n",
    "2. Provide a concise overview of the key points related to the report topic\n",
    "3. Emphasize significant findings or insights\n",
    "4. Ensure a coherent flow of information\n",
    "\n",
    "CRITICAL REQUIREMENTS:\n",
    "- Start IMMEDIATELY with the summary content - no introductions or meta-commentary\n",
    "- Focus ONLY on factual, objective information\n",
    "- Maintain a consistent technical depth\n",
    "- Avoid redundancy and repetition\n",
    "- DO NOT use phrases like \"based on the new results\" or \"according to additional sources\"\n",
    "- DO NOT add a References or Works Cited section\n",
    "- Begin directly with the summary text\n",
    "\"\"\"\n",
    "\n",
    "reflection_instructions = \"\"\"You are an expert research assistant analyzing a summary about {research_topic}.\n",
    "\n",
    "Your tasks:\n",
    "1. Identify knowledge gaps or areas that need deeper exploration\n",
    "2. Generate a follow-up question that would help expand your understanding\n",
    "3. Focus on technical details, implementation specifics, or emerging trends that weren't fully covered\n",
    "\n",
    "Ensure the follow-up question is self-contained and includes necessary context for web search.\n",
    "\n",
    "Return your analysis as a JSON object:\n",
    "{{ \n",
    "    \"knowledge_gap\": \"string\",\n",
    "    \"follow_up_query\": \"string\"\n",
    "}}\"\"\"\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 6. Agent Nodes\n",
    "# Define the core functions that make up our agent's workflow\n",
    "\n",
    "# %%\n",
    "def generate_query(state: SummaryState, config: RunnableConfig):\n",
    "    \"\"\"Generate a query for web search\"\"\"\n",
    "    query_writer_instructions_formatted = query_writer_instructions.format(\n",
    "        research_topic=state.research_topic\n",
    "    )\n",
    "    print(\"Research topic:\")\n",
    "    print(state.research_topic)\n",
    "\n",
    "    configurable = Configuration.from_runnable_config(config)\n",
    "    llm_json_mode = ChatOllama(model=configurable.local_llm, temperature=0, format=\"json\")\n",
    "    result = llm_json_mode.invoke(\n",
    "        [SystemMessage(content=query_writer_instructions_formatted),\n",
    "        HumanMessage(content=f\"Given this research topic: {state.research_topic}, generate a query for web search, your output should contain a json with a query key:\")]\n",
    "    )\n",
    "    print(\"result\")\n",
    "    print(result)   \n",
    "    query = json.loads(result.content)\n",
    "    \n",
    "    print(\"Query created in generate_query: \", query)\n",
    "    \n",
    "    return {\"search_query\": query['query']}\n",
    "\n",
    "def web_research(state: SummaryState):\n",
    "    \"\"\"Gather information from the web\"\"\"\n",
    "    print(\"Current search query\", )\n",
    "    print(state.search_query)\n",
    "    print(state.research_topic)\n",
    "    search_results = tavily_search(state.search_query, include_raw_content=True, max_results=1)\n",
    "    search_str = deduplicate_and_format_sources(search_results, max_tokens_per_source=1000)\n",
    "    return {\n",
    "        \"sources_gathered\": [format_sources(search_results)], \n",
    "        \"research_loop_count\": state.research_loop_count + 1, \n",
    "        \"web_research_results\": [search_str]\n",
    "    }\n",
    "\n",
    "def summarize_sources(state: SummaryState, config: RunnableConfig):\n",
    "    \"\"\"Summarize the gathered sources\"\"\"\n",
    "    existing_summary = state.running_summary\n",
    "    most_recent_web_research = state.web_research_results[-1]\n",
    "\n",
    "    if existing_summary:\n",
    "        human_message_content = (\n",
    "            f\"Extend the existing summary: {existing_summary}\\n\\n\"\n",
    "            f\"Include new search results: {most_recent_web_research} \"\n",
    "            f\"That addresses the following topic: {state.research_topic}\"\n",
    "        )\n",
    "    else:\n",
    "        human_message_content = (\n",
    "            f\"Generate a summary of these search results: {most_recent_web_research} \"\n",
    "            f\"That addresses the following topic: {state.research_topic}\"\n",
    "        )\n",
    "\n",
    "    configurable = Configuration.from_runnable_config(config)\n",
    "    llm = ChatOllama(model=configurable.local_llm, temperature=0)\n",
    "    result = llm.invoke(\n",
    "        [SystemMessage(content=summarizer_instructions),\n",
    "        HumanMessage(content=human_message_content)]\n",
    "    )\n",
    "\n",
    "    running_summary = result.content\n",
    "    return {\"running_summary\": running_summary}\n",
    "\n",
    "def reflect_on_summary(state: SummaryState, config: RunnableConfig):\n",
    "    \"\"\"Reflect on the summary and generate a follow-up query\"\"\"\n",
    "    configurable = Configuration.from_runnable_config(config)\n",
    "    llm_json_mode = ChatOllama(model=configurable.local_llm, temperature=0, format=\"json\")\n",
    "    result = llm_json_mode.invoke(\n",
    "        [SystemMessage(content=reflection_instructions.format(research_topic=state.research_topic)),\n",
    "        HumanMessage(content=f\"Identify a knowledge gap and generate a follow-up web search query based on our existing knowledge: {state.running_summary}\")]\n",
    "    )   \n",
    "    follow_up_query = json.loads(result.content)\n",
    "    query = follow_up_query.get('follow_up_query')\n",
    "    \n",
    "    if not query:\n",
    "        return {\"search_query\": f\"Tell me more about {state.research_topic}\"}\n",
    "    \n",
    "    return {\"search_query\": follow_up_query['follow_up_query']}\n",
    "\n",
    "def finalize_summary(state: SummaryState):\n",
    "    \"\"\"Finalize the summary\"\"\"\n",
    "    all_sources = \"\\n\".join(source for source in state.sources_gathered)\n",
    "    final_summary = f\"## Summary\\n\\n{state.running_summary}\\n\\n### Sources:\\n{all_sources}\"\n",
    "    return {\"running_summary\": final_summary}\n",
    "\n",
    "def route_research(state: SummaryState, config: RunnableConfig) -> Literal[\"finalize_summary\", \"web_research\"]:\n",
    "    \"\"\"Route the research based on the follow-up query\"\"\"\n",
    "    configurable = Configuration.from_runnable_config(config)\n",
    "    if state.research_loop_count <= configurable.max_web_research_loops:\n",
    "        return \"web_research\"\n",
    "    else:\n",
    "        return \"finalize_summary\"\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 7. Build and Run the Agent\n",
    "# Now let's put it all together and create our research agent\n",
    "\n",
    "# %%\n",
    "# Create the graph\n",
    "builder = StateGraph(SummaryState, \n",
    "                    input=SummaryStateInput, \n",
    "                    output=SummaryStateOutput, \n",
    "                    config_schema=Configuration)\n",
    "\n",
    "# Add nodes\n",
    "builder.add_node(\"generate_query\", generate_query)\n",
    "builder.add_node(\"web_research\", web_research)\n",
    "builder.add_node(\"summarize_sources\", summarize_sources)\n",
    "builder.add_node(\"reflect_on_summary\", reflect_on_summary)\n",
    "builder.add_node(\"finalize_summary\", finalize_summary)\n",
    "\n",
    "# Add edges\n",
    "builder.add_edge(START, \"generate_query\")\n",
    "builder.add_edge(\"generate_query\", \"web_research\")\n",
    "builder.add_edge(\"web_research\", \"summarize_sources\")\n",
    "builder.add_edge(\"summarize_sources\", \"reflect_on_summary\")\n",
    "builder.add_conditional_edges(\"reflect_on_summary\", route_research)\n",
    "builder.add_edge(\"finalize_summary\", END)\n",
    "\n",
    "# Compile the graph\n",
    "graph = builder.compile()\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 8. Example Usage\n",
    "# Here's how to use the research agent\n",
    "\n",
    "# %%\n",
    "# Example research topic\n",
    "research_topic = \"Write a quick report on the latest LLMs that came out in 2025.\"\n",
    "\n",
    "# Run the research agent\n",
    "config = {\"configurable\": {\"max_web_research_loops\": 3, \"local_llm\": \"llama3.1\"}}\n",
    "result = graph.invoke(\n",
    "    {\"research_topic\": research_topic},\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Summary\n",
       "\n",
       "The top 9 large language models (LLMs) as of January 2025 include:\n",
       "\n",
       "* Command R+, boasting 104 billion parameters and an industry-leading 128,000 token context window for enhanced long-form processing and multi-turn conversation capabilities.\n",
       "* ChatGPT-4o and ChatGPT-4o mini models from OpenAI, offering significantly faster processing speeds and enhanced capabilities across text, voice, and vision.\n",
       "* Other notable LLMs include GPT, which has consistently exceeded its previous capabilities with each new release.\n",
       "\n",
       "These models have more than 175 billion parameters and a substantial context window of 128,000 tokens, making them highly efficient at processing and generating large amounts of data.\n",
       "\n",
       "In addition to these top-tier models, several other LLMs are worth mentioning:\n",
       "\n",
       "* Llama 3.3: Meta's latest LLM, designed with developers in mind, boasts a massive 70 billion parameters and excels in generating highly accurate code snippets across multiple programming languages.\n",
       "* Claude 3.5 Sonnet: Anthropic's newest LLM, optimized for safety and reliability, is an excellent choice for developers concerned about ethical AI use.\n",
       "* GPT-O1: OpenAI's cutting-edge LLM, known for its unmatched ability to understand and generate human-like code, has top-notch natural language understanding and integrates well with tools like GitHub Copilot.\n",
       "* Qwen Qwq: Alibaba Cloud's open-source solution, perfect for developers who need an adaptable tool for diverse applications, combines flexibility and scalability.\n",
       "\n",
       "These models have unique strengths and trade-offs, making them suitable for different use cases. For example:\n",
       "\n",
       "* Llama 3.3 is ideal for generating code snippets across multiple programming languages.\n",
       "* Claude 3.5 Sonnet excels in debugging and algorithm generation while being ethically aligned.\n",
       "* GPT-O1 has unmatched natural language understanding and integrates well with tools like GitHub Copilot.\n",
       "* Qwen Qwq offers a customizable, multi-modal solution that performs well across small to large-scale applications.\n",
       "\n",
       "To effectively train LLMs, researchers have identified four key pillars: Data Curation and Preprocessing, Model Architecture Design, Training Methodology, and Evaluation Metrics. These pillars are crucial in ensuring the success of LLMs in various applications.\n",
       "\n",
       "Data Curation and Preprocessing is a critical aspect of effective training, as high-quality data is essential for LLMs to learn from. This involves collecting and preprocessing large datasets that address specific tasks or domains, such as text generation or machine translation.\n",
       "\n",
       "The latest LLMs have shown significant improvements in processing and generating large amounts of data. However, they still face challenges in handling ambiguous inputs. To overcome this limitation, researchers are working on developing new techniques for aligning language models to explicitly handle ambiguity.\n",
       "\n",
       "These advancements will enable the development of more sophisticated conversational agents that can effectively process and respond to complex and ambiguous user inputs.\n",
       "\n",
       "### Sources:\n",
       "* 4 Pillars to Effective Training of Large Language Models : https://hyperight.com/4-pillars-to-effective-training-of-large-language-models/"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "Markdown(result[\"running_summary\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oreilly-agents",
   "language": "python",
   "name": "oreilly-agents"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
