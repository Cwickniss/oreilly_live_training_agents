{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research Agent with LangGraph and Ollama\n",
    "\n",
    "\n",
    "## This notebook implements a research agent that:\n",
    " 1. Generates search queries\n",
    " 2. Performs web research\n",
    " 3. Summarizes findings\n",
    " 4. Reflects on results to identify knowledge gaps\n",
    "# We'll use LangGraph for the agent's workflow and Ollama for local LLM inference.\n",
    "\n",
    "\n",
    "# 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "# os.environ[\"LANGCHAIN_API_KEY\"] = \"your-api-key\"\n",
    "# os.environ[\"TAVILY_API_KEY\"] = \"your-api-key\"\n",
    "# os.environ[\"OPENAI_API_KEY\"] = 'your-api-key'\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import Optional, Any, List\n",
    "from typing_extensions import Literal, Annotated\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_ollama import ChatOllama\n",
    "from langgraph.graph import START, END, StateGraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Configuration Model\n",
    "First, let's define our configuration using Pydantic instead of dataclasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Research topic:\n",
      "Write a quick report on the latest LLMs that came out in 2025.\n",
      "result\n",
      "content='{\"query\": \"latest large language models 2025\", \"aspect\": \"technological advancements\", \"rationale\": \"To gather information on the newest LLMs released in 2025, focusing on their capabilities, applications, and potential impact on various industries.\"}' additional_kwargs={} response_metadata={'model': 'llama3.3', 'created_at': '2025-01-29T14:30:22.693042Z', 'done': True, 'done_reason': 'stop', 'total_duration': 24745997667, 'load_duration': 818026792, 'prompt_eval_count': 129, 'prompt_eval_duration': 16830000000, 'eval_count': 56, 'eval_duration': 6798000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)} id='run-7fd57ac3-fbea-4597-bcdb-b0265690350c-0' usage_metadata={'input_tokens': 129, 'output_tokens': 56, 'total_tokens': 185}\n",
      "Query created in generate_query:  {'query': 'latest large language models 2025', 'aspect': 'technological advancements', 'rationale': 'To gather information on the newest LLMs released in 2025, focusing on their capabilities, applications, and potential impact on various industries.'}\n",
      "Current search query\n",
      "latest large language models 2025\n",
      "Write a quick report on the latest LLMs that came out in 2025.\n",
      "Current search query\n",
      "What are the key architectural components and training methodologies used in Claude 3.0 to achieve its constitutional AI goals, and how does it compare to other LLMs in terms of safety, ethics, and customization capabilities?\n",
      "Write a quick report on the latest LLMs that came out in 2025.\n",
      "Current search query\n",
      "What are the detailed architectural and algorithmic differences between traditional transformer-based LLMs and those incorporating character training like Claude 3.0, including any novel loss functions or optimization techniques used during the training process?\n",
      "Write a quick report on the latest LLMs that came out in 2025.\n"
     ]
    }
   ],
   "source": [
    "class Configuration(BaseModel):\n",
    "    \"\"\"The configurable fields for the research assistant.\"\"\"\n",
    "    max_web_research_loops: int = 3\n",
    "    local_llm: str = \"deepseek-r1\"\n",
    "\n",
    "    @classmethod\n",
    "    def from_runnable_config(\n",
    "        cls, config: Optional[RunnableConfig] = None\n",
    "    ) -> \"Configuration\":\n",
    "        \"\"\"Create a Configuration instance from a RunnableConfig.\"\"\"\n",
    "        configurable = (\n",
    "            config[\"configurable\"] if config and \"configurable\" in config else {}\n",
    "        )\n",
    "        values: dict[str, Any] = {\n",
    "            field: os.environ.get(field.upper(), configurable.get(field))\n",
    "            for field in cls.model_fields.keys()\n",
    "        }\n",
    "        return cls(**{k: v for k, v in values.items() if v is not None})\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 3. State Models\n",
    "# Now let's define our state models using Pydantic\n",
    "\n",
    "# %%\n",
    "class SummaryState(BaseModel):\n",
    "    \"\"\"Main state model for the research agent.\"\"\"\n",
    "    research_topic: Optional[str] = None\n",
    "    search_query: Optional[str] = None\n",
    "    web_research_results: List[str] = Field(default_factory=list)\n",
    "    sources_gathered: List[str] = Field(default_factory=list)\n",
    "    research_loop_count: int = 0\n",
    "    running_summary: Optional[str] = None\n",
    "\n",
    "    class Config:\n",
    "        arbitrary_types_allowed = True\n",
    "\n",
    "class SummaryStateInput(BaseModel):\n",
    "    \"\"\"Input state model.\"\"\"\n",
    "    research_topic: Optional[str] = None\n",
    "\n",
    "class SummaryStateOutput(BaseModel):\n",
    "    \"\"\"Output state model.\"\"\"\n",
    "    running_summary: Optional[str] = None\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 4. Utility Functions\n",
    "# Let's define our helper functions for web search and source formatting\n",
    "\n",
    "# %%\n",
    "from langsmith import traceable\n",
    "from tavily import TavilyClient\n",
    "\n",
    "def deduplicate_and_format_sources(search_response, max_tokens_per_source, include_raw_content=True):\n",
    "    \"\"\"\n",
    "    Takes either a single search response or list of responses from Tavily API and formats them.\n",
    "    Limits the raw_content to approximately max_tokens_per_source.\n",
    "    \"\"\"\n",
    "    # Convert input to list of results\n",
    "    if isinstance(search_response, dict):\n",
    "        sources_list = search_response['results']\n",
    "    elif isinstance(search_response, list):\n",
    "        sources_list = []\n",
    "        for response in search_response:\n",
    "            if isinstance(response, dict) and 'results' in response:\n",
    "                sources_list.extend(response['results'])\n",
    "            else:\n",
    "                sources_list.extend(response)\n",
    "    else:\n",
    "        raise ValueError(\"Input must be either a dict with 'results' or a list of search results\")\n",
    "    \n",
    "    # Deduplicate by URL\n",
    "    unique_sources = {}\n",
    "    for source in sources_list:\n",
    "        if source['url'] not in unique_sources:\n",
    "            unique_sources[source['url']] = source\n",
    "    \n",
    "    # Format output\n",
    "    formatted_text = \"Sources:\\n\\n\"\n",
    "    for i, source in enumerate(unique_sources.values(), 1):\n",
    "        formatted_text += f\"Source {source['title']}:\\n===\\n\"\n",
    "        formatted_text += f\"URL: {source['url']}\\n===\\n\"\n",
    "        formatted_text += f\"Most relevant content from source: {source['content']}\\n===\\n\"\n",
    "        if include_raw_content:\n",
    "            char_limit = max_tokens_per_source * 4\n",
    "            raw_content = source.get('raw_content', '')\n",
    "            if raw_content is None:\n",
    "                raw_content = ''\n",
    "                print(f\"Warning: No raw_content found for source {source['url']}\")\n",
    "            if len(raw_content) > char_limit:\n",
    "                raw_content = raw_content[:char_limit] + \"... [truncated]\"\n",
    "            formatted_text += f\"Full source content limited to {max_tokens_per_source} tokens: {raw_content}\\n\\n\"\n",
    "                \n",
    "    return formatted_text.strip()\n",
    "\n",
    "def format_sources(search_results):\n",
    "    \"\"\"Format search results into a bullet-point list of sources.\"\"\"\n",
    "    return '\\n'.join(\n",
    "        f\"* {source['title']} : {source['url']}\"\n",
    "        for source in search_results['results']\n",
    "    )\n",
    "\n",
    "@traceable\n",
    "def tavily_search(query, include_raw_content=True, max_results=3):\n",
    "    \"\"\"Search the web using the Tavily API.\"\"\"\n",
    "    tavily_client = TavilyClient()\n",
    "    return tavily_client.search(query, \n",
    "                         max_results=max_results, \n",
    "                         include_raw_content=include_raw_content)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 5. Agent Prompts\n",
    "# Define the instruction prompts for our agent's components\n",
    "\n",
    "# %%\n",
    "query_writer_instructions = \"\"\"Your goal is to generate targeted web search query.\n",
    "\n",
    "The query will gather information related to a specific topic.\n",
    "\n",
    "Topic:\n",
    "{research_topic}\n",
    "\n",
    "Return your query as a JSON object:\n",
    "{{\n",
    "    \"query\": \"string\",\n",
    "    \"aspect\": \"string\",\n",
    "    \"rationale\": \"string\"\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "summarizer_instructions = \"\"\"Your goal is to generate a high-quality summary of the web search results.\n",
    "\n",
    "When EXTENDING an existing summary:\n",
    "1. Seamlessly integrate new information without repeating what's already covered\n",
    "2. Maintain consistency with the existing content's style and depth\n",
    "3. Only add new, non-redundant information\n",
    "4. Ensure smooth transitions between existing and new content\n",
    "\n",
    "When creating a NEW summary:\n",
    "1. Highlight the most relevant information from each source\n",
    "2. Provide a concise overview of the key points related to the report topic\n",
    "3. Emphasize significant findings or insights\n",
    "4. Ensure a coherent flow of information\n",
    "\n",
    "CRITICAL REQUIREMENTS:\n",
    "- Start IMMEDIATELY with the summary content - no introductions or meta-commentary\n",
    "- Focus ONLY on factual, objective information\n",
    "- Maintain a consistent technical depth\n",
    "- Avoid redundancy and repetition\n",
    "- DO NOT use phrases like \"based on the new results\" or \"according to additional sources\"\n",
    "- DO NOT add a References or Works Cited section\n",
    "- Begin directly with the summary text\n",
    "\"\"\"\n",
    "\n",
    "reflection_instructions = \"\"\"You are an expert research assistant analyzing a summary about {research_topic}.\n",
    "\n",
    "Your tasks:\n",
    "1. Identify knowledge gaps or areas that need deeper exploration\n",
    "2. Generate a follow-up question that would help expand your understanding\n",
    "3. Focus on technical details, implementation specifics, or emerging trends that weren't fully covered\n",
    "\n",
    "Ensure the follow-up question is self-contained and includes necessary context for web search.\n",
    "\n",
    "Return your analysis as a JSON object:\n",
    "{{ \n",
    "    \"knowledge_gap\": \"string\",\n",
    "    \"follow_up_query\": \"string\"\n",
    "}}\"\"\"\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 6. Agent Nodes\n",
    "# Define the core functions that make up our agent's workflow\n",
    "\n",
    "# %%\n",
    "def generate_query(state: SummaryState, config: RunnableConfig):\n",
    "    \"\"\"Generate a query for web search\"\"\"\n",
    "    query_writer_instructions_formatted = query_writer_instructions.format(\n",
    "        research_topic=state.research_topic\n",
    "    )\n",
    "    print(\"Research topic:\")\n",
    "    print(state.research_topic)\n",
    "\n",
    "    configurable = Configuration.from_runnable_config(config)\n",
    "    llm_json_mode = ChatOllama(model=configurable.local_llm, temperature=0, format=\"json\")\n",
    "    result = llm_json_mode.invoke(\n",
    "        [SystemMessage(content=query_writer_instructions_formatted),\n",
    "        HumanMessage(content=f\"Given this research topic: {state.research_topic}, generate a query for web search, your output should contain a json with a query key:\")]\n",
    "    )\n",
    "    print(\"result\")\n",
    "    print(result)   \n",
    "    query = json.loads(result.content)\n",
    "    \n",
    "    print(\"Query created in generate_query: \", query)\n",
    "    \n",
    "    return {\"search_query\": query['query']}\n",
    "\n",
    "def web_research(state: SummaryState):\n",
    "    \"\"\"Gather information from the web\"\"\"\n",
    "    print(\"Current search query\", )\n",
    "    print(state.search_query)\n",
    "    print(state.research_topic)\n",
    "    search_results = tavily_search(state.search_query, include_raw_content=True, max_results=1)\n",
    "    search_str = deduplicate_and_format_sources(search_results, max_tokens_per_source=1000)\n",
    "    return {\n",
    "        \"sources_gathered\": [format_sources(search_results)], \n",
    "        \"research_loop_count\": state.research_loop_count + 1, \n",
    "        \"web_research_results\": [search_str]\n",
    "    }\n",
    "\n",
    "def summarize_sources(state: SummaryState, config: RunnableConfig):\n",
    "    \"\"\"Summarize the gathered sources\"\"\"\n",
    "    existing_summary = state.running_summary\n",
    "    most_recent_web_research = state.web_research_results[-1]\n",
    "\n",
    "    if existing_summary:\n",
    "        human_message_content = (\n",
    "            f\"Extend the existing summary: {existing_summary}\\n\\n\"\n",
    "            f\"Include new search results: {most_recent_web_research} \"\n",
    "            f\"That addresses the following topic: {state.research_topic}\"\n",
    "        )\n",
    "    else:\n",
    "        human_message_content = (\n",
    "            f\"Generate a summary of these search results: {most_recent_web_research} \"\n",
    "            f\"That addresses the following topic: {state.research_topic}\"\n",
    "        )\n",
    "\n",
    "    configurable = Configuration.from_runnable_config(config)\n",
    "    llm = ChatOllama(model=configurable.local_llm, temperature=0)\n",
    "    result = llm.invoke(\n",
    "        [SystemMessage(content=summarizer_instructions),\n",
    "        HumanMessage(content=human_message_content)]\n",
    "    )\n",
    "\n",
    "    running_summary = result.content\n",
    "    return {\"running_summary\": running_summary}\n",
    "\n",
    "def reflect_on_summary(state: SummaryState, config: RunnableConfig):\n",
    "    \"\"\"Reflect on the summary and generate a follow-up query\"\"\"\n",
    "    configurable = Configuration.from_runnable_config(config)\n",
    "    llm_json_mode = ChatOllama(model=configurable.local_llm, temperature=0, format=\"json\")\n",
    "    result = llm_json_mode.invoke(\n",
    "        [SystemMessage(content=reflection_instructions.format(research_topic=state.research_topic)),\n",
    "        HumanMessage(content=f\"Identify a knowledge gap and generate a follow-up web search query based on our existing knowledge: {state.running_summary}\")]\n",
    "    )   \n",
    "    follow_up_query = json.loads(result.content)\n",
    "    query = follow_up_query.get('follow_up_query')\n",
    "    \n",
    "    if not query:\n",
    "        return {\"search_query\": f\"Tell me more about {state.research_topic}\"}\n",
    "    \n",
    "    return {\"search_query\": follow_up_query['follow_up_query']}\n",
    "\n",
    "def finalize_summary(state: SummaryState):\n",
    "    \"\"\"Finalize the summary\"\"\"\n",
    "    all_sources = \"\\n\".join(source for source in state.sources_gathered)\n",
    "    final_summary = f\"## Summary\\n\\n{state.running_summary}\\n\\n### Sources:\\n{all_sources}\"\n",
    "    return {\"running_summary\": final_summary}\n",
    "\n",
    "def route_research(state: SummaryState, config: RunnableConfig) -> Literal[\"finalize_summary\", \"web_research\"]:\n",
    "    \"\"\"Route the research based on the follow-up query\"\"\"\n",
    "    configurable = Configuration.from_runnable_config(config)\n",
    "    if state.research_loop_count <= configurable.max_web_research_loops:\n",
    "        return \"web_research\"\n",
    "    else:\n",
    "        return \"finalize_summary\"\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 7. Build and Run the Agent\n",
    "# Now let's put it all together and create our research agent\n",
    "\n",
    "# %%\n",
    "# Create the graph\n",
    "builder = StateGraph(SummaryState, \n",
    "                    input=SummaryStateInput, \n",
    "                    output=SummaryStateOutput, \n",
    "                    config_schema=Configuration)\n",
    "\n",
    "# Add nodes\n",
    "builder.add_node(\"generate_query\", generate_query)\n",
    "builder.add_node(\"web_research\", web_research)\n",
    "builder.add_node(\"summarize_sources\", summarize_sources)\n",
    "builder.add_node(\"reflect_on_summary\", reflect_on_summary)\n",
    "builder.add_node(\"finalize_summary\", finalize_summary)\n",
    "\n",
    "# Add edges\n",
    "builder.add_edge(START, \"generate_query\")\n",
    "builder.add_edge(\"generate_query\", \"web_research\")\n",
    "builder.add_edge(\"web_research\", \"summarize_sources\")\n",
    "builder.add_edge(\"summarize_sources\", \"reflect_on_summary\")\n",
    "builder.add_conditional_edges(\"reflect_on_summary\", route_research)\n",
    "builder.add_edge(\"finalize_summary\", END)\n",
    "\n",
    "# Compile the graph\n",
    "graph = builder.compile()\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 8. Example Usage\n",
    "# Here's how to use the research agent\n",
    "\n",
    "# %%\n",
    "# Example research topic\n",
    "research_topic = \"Write a quick report on the latest LLMs that came out in 2025.\"\n",
    "\n",
    "# Run the research agent\n",
    "config = {\"configurable\": {\"max_web_research_loops\": 3, \"local_llm\": \"llama3.3\"}}\n",
    "result = graph.invoke(\n",
    "    {\"research_topic\": research_topic},\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Summary\n",
       "\n",
       "<think>\n",
       "Alright, I need to create a quick report summarizing the latest large language models (LLMs) as of 2025 based on the provided sources. The user has given specific instructions for summarization: starting immediately, focusing only on facts, maintaining technical depth without redundancy, and avoiding certain phrases.\n",
       "\n",
       "First, I'll review the source content about LLMs from 2024 and a partial report from 2025. The key points include:\n",
       "\n",
       "1. **Definition of LLMs**: Large language models are AI systems trained on vast datasets to understand and generate human language using transformer architectures.\n",
       "2. **Architectures**: They use neural networks, specifically transformers, which allow for context-aware processing.\n",
       "3. **Applications**: Beyond text generation, they're used in various industries like healthcare, finance, and customer service due to their ability to process large amounts of data quickly.\n",
       "4. **Challenges**: Concerns about bias, inaccuracy, and toxicity limit broader adoption and raise ethical issues.\n",
       "5. **Future Directions**: Approaches like self-training, fact-checking, and sparse expertise are being explored to mitigate these issues.\n",
       "\n",
       "From the sources, I can gather that GPT-3 and its successor GPT-4 are highlighted as significant models. Additionally, BLOOM is mentioned but only partially due to truncation in the source content.\n",
       "\n",
       "I need to structure this information into a concise report without using certain phrases. The report should be technical yet clear, focusing on facts and avoiding any unnecessary jargon that might not add value or could be misinterpreted.\n",
       "\n",
       "I'll start by introducing LLMs, their architecture, applications, challenges, and then discuss the latest models like GPT-3 and GPT-4, along with emerging approaches to improve them. I should ensure each section flows logically into the next, providing a clear progression of ideas.\n",
       "\n",
       "Finally, I'll conclude with a summary that encapsulates the key points without introducing new information.\n",
       "</think>\n",
       "\n",
       "**Summary of Large Language Models (LLMs) as of 2025**\n",
       "\n",
       "Large language models are AI systems designed to generate and understand human-like text by analyzing vast datasets. Utilizing transformer architectures, these models process context-aware information, enabling tasks such as text generation and comprehension.\n",
       "\n",
       "These models find applications across industries, leveraging their ability to handle large data efficiently for various purposes like customer service and healthcare. However, challenges related to bias, accuracy, and toxicity constrain broader adoption and raise ethical concerns.\n",
       "\n",
       "Emerging approaches aim to enhance these models by incorporating self-training, fact-checking, and sparse expertise to address biases and improve reliability. Notable advancements include GPT-3 and its successor, GPT-4, which represent significant milestones in AI development, with ongoing research focused on refining their capabilities for real-world applications.\n",
       "\n",
       "### Sources:\n",
       "* The Future of Large Language Models in 2025 - AIMultiple : https://research.aimultiple.com/future-of-large-language-models/"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(result[\"running_summary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'sources_gathered'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mresult\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msources_gathered\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'sources_gathered'"
     ]
    }
   ],
   "source": [
    "result[\"sources_gathered\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oreilly-agents",
   "language": "python",
   "name": "oreilly-agents"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
