{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a research Agent with LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install openai\n",
    "# !pip install \"langchain[all]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Or if you are in Colab, uncoment below and add your api key\n",
    "import os\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"your-api-key\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chat = ChatOpenAI(temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.reddit.com/r/AskCulinary/comments/emeul2/why_are_diner_style_pancakes_so_good_regardless/',\n",
       " 'https://www.quora.com/Why-do-pancakes-at-restaurants-like-Dennys-or-IHOP-taste-better-than-homemade-ones',\n",
       " 'https://www.prnewswire.com/news-releases/packaged-facts-still-room-for-pancakes-and-waffles-at-americas-breakfast-table-300317750.html',\n",
       " 'https://www.fitday.com/fitness-articles/nutrition/healthy-eating/healthy-breakfast-pancakes-vs-waffles.html',\n",
       " 'https://www.buzzfeed.com/ryankincaid/stacks-on-stacks',\n",
       " 'https://www.snackandbakery.com/articles/94837-state-of-the-industry-2020-breakfast-products-combine-nutrition-and-good-taste',\n",
       " 'https://health.ucdavis.edu/blog/good-food/why-is-breakfast-important-our-dietitian-gives-healthy-breakfast-ideas/2022/04',\n",
       " 'https://www.kingarthurbaking.com/blog/2021/10/26/best-pancake-tips',\n",
       " 'https://www.smithsonianmag.com/history/a-brief-history-of-pancakes-180981667/']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.utilities import SerpAPIWrapper\n",
    "from langchain.agents import tool\n",
    "\n",
    "search = SerpAPIWrapper()\n",
    "\n",
    "@tool\n",
    "def web_search(query: str):\n",
    "    \"\"\"Searches the web given a user query and returns the most relevant links\"\"\"\n",
    "    output = search.results(query)\n",
    "    results = output[\"organic_results\"]\n",
    "    return [r[\"link\"] for r in results]\n",
    "\n",
    "web_search(\"Current resources on why pancakes are the best breakfast\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['agent_scratchpad', 'input'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]], 'agent_scratchpad': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a helpful assistant')), MessagesPlaceholder(variable_name='chat_history', optional=True), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}')), MessagesPlaceholder(variable_name='agent_scratchpad')])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import hub\n",
    "\n",
    "prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['agent_scratchpad', 'input']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.input_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]],\n",
       " 'agent_scratchpad': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.input_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': None,\n",
       " 'input_variables': ['agent_scratchpad', 'input'],\n",
       " 'input_types': {'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]],\n",
       "  'agent_scratchpad': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]},\n",
       " 'output_parser': None,\n",
       " 'partial_variables': {},\n",
       " 'metadata': None,\n",
       " 'tags': None,\n",
       " 'messages': [SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a helpful assistant')),\n",
       "  MessagesPlaceholder(variable_name='chat_history', optional=True),\n",
       "  HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}')),\n",
       "  MessagesPlaceholder(variable_name='agent_scratchpad')],\n",
       " 'validate_template': False}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build an agent I need to do 2 essential things:\n",
    "\n",
    "1. Connect a model to tools\n",
    "2. Set up a loop in which the model continuously tries to solve the problem by either just calling the llm or calling a tool, observing the result and repeating until finished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['agent_scratchpad', 'input'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]], 'agent_scratchpad': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a helpful assistant')), MessagesPlaceholder(variable_name='chat_history', optional=True), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}')), MessagesPlaceholder(variable_name='agent_scratchpad')])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_openai_tools_agent\n",
    "\n",
    "tools = [web_search]\n",
    "\n",
    "agent = create_openai_tools_agent(llm_chat, tools, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor\n",
    "\n",
    "\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `web_search` with `{'query': 'resources to learn new languages'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m['https://nymag.com/strategist/article/how-to-learn-languages-at-home.html', 'https://www.lingualift.com/blog/best-language-learning-resources/', 'https://www.theintrepidguide.com/top-language-learning-resources/', 'https://www.reddit.com/r/languagelearning/comments/uhiuab/what_are_apps_or_websites_where_you_can_learn/', 'https://www.quora.com/What-resources-do-you-use-to-learn-languages', 'https://libguides.csudh.edu/modernlanguages/freeresources', 'https://www.fluentu.com/blog/free-language-learning-websites/', 'https://www.alllanguageresources.com/']\u001b[0m\u001b[32;1m\u001b[1;3mI found some resources that can help you learn new languages:\n",
      "\n",
      "1. [How to Learn Languages at Home - NYMag](https://nymag.com/strategist/article/how-to-learn-languages-at-home.html)\n",
      "2. [Best Language Learning Resources - LinguaLift](https://www.lingualift.com/blog/best-language-learning-resources/)\n",
      "3. [Top Language Learning Resources - The Intrepid Guide](https://www.theintrepidguide.com/top-language-learning-resources/)\n",
      "4. [Apps or Websites to Learn Languages - Reddit](https://www.reddit.com/r/languagelearning/comments/uhiuab/what_are_apps_or_websites_where_you_can_learn/)\n",
      "5. [Resources for Learning Languages - Quora](https://www.quora.com/What-resources-do-you-use-to-learn-languages)\n",
      "6. [Free Language Learning Websites - FluentU](https://www.fluentu.com/blog/free-language-learning-websites/)\n",
      "7. [Language Learning Resources - All Language Resources](https://www.alllanguageresources.com/)\n",
      "\n",
      "You can explore these resources to find the best fit for your language learning journey.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What are good resources to learn about learning new languages?',\n",
       " 'agent_scratchpad': [],\n",
       " 'output': 'I found some resources that can help you learn new languages:\\n\\n1. [How to Learn Languages at Home - NYMag](https://nymag.com/strategist/article/how-to-learn-languages-at-home.html)\\n2. [Best Language Learning Resources - LinguaLift](https://www.lingualift.com/blog/best-language-learning-resources/)\\n3. [Top Language Learning Resources - The Intrepid Guide](https://www.theintrepidguide.com/top-language-learning-resources/)\\n4. [Apps or Websites to Learn Languages - Reddit](https://www.reddit.com/r/languagelearning/comments/uhiuab/what_are_apps_or_websites_where_you_can_learn/)\\n5. [Resources for Learning Languages - Quora](https://www.quora.com/What-resources-do-you-use-to-learn-languages)\\n6. [Free Language Learning Websites - FluentU](https://www.fluentu.com/blog/free-language-learning-websites/)\\n7. [Language Learning Resources - All Language Resources](https://www.alllanguageresources.com/)\\n\\nYou can explore these resources to find the best fit for your language learning journey.'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\": \"What are good resources to learn about learning new languages?\",\n",
    "                             \"agent_scratchpad\": []})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mLearning good programming practices is essential for becoming a successful programmer.\n",
      "Action: web_search\n",
      "Action Input: \"best way to learn good programming practices\"\u001b[0m\u001b[36;1m\u001b[1;3m['https://www.reddit.com/r/learnprogramming/comments/6bv3ux/how_do_you_practice_programming/', 'https://www.quora.com/How-can-I-learn-the-best-practices-in-programming', 'https://www.thinkful.com/blog/coding-best-practices/', 'https://www.educative.io/blog/coding-best-practices', 'https://www.datacamp.com/tutorial/coding-best-practices-and-guidelines', 'https://flatironschool.com/blog/best-websites-to-practice-coding-for-beginners/', 'https://codesigningstore.com/ultimate-programming-best-practices-guide', 'https://www.browserstack.com/guide/coding-standards-best-practices', 'https://medium.com/@josueparra2892/20-best-programming-practices-407df688b96e']\u001b[0m\u001b[32;1m\u001b[1;3mThere are many resources available online to learn good programming practices.\n",
      "Action: web_search\n",
      "Action Input: \"best programming practices websites\"\u001b[0m\u001b[36;1m\u001b[1;3m['https://flatironschool.com/blog/best-websites-to-practice-coding-for-beginners/', 'https://analyticsindiamag.com/top-platforms-to-practice-coding/', 'https://www.freecodecamp.org/news/the-most-popular-coding-challenge-websites/', 'https://www.quora.com/Which-is-the-best-website-where-you-can-practice-coding-questions-and-improve-your-skills-efficiently-other-than-LeetCode', 'https://www.reddit.com/r/learnprogramming/comments/dpzieu/best_sites_to_practice_programming_ie_coding/', 'https://www.zdnet.com/education/computers-tech/practice-coding-problems/', 'https://www.ultraedit.com/blog/how-to-practice-coding-12-best-websites-to-practice-coding-for-beginners/', 'https://www.linkedin.com/pulse/top-10-coding-challengepractice-websites-arbaz-asif']\u001b[0m\u001b[32;1m\u001b[1;3mThere are specific websites dedicated to practicing coding and learning good programming practices.\n",
      "Action: web_search\n",
      "Action Input: \"coding practice websites\"\u001b[0m\u001b[36;1m\u001b[1;3m['https://flatironschool.com/blog/best-websites-to-practice-coding-for-beginners/', 'https://leetcode.com/', 'https://www.codewars.com/', 'https://www.reddit.com/r/learnpython/comments/11likgn/where_to_practice_coding/', 'https://www.quora.com/Which-is-the-best-website-where-you-can-practice-coding-questions-and-improve-your-skills-efficiently-other-than-LeetCode', 'https://analyticsindiamag.com/top-platforms-to-practice-coding/', 'https://www.zdnet.com/education/computers-tech/practice-coding-problems/', 'https://www.freecodecamp.org/news/the-most-popular-coding-challenge-websites/', 'https://www.sololearn.com/Discuss/3226510/the-best-free-coding-practice-websites-and-how-to-use-them-properly', 'https://www.codecademy.com/']\u001b[0m\u001b[32;1m\u001b[1;3mThere are popular websites like LeetCode, CodeWars, and Codecademy that offer coding practice to help learn good programming practices.\n",
      "Final Answer: The best way to learn good programming practices is to practice coding regularly on websites like LeetCode, CodeWars, and Codecademy.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What is the best way to learn good programming practices?',\n",
       " 'agent_scratchpad': [],\n",
       " 'output': 'The best way to learn good programming practices is to practice coding regularly on websites like LeetCode, CodeWars, and Codecademy.'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.agents import AgentExecutor, create_react_agent\n",
    "\n",
    "prompt = hub.pull(\"hwchase17/react\")\n",
    "\n",
    "agent = create_react_agent(llm_chat, tools, prompt)\n",
    "\n",
    "react_agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "\n",
    "react_agent_executor.invoke({\"input\": \"What is the best way to learn good programming practices?\",\n",
    "                             \"agent_scratchpad\": []})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's build this same agent but using a more in depth approach going one level down in the abstraction layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's go down the latter of abstraction and build this same agent but leveraging the lower-level capabilities of LangChain's LCEL language. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chat = ChatOpenAI(model=\"gpt-3.5-turbo-16k\",temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import tool\n",
    "from langchain.utilities import SerpAPIWrapper\n",
    "\n",
    "search = SerpAPIWrapper()\n",
    "\n",
    "@tool\n",
    "def web_search(query: str):\n",
    "    \"\"\"Searches the web given a user query and returns the most relevant links\"\"\"\n",
    "    output = search.results(query)\n",
    "    results = output[\"organic_results\"]\n",
    "    return [r[\"link\"] for r in results]\n",
    "\n",
    "\n",
    "tools = [web_search]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\", \"You are a powerful research assistant.\\\n",
    "            Given a user query you will perform research on the \\\n",
    "                topic using the tools available.\"\n",
    "        ),\n",
    "        (\"user\", \"{input}\"),\n",
    "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bind tools to the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.convert_to_openai import format_tool_to_openai_tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/greatmaster/miniconda3/envs/oreilly-agents/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `format_tool_to_openai_tool` was deprecated in LangChain 0.1.16 and will be removed in 0.2.0. Use langchain_core.utils.function_calling.convert_to_openai_tool() instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "llm_with_tools = llm_chat.bind(tools=[format_tool_to_openai_tool(tool) for tool in tools])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, all we have to do is import two last utility functions:\n",
    "- a component for formatting intermediate steps (agent action, tool output pairs) to input messages that can be sent to the model, \n",
    "- and a component for converting the output message into an agent action/agent finish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.format_scratchpad.openai_tools import format_to_openai_tool_messages\n",
    "from langchain.agents.output_parsers.openai_tools import OpenAIToolsAgentOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = (\n",
    "    {\n",
    "        \"input\": lambda x: x[\"input\"], \n",
    "        \"agent_scratchpad\": lambda x: format_to_openai_tool_messages(\n",
    "            x[\"intermediate_steps\"]\n",
    "        ),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm_with_tools\n",
    "    | OpenAIToolsAgentOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor\n",
    "\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `web_search` with `{'query': 'latest open source LLMs'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m['https://github.com/eugeneyan/open-llms', 'https://www.datacamp.com/blog/top-open-source-llms', 'https://www.unite.ai/best-open-source-llms/', 'https://www.elastic.co/blog/open-source-llms-guide', 'https://blog.n8n.io/open-source-llm/', 'https://klu.ai/blog/open-source-llm-models', 'https://github.com/Hannibal046/Awesome-LLM', 'https://www.simform.com/blog/open-source-llm/', 'https://www.techopedia.com/6-best-open-source-llms-to-watch-out-for-in-2024', 'https://www.upstage.ai/feed/insight/top-open-source-llms-2024']\u001b[0m\u001b[32;1m\u001b[1;3mHere are some of the latest open source LLMs available:\n",
      "\n",
      "1. [Open-LLMs](https://github.com/eugeneyan/open-llms) - A collection of open source LLM models on GitHub.\n",
      "2. [Top Open Source LLMs](https://www.datacamp.com/blog/top-open-source-llms) - A blog post on DataCamp that lists some of the top open source LLMs.\n",
      "3. [Best Open Source LLMs](https://www.unite.ai/best-open-source-llms/) - An article on Unite.ai that highlights the best open source LLMs.\n",
      "4. [Open Source LLMs Guide](https://www.elastic.co/blog/open-source-llms-guide) - A guide on open source LLMs by Elastic.\n",
      "5. [Open Source LLM](https://blog.n8n.io/open-source-llm/) - A blog post on n8n.io that discusses open source LLMs.\n",
      "6. [Open Source LLM Models](https://klu.ai/blog/open-source-llm-models) - A blog post on klu.ai that showcases open source LLM models.\n",
      "7. [Awesome-LLM](https://github.com/Hannibal046/Awesome-LLM) - A curated list of open source LLM models on GitHub.\n",
      "8. [Open Source LLM](https://www.simform.com/blog/open-source-llm/) - An article on Simform that explores open source LLMs.\n",
      "9. [6 Best Open Source LLMs to Watch Out for in 2024](https://www.techopedia.com/6-best-open-source-llms-to-watch-out-for-in-2024) - A list of the best open source LLMs to watch out for in 2024.\n",
      "10. [Top Open Source LLMs 2024](https://www.upstage.ai/feed/insight/top-open-source-llms-2024) - An article on Upstage.ai that highlights the top open source LLMs in 2024.\n",
      "\n",
      "You can visit these links to learn more about the latest open source LLMs.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'actions': [OpenAIToolAgentAction(tool='web_search', tool_input={'query': 'latest open source LLMs'}, log=\"\\nInvoking: `web_search` with `{'query': 'latest open source LLMs'}`\\n\\n\\n\", message_log=[AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_phz3F3TMThgLMYTlLtuLDKN0', 'function': {'arguments': '{\\n  \"query\": \"latest open source LLMs\"\\n}', 'name': 'web_search'}, 'type': 'function'}]})], tool_call_id='call_phz3F3TMThgLMYTlLtuLDKN0')],\n",
       "  'messages': [AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_phz3F3TMThgLMYTlLtuLDKN0', 'function': {'arguments': '{\\n  \"query\": \"latest open source LLMs\"\\n}', 'name': 'web_search'}, 'type': 'function'}]})]},\n",
       " {'steps': [AgentStep(action=OpenAIToolAgentAction(tool='web_search', tool_input={'query': 'latest open source LLMs'}, log=\"\\nInvoking: `web_search` with `{'query': 'latest open source LLMs'}`\\n\\n\\n\", message_log=[AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_phz3F3TMThgLMYTlLtuLDKN0', 'function': {'arguments': '{\\n  \"query\": \"latest open source LLMs\"\\n}', 'name': 'web_search'}, 'type': 'function'}]})], tool_call_id='call_phz3F3TMThgLMYTlLtuLDKN0'), observation=['https://github.com/eugeneyan/open-llms', 'https://www.datacamp.com/blog/top-open-source-llms', 'https://www.unite.ai/best-open-source-llms/', 'https://www.elastic.co/blog/open-source-llms-guide', 'https://blog.n8n.io/open-source-llm/', 'https://klu.ai/blog/open-source-llm-models', 'https://github.com/Hannibal046/Awesome-LLM', 'https://www.simform.com/blog/open-source-llm/', 'https://www.techopedia.com/6-best-open-source-llms-to-watch-out-for-in-2024', 'https://www.upstage.ai/feed/insight/top-open-source-llms-2024'])],\n",
       "  'messages': [FunctionMessage(content='[\"https://github.com/eugeneyan/open-llms\", \"https://www.datacamp.com/blog/top-open-source-llms\", \"https://www.unite.ai/best-open-source-llms/\", \"https://www.elastic.co/blog/open-source-llms-guide\", \"https://blog.n8n.io/open-source-llm/\", \"https://klu.ai/blog/open-source-llm-models\", \"https://github.com/Hannibal046/Awesome-LLM\", \"https://www.simform.com/blog/open-source-llm/\", \"https://www.techopedia.com/6-best-open-source-llms-to-watch-out-for-in-2024\", \"https://www.upstage.ai/feed/insight/top-open-source-llms-2024\"]', name='web_search')]},\n",
       " {'output': 'Here are some of the latest open source LLMs available:\\n\\n1. [Open-LLMs](https://github.com/eugeneyan/open-llms) - A collection of open source LLM models on GitHub.\\n2. [Top Open Source LLMs](https://www.datacamp.com/blog/top-open-source-llms) - A blog post on DataCamp that lists some of the top open source LLMs.\\n3. [Best Open Source LLMs](https://www.unite.ai/best-open-source-llms/) - An article on Unite.ai that highlights the best open source LLMs.\\n4. [Open Source LLMs Guide](https://www.elastic.co/blog/open-source-llms-guide) - A guide on open source LLMs by Elastic.\\n5. [Open Source LLM](https://blog.n8n.io/open-source-llm/) - A blog post on n8n.io that discusses open source LLMs.\\n6. [Open Source LLM Models](https://klu.ai/blog/open-source-llm-models) - A blog post on klu.ai that showcases open source LLM models.\\n7. [Awesome-LLM](https://github.com/Hannibal046/Awesome-LLM) - A curated list of open source LLM models on GitHub.\\n8. [Open Source LLM](https://www.simform.com/blog/open-source-llm/) - An article on Simform that explores open source LLMs.\\n9. [6 Best Open Source LLMs to Watch Out for in 2024](https://www.techopedia.com/6-best-open-source-llms-to-watch-out-for-in-2024) - A list of the best open source LLMs to watch out for in 2024.\\n10. [Top Open Source LLMs 2024](https://www.upstage.ai/feed/insight/top-open-source-llms-2024) - An article on Upstage.ai that highlights the top open source LLMs in 2024.\\n\\nYou can visit these links to learn more about the latest open source LLMs.',\n",
       "  'messages': [AIMessage(content='Here are some of the latest open source LLMs available:\\n\\n1. [Open-LLMs](https://github.com/eugeneyan/open-llms) - A collection of open source LLM models on GitHub.\\n2. [Top Open Source LLMs](https://www.datacamp.com/blog/top-open-source-llms) - A blog post on DataCamp that lists some of the top open source LLMs.\\n3. [Best Open Source LLMs](https://www.unite.ai/best-open-source-llms/) - An article on Unite.ai that highlights the best open source LLMs.\\n4. [Open Source LLMs Guide](https://www.elastic.co/blog/open-source-llms-guide) - A guide on open source LLMs by Elastic.\\n5. [Open Source LLM](https://blog.n8n.io/open-source-llm/) - A blog post on n8n.io that discusses open source LLMs.\\n6. [Open Source LLM Models](https://klu.ai/blog/open-source-llm-models) - A blog post on klu.ai that showcases open source LLM models.\\n7. [Awesome-LLM](https://github.com/Hannibal046/Awesome-LLM) - A curated list of open source LLM models on GitHub.\\n8. [Open Source LLM](https://www.simform.com/blog/open-source-llm/) - An article on Simform that explores open source LLMs.\\n9. [6 Best Open Source LLMs to Watch Out for in 2024](https://www.techopedia.com/6-best-open-source-llms-to-watch-out-for-in-2024) - A list of the best open source LLMs to watch out for in 2024.\\n10. [Top Open Source LLMs 2024](https://www.upstage.ai/feed/insight/top-open-source-llms-2024) - An article on Upstage.ai that highlights the top open source LLMs in 2024.\\n\\nYou can visit these links to learn more about the latest open source LLMs.')]}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(agent_executor.stream({\"input\": \"What are the latest open source LLMs available?\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, what if we want the agent to actually read the links and return a research report rather than just the urls?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "@tool\n",
    "def scrape_text(url: str):\n",
    "    \"\"\"Scrapes the text from a given URL and returns it.\"\"\"\n",
    "    # Send a GET request to the webpage\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "\n",
    "        # Check if the request was successful\n",
    "        if response.status_code == 200:\n",
    "            # Parse the content of the request with BeautifulSoup\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "            # Extract all text from the webpage\n",
    "            page_text = soup.get_text(separator=\" \", strip=True)\n",
    "\n",
    "            # Print the extracted text\n",
    "            return page_text[:10000]\n",
    "        else:\n",
    "            return f\"Failed to retrieve the webpage: Status code {response.status_code}\"\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return f\"Failed to retrieve the webpage: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [web_search, scrape_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_with_tools = llm_chat.bind(tools=[format_tool_to_openai_tool(tool) for tool in tools])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\", \"You are a powerful research assistant. Given a user query you will perform research on the topic using the tools available.\"\n",
    "        ),\n",
    "        (\"user\", \"Scrape and summarize the contents from these urls: {input}\"),\n",
    "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = (\n",
    "    {\n",
    "        \"input\": lambda x: x[\"input\"], \n",
    "        \"agent_scratchpad\": lambda x: format_to_openai_tool_messages(\n",
    "            x[\"intermediate_steps\"]\n",
    "        ),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm_with_tools\n",
    "    | OpenAIToolsAgentOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor\n",
    "\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `scrape_text` with `{'url': 'https://towardsdatascience.com/deploying-llm-agents-to-production-best-practices-7e4e8e8e8e8e'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mFailed to retrieve the webpage: Status code 403\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `scrape_text` with `{'url': 'https://towardsdatascience.com/deploying-llm-agents-to-production-best-practices-7e4e8e8e8e8e'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mFailed to retrieve the webpage: Status code 403\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `scrape_text` with `{'url': 'https://towardsdatascience.com/deploying-llm-agents-to-production-best-practices-7e4e8e8e8e8e'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mFailed to retrieve the webpage: Status code 403\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `web_search` with `{'query': 'best practices to deploy LLM agents to production'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m['https://www.qwak.com/post/best-practices-when-deploying-llms-in-production', 'https://arize.com/blog-course/large-language-model-llm-deployment/', 'https://www.reddit.com/r/MachineLearning/comments/16fklqj/discussion_how_to_deploy_llm_in_production/', 'https://www.linkedin.com/pulse/deploying-llms-production-anatomy-llm-applications-xenonstack', 'https://www.youtube.com/watch?v=NRjUMB9BrhQ', 'https://aws.amazon.com/blogs/machine-learning/learn-how-to-build-and-deploy-tool-using-llm-agents-using-aws-sagemaker-jumpstart-foundation-models/', 'https://huyenchip.com/2023/04/11/llm-engineering.html', 'https://www.seldon.io/deploying-large-language-models-in-production']\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `scrape_text` with `{'url': 'https://www.qwak.com/post/best-practices-when-deploying-llms-in-production'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mBest Practices for Deploying LLMs in Production | Qwak ð£ MONTHLY LIVE DEMO: See Qwak in ActionÂ Â |Â Â Apr. 3, 2024, 11am EST -> x Platform Our Products Model Registry Model Training Model Serving Model Monitoring Feature Store Vector Store Feature + Vector Pipeline Managed Notebooks Solutions Use Cases LLMOps Recommender Systems Search & Ranking Fraud Detection Resources Blog Events & Webinars Compare Qwak Customer Stories Model Examples Integrations Virtual Conference Join Infer Spring â24: The engineering behind AI and ML. Documentation Pricing Company About us Partners Careers Contact us Follow us Log in Book Demo Start Free Book a Demo Start for Free Back to Blog LLMops Best Practices for Deploying LLMs in Production Large Language Models (LLMs) have taken the world by storm. Thanks to powerful AI models such as OpenAI's GPT-4 and the popularity of products such as ChatGPT, companies are now racing to integrate LLMs into their products. Guy Eshet Product Manager at Qwak April 30, 2023 Table of contents Introduction Large Language Models (LLMs) have taken the world by storm. Thanks to powerful AI models such as OpenAI's GPT-4 and the popularity of products such as ChatGPT, companies are now racing to integrate LLMs into their products. Launching exciting prototypes with generative models may seem easy, however deploying LLMs in production can be a complex process that requires robust infrastructure and proper tools.Â In this article, we will explore the challenges and considerations for deploying LLMs in production, and learn how Qwak is helping customers in various industries to streamline their MLOps workflows and deploy LLMs in production. What are LLMs? LLMs are large machine learning models that use deep learning to generate, summarize and understand natural language, text and code. LLMs are trained on vast amounts of data, and demand immense compute power to train and maintain.Â In recent years, there has been a significant advancement in the inference abilities of LLMs. These models are now capable of performing complex tasks such as creative writing, coding, information retrieval, summarization, and many more. LLMs have become valuable tools for a wide range of applications, including chatbots, voice assistants, search and automated content generation, among others. Closed-source and open-source LLMs LLMs can be broadly classified into two categories: closed-source and open-source models.Â Closed-source models, such as GPT-4 by OpenAI, LaMDA by Google, and Jurassic by AI21, are proprietary models that are only available through APIs. These models offer limited customization options and control over your data, as the source code and model weights are not available to the public. However, closed-source models are often more powerful than their open-source counterparts, as they are developed by industry leaders with access to vast resources and expertise. On the other hand, open-source models, such as BLOOM, Alpaca, LLaMA, OpenAssistant, and BERT, are rapidly evolving and catching up. They offer more flexibility and can be deployed on your own infrastructure while being customized to your specific needs. Make sure to check the model licenses before using them. Sme models are available for research purposes only and cannot be used for commercial applications.Â Customizing large language models Prompt engineering, fine-tuning, and creating chains are three common techniques used to generate output from LLMs. Prompt Engineering Prompt engineering involves the art of providing a prompt to the model. This can include a question, sentence, or topic for the model to focus on. The purpose is to guide the model output towards the desired direction. Prompt engineering can help to control the output and ensure that it's relevant and useful. Model Chains Creating chains involves using the output of one model as the input for another model, creating a chain of models that work together to generate the final output. This technique is often used to improve the quality and relevance of the output. As an example, LangChain is one of the most common LLM frameworks supporting these use cases. Additionally, AI-Agent frameworks such as Auto-GPT use similar mechanisms to use LLMs to complete complex tasks. Model Fine Tuning Fine-tuning involves training an LLM on a specific dataset or task to improve its performance on that particular task. Fine-tuning can help to optimize the model's output for a specific use case or application. For instance, a fine-tuned LLM could be designed to process financial data or expertly handle customer requests. Fine-tuning models requires significant infrastructure and can be done on managed models like GPT-4, although the inference cost is generally higher. Fine Tuning on Qwak To make the process easier, Qwak enables our customers to fine-tune, tag, and track their fine-tuned models more effectively. This helps to ensure better control over the various model versions between different fine-tuning processes. Additionally, Qwak makes it easier to connect your existing data pipelines with the model fine-tuning process, allowing data science teams to operate faster.Â Integrating LLMs into your products Incorporating LLMs into your products can be challenging, as it involves considerations such as privacy, security, and legal issues. These concerns vary widely when choosing to deploy in-house LLMs or when using external models. In the near future, it's possible that companies will use external GPT-4-like models for early prototyping and high-stakes applications, while opting for smaller, more fine-tuned open-source, self hosted LLMs for other use cases. This approach will likely lead to a mix of both large and small models in the production pipeline. About Qwak Qwak is a fully managed, accessible, and reliable ML Platform. It allows builders to transform and store data, build, train, and deploy models, and monitor the entire Machine Learning pipeline. Pay-as-you-go pricing makes it easy to scale when needed. Infer Virtual Conference by Qwak March 20th, 11AM EST -> Say goodbye to complex MLOps Chat with us to see the platform live and discover how we can help simplify your ML journey. Book a Meeting Related posts LLMops RAG and LLM: A New Frontier in Dynamic Language Modeling Pavel Klushin December 28, 2023 LLMops What is Vector Search? Building a Production-Grade Vector Search System Hudson Buzby February 5, 2024 LLMops The Costs of Leveraging the Power of GPT4 Lior Penso March 19, 2023 Get started with Qwak today. See how the best ML engineers and data scientists deploy models in production. Say goodbye to complex MLOps and start delivering today. Try Qwak free Book a Demo Build high-quality ML simply. Â© 2024 Qwak. All rights reserved. Terms & Conditions Privacy Policy Products Model Registry Model Training Model Serving Model Monitoring Feature Store Vector Store Feature + Vector Pipeline Managed Notebooks Company Blog Pricing Case Studies Partners Careers Compare Qwak Qwak vs. SageMaker Qwak vs. Vertex AI Qwak vs. Databricks Follow us Sign up for Qwak updates\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `scrape_text` with `{'url': 'https://arize.com/blog-course/large-language-model-llm-deployment/'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mBest Practices for Large Language Model (LLM) Deployment - Arize AI Join us on July 11th for Arize:Observe @ Shack15 → Platform Overview Left Column Monitors Dashboards Eval & Performance Tracing Explainability & Fairness Right Column Embeddings & RAG Analyzer LLM Tracing Fine Tune Phoenix OSS Arize Product Demo: See the Platform in Action Watch now Solutions Overview Left Column Use Cases Computer Vision Recommender Systems Regression & Classification Forecasting Right Column Industries Financial Services eCommerce Media & Entertainment Autonomous Vehicles Manufacturing Biotechnology & Pharmaceutical Research Customers Learn more Pricing Learn Left Column Resources Blog Podcast Events Videos Paper Readings Right Column Arize University Certification Basic and advanced course certification LLMOps Self-guided LLMops learning ML Observability Self-guided observability learning Arize Community Great discussions, support, and random acts of swag await! Join now Docs Company About Partners Careers Press Security Arize AI Introduces Next Generation of Its ML Observability Platform, Goes Self-Serve Learn more Book a Demo Sign In Sign Up × ARIZE GENERATIVE AI COURSE Intro To LLMOps Intro To LLMOps and LLM Observability What is LLM Observability? Introduction To LLMOps Foundation Models OpenAI API LLM Deployment LLM Evaluation LLM Evaluation: Build and Benchmark Evals What To Avoid The Needle In a Haystack Test Evals from OpenAI Traces and Spans Traces and Spans in Orchestration Frameworks Search and Retrieval: Build Your Own Chatbot Retrieval Augmented Generation (RAG) Benchmarking Evaluation of LLM RAG RAG Evaluation Prompt Engineering Evaluating Prompts: A Developer’s Guide Chains and Agents AI Agents: When and How To Implement LLM Agent: Set Up ReAct Architecture | Simple AI Agents LLM Guardrails and Controls for Deployment Guardrails Use Cases Structured Data Extraction Certifications LLM Certification Agents, Tools, and Chains LLM Evaluations Traces and Spans ML Observability Fundamentals Advanced Model Performance Metrics New to Arize? Arize onboarding Best Practices for Large Language Model (LLM) Deployment Sally-Ann DeLucia, ML Solutions Engineer | Published June 06, 2023 Large language models (LLMs) are revolutionizing the AI landscape. Despite being relatively new, many teams are jumping at the opportunity to deploy these powerful models in their organizations. While excitement is high, knowing where to start can be a daunting task. This post covers the ins-and-outs of deploying LLMs, including: tradeoffs between models, optimization — prompt engineering, fine tuning, and context retrieval — deployment strategies, and what to do after launch. Tradeoffs Between Custom and Commercial Models (Build Versus Buy LLMs) Benefits of Building One of the first decisions to make when deciding to deploy a LLM is whether to build a LLM from scratch or use a commercial model. Both options have their advantages, and the answer might actually be both. Let’s get into it. One benefit of building an LLM from scratch is that you have complete data ownership and privacy. Not only will your proprietary data be kept private but you can also make the most out of it. By leveraging your own data, you have the ability to control where the data powering your LLM comes from. This allows you to make sure the LLM is trained on reliable sources that won’t contribute to bias. You also don’t have to worry about your proprietary data being inadvertently used by a third party for training or leaked . Using your own data could also lead to superior performance, potentially giving you a competitive advantage. You can also decide exactly what content filters to use based on your specific business use case. For example, you might need a longer sequence length than what a commercial model offers or the ability to add specific content filters to your training data. When you opt to use a commercial LLM, you have to work around the sequence limits and have no visibility of the data used for training. Commercial models, like GPT-4, are known to have a delay ranging from 5 to 30 seconds depending on the query. This latency issue is not ideal for applications that require real-time responses. When you build your LLM from scratch, you can opt for a smaller model with fewer layers and less parameters which tend to have faster inference speeds than their larger peers. There’s growing evidence that smaller models can be just as powerful as larger models for domain specific tasks. BioMedLM , a biomedical-specific LLM using a mere 2.7 billion parameters, performed just as well if not better than a 120 billion parameter competitor. Another benefit: your model is smaller this will save you a bunch in training and serving costs. Since you have complete model ownership when you opt to build from scratch, you also have better introspection and explainability . A commercial model is a black box; since you have little to no access to the inner workings of the model, understanding why the model is behaving the way it does is extremely difficult. Benefits of Commercial Models One of the biggest challenges when building an LLM from scratch is the cost. Building a LLM can be extremely expensive. GPT-4, for example, reportedly cost $100 million to train. There’s also a lot of risk. With little room for error, you could end up wasting thousands or even millions of dollars — leaving you with only a suboptimal model. You also need a large volume of high quality and diverse data in order for the model to gain the required generalization capabilities to power your system. Using a commercial model, on the other hand, means far less training costs. Since you do not have to worry about hosting or pre-training a commercial LLM, the only cost occurs at inference. Training costs would only be acquired from doing tests and experiments during development. Another benefit of commercial LLMs is they require less technical expertise. Finally, commercial models can also are a great tool for prototyping and exploring. So…Which? Overall, the choice of whether or not to build or buy an LLM comes down to your specific application, resources, traffic, and data ownership concerns. Teams with domain-specific applications might opt to build a model from scratch whereas teams looking to leverage the latest and greatest to build downstream applications might use a commercial model. Prior to heavily investing in either option, you may want to experiment with the technology to understand what’s possible and carefully consider your specific requirements. What Are the Benefits of Open Source Over ChatGPT and Other Commercial LLMs? While build-versus-commercial is the mainstream debate, let’s not forget about the open source options. There have been some impressive open source models that are available for commercial use. Dolly, MPT-7B, and RedPajama are just a few examples of open source models with commercial licenses that are rivaling popular commercial models like GPT-4 and LLaMA. Open source LLMs allow you to leverage powerful models that have already learned a vast amount of data without having to be dependent on a service. Since you are not starting completely from scratch, there can be huge savings on training time and budget. This allows you to get your model in production sooner. Just like building and using commercial LLMs, open source LLMs have downsides as well. While you save on costs at inference time by not having to pay a service provider, if you have a low usage then using a commercial model might actually lead to a cost savings. The cost benefits of open source models is seen when the requested volume is more than one million (see: Skanda Vivek’s great piece on LLM economics). The main cost associated with using open source is hosting and deploying these large models. When you have thousands of requests a day, paying a service provider often works out cheaper than paying at inference. In addition to cost, open source models, while less demanding than building from scratch, still require substantial lift. Similar domain expertise is needed to train, fine-tune, and host an open source LLM. There is also evidence to support that reproducibility is still an issue for open source LLMs. Without the proper expertise you risk wasting time and resources. A Practical Guide to Optimization of Foundation Models The steps to deployment will differ depending on your use case and model choice. Commercial models, for example, do not allow for fine tuning. Instead, you opt for prompt engineering and context retrieval to optimize your LLM. When using an open source or your own LLM, you’ll likely use a combination of these techniques to refine the model’s performance. Prompt Engineering — Shaping Performance with the Right Instructions Prompt engineering is a new field in AI where engineers focus on crafting prompts to feed the model, with prompts defined as the set of instructions the LLM will use to perform the task. Prompts play a huge role in optimizing and controlling the performance of LLMs. Prompt engineering allows developers to provide the LLM with context to perform a specific task. A good prompt will assist the model in providing high quality responses that are relevant and accurate. This allows you to harness the power of LLMs without needing extensive fine-tuning. Since prompts are at the center of guiding model output towards the desired direction, prompt engineering is a crucial step in deploying an LLM. An effective prompt will allow your LLM to correctly and reliably respond to your query. You want your prompt to be clear and include specific instructions. Some best practices for prompts include: Write clear and specific instructions Use delimiters to indicate specific pieces of the prompt Outline the structure of the desired output Use guidelines to check if certain conditions are met Leverage few-shot prompting to give successful examples of completing the task Prompt engineering is a\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `scrape_text` with `{'url': 'https://www.reddit.com/r/MachineLearning/comments/16fklqj/discussion_how_to_deploy_llm_in_production/'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mReddit - Dive into anything Skip to main content Reddit and its partners use cookies and similar technologies to provide you with a better experience. By accepting all cookies, you agree to our use of cookies to deliver and maintain our services and site, improve the quality of Reddit, personalize Reddit content and advertising, and measure the effectiveness of advertising. By rejecting non-essential cookies, Reddit may still use certain cookies to ensure the proper functionality of our platform. For more information, please see our Cookie Notice and our Privacy Policy . Open menu Open navigation Go to Reddit Home r/MachineLearning A chip A close button Get app Get the Reddit app Log In Log in to Reddit Expand user menu Open settings menu Log In / Sign Up Advertise on Reddit Shop Collectible Avatars Get the Reddit app Scan this QR code to download the app now Or check it out in the app stores Go to MachineLearning r/MachineLearning r/MachineLearning ml Members Online • unknow_from_vietnam ADMIN MOD [Discussion] How to deploy LLM in production Discussion hello everyone, I am currently developing a response generator using GPT2, I use weights and code from Huggingface. Currently, I am having some problems deploying the model to production, with the need to handle many requests within the allowed time. I used ONNX to convert but it encountered many problems when having to separate the modules. Is there any other way for me to convert quickly and more conveniently, or is there a way to serve LLM models in production? Read more &nbsp; TOPICS Gaming Valheim Genshin Impact Minecraft Pokimane Halo Infinite Call of Duty: Warzone Path of Exile Hollow Knight: Silksong Escape from Tarkov Watch Dogs: Legion Sports NFL NBA Megan Anderson Atlanta Hawks Los Angeles Lakers Boston Celtics Arsenal F.C. Philadelphia 76ers Premier League UFC Business GameStop Moderna Pfizer Johnson & Johnson AstraZeneca Walgreens Best Buy Novavax SpaceX Tesla Crypto Cardano Dogecoin Algorand Bitcoin Litecoin Basic Attention Token Bitcoin Cash Television The Real Housewives of Atlanta The Bachelor Sister Wives 90 Day Fiance Wife Swap The Amazing Race Australia Married at First Sight The Real Housewives of Dallas My 600-lb Life Last Week Tonight with John Oliver Celebrity Kim Kardashian Doja Cat Iggy Azalea Anya Taylor-Joy Jamie Lee Curtis Natalie Portman Henry Cavill Millie Bobby Brown Tom Hiddleston Keanu Reeves RESOURCES About Reddit Advertise Help Blog Careers Press Communities Best of Reddit Topics Content Policy Privacy Policy User Agreement Reddit, Inc. © 2024. All rights reserved. Top 1% Rank by size\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `scrape_text` with `{'url': 'https://www.linkedin.com/pulse/deploying-llms-production-anatomy-llm-applications-xenonstack'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mDeploying LLMS in Production: The Anatomy of LLM Applications LinkedIn and 3rd parties use essential and non-essential cookies to provide, secure, analyze and improve our Services, and to show you relevant ads (including professional and job ads ) on and off LinkedIn. Learn more in our Cookie Policy . Select Accept to consent or Reject to decline non-essential cookies for this use. You can update your choices at any time in your settings . Accept Reject Agree & Join LinkedIn By clicking Continue, you agree to LinkedIn’s User Agreement , Privacy Policy , and Cookie Policy . Sign in to view more content Create your free account or sign in to continue your search Sign in Welcome back Email or phone Password Show Forgot password? Sign in or By clicking Continue, you agree to LinkedIn’s User Agreement , Privacy Policy , and Cookie Policy . New to LinkedIn? Join now or By clicking Continue, you agree to LinkedIn’s User Agreement , Privacy Policy , and Cookie Policy . New to LinkedIn? Join now Skip to main content LinkedIn Articles People Learning Jobs Join now Sign in Anatomy of LLM Applications Deploying LLMs in Production: The Anatomy of LLM Applications Report this article XenonStack XenonStack Platform Engineering for Autonomous Operations #autonomousops Published Oct 10, 2023 + Follow Introduction Large Language Models (LLMs) are deep learning algorithms that can be used for a wide range of NLP (Natural Language Processing). Large language models use transformer models and are trained using massive datasets — hence, large. This enables them to recognize, translate, predict, or generate text or other content. Large language models are also referred to as neural networks (NNs), which are computing systems inspired by the human brain. These neural networks work using a network of nodes that are layered, much like neurons. Subscribe the Newsletter LLM applications and use cases LLMs have opened a new class of enterprise use cases. Being trained on such a large amount of information, these models provide a strong foundation to build off. This makes them incredibly versatile and easily adaptable to suit a variety of applications. Whether leveraging the built-in knowledge of the model or fine-tuning to your use case, there are many ways to provide a useful and intuitive user experience. Some LLM use applications are as follows: - Chatbots It provides customer support, product recommendations and conversational search. Document Understanding Summarize, exact key information and analyse sentiments. Code completion Explain and document code, suggest improvement and auto generate code. Content generation Write a blog post, emails, ad copies and outline based on prompts. Search Answer natural language questions on top of a large collection of texts. Translation Translate between languages and perform style transfer to change the tone and reading level. LLM components and reference architecture LLM Models The LLM model is the heart of the LLM application and selecting a model depends on a variety of factors. The primary elements are the size of the model (# of parameters) and whether it is open sourced or behind an API. Some models may have tokenizers and embedding models built in, while others will require you to run these steps yourself. Tokenizers split input text into smaller chunks while embedding models convert the text into numeric vectors which an LLM can understand. The easiest place to begin prototyping is with OpenAI’s GPT-4, as it is quick to get up and running given how adaptable the model is. In some cases, a long context window may be required if performing a lot of in-context learning, making Claude 2 an excellent choice. However, when using such large models, both the cost and latency could become prohibitive. Prompt Template Much like the experimentation process that has evolved in traditional Machine Learning model training, prompt engineering has emerged as a core activity in LLM application development. There are several prompting techniques that lead to better results. The first technique is to write clear instructions and specify the steps required to complete the task. Most tasks can be done using a zero-shot prompt but adding some examples with a few-shot prompt can further improve and tailor the response. Even simply including phrases such as show your work or let’s think step by step leads the model to iteratively develop a solution iteratively, increasing the chances of a correct answer. This chain-of-thought prompting is even more powerful if a few examples of reasoning logic are provided. Prompts can then be templated and reused by an LLM application. They can be integrated into your code with a simple f-string or str. format(), but there are also libraries like LangChain, Guidance, and LMQL that offer more control. For chat completion APIs, there is generally a system prompt to assign the task, followed by alternating user and assistant messages. You can seed the chat with examples of user questions and assistant answers before running it. Experimenting and iterating on these prompt templates in a structured way will lead to improved model performance. The outputs of the model should be evaluated by either a scoring function or human feedback. Vector Database Many use cases will require access to information that the LLM has not been trained on. This may be a private knowledge base (e.g., a company wiki), recent information (e.g., events this weekend), or a specific document (e.g., a research paper). There might be other cases where the context window of the model is too small to include everything in the prompt (e.g., an entire book). Adding an external database can provide access to supplementary information in a technique known as retrieval augmented generation (RAG). Vector databases have been around for some time but have surged in popularity recently as they make their way into LLM applications. Relevant external information is first chunked into blocks (e.g., sentences or paragraphs), tokenized, and run through an embedding model. These are then fed into a vector database, such as Pinecone, Chrome, Qdrant, or pgvector (open source). When a prompt is made, it is also vectorized and a similarity search is performed to retrieve relevant entries from the database. These entries are then fed in alongside the original prompt, providing the context needed to provide a coherent response. The LLM may cite which chunks it used in its response, which adds a degree of trust to counteract potential hallucinations. Agents and Tools Using LLMs alone can enable powerful applications, but there are inherent limitations. For example, they cannot continually prompt themselves, make external API calls, nor retrieve a web page. An LLM agent has access to tools that can perform actions beyond text generation. For example, an LLM agent could perform a web search, execute code, perform a database lookup, or do maths calculations. OpenAI language models can decide which tools to use and return a JSON object with the arguments to call the function. This enables a whole new suite of use cases, such as booking a flight, generating an image, or sending an email. Some frameworks enable agents to execute iteratively to complete a task. This could be as simple as breaking down a task into several subtasks or “self-asking” to continue gathering relevant information for the task. These can then be executed in a chain to work towards a correct answer iteratively. The idea of combining this reasoning alongside taking actions using tools led to the emergence of a framework called React. ReAct enters a loop of thinking about which action to take, taking that action, observing the result, and deciding on a subsequent action until a solution is found. Although this approach has proved to outperform baseline LLMs, it should be noted that evaluating the performance of these systems and achieving reliable results is still a major challenge. Security is also a concern as LLMs are given the ability to take actions, such as posting on the web. Orchestrator Orchestrators provide a framework to tie these components together. They create abstractions on top of LLMs, prompt templates, data sources, agents, and tools. Templating frameworks like Guidance and LMQL enable complex prompts that specify inputs, outputs, and constraints. In addition, these tools can improve performance by providing memory management, token healing, beam search, session management, error handling, and more. Being able to create a logical control flow on top of LLMs enables developers to build differentiated, specialized, and hardened use cases. Monitoring Given the stochastic nature and rapid evolution of LLM models, your application should be monitored in production. Standard monitoring of CPU, GPU, memory usage, latency, and throughput should be tracked. Drift and outlier detectors can also be deployed to alert you of changing or anomalous inputs over time. Requests and responses should also be logged so that unexpected or harmful outputs can be evaluated. With complex chains, or graphs, of LLM agents strung together, it can be difficult to track exactly how the prompt is evolving. Tools like LangSmith and Seldon Core v2 provide the ability to trace the flow of data, providing visibility into the behaviour of your LLM application. Not only that, but Seldon Core v2 provides a data-centric deployment graph that enables advanced monitoring with drift detectors and explainers. Conclusion Deploying and managing LLMs in a production environment can be challenging due to resource management, model performance, model versioning, and infrastructure issues. LLMs are simple to deploy and administer in a production setting using MLflow’s tools and APIs for managing the model lifecycle. In this blog, we discussed how to use MLflow to deploy and manage LLMs in a production environment, along with support for Hugging Face transformers, Open AI, and Lang Chain models. The c\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `scrape_text` with `{'url': 'https://www.youtube.com/watch?v=NRjUMB9BrhQ'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mA Must for Deploying LLM applications to Production - YouTube Acerca de Imprensa Direitos de autor Contactar-nos Criadores Publicidade Programadores Termos Privacidade Política e segurança Como funciona o YouTube Testar novas funcionalidades © 2024 Google LLC\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `scrape_text` with `{'url': 'https://aws.amazon.com/blogs/machine-learning/learn-how-to-build-and-deploy-tool-using-llm-agents-using-aws-sagemaker-jumpstart-foundation-models/'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mLearn how to build and deploy tool-using LLM agents using AWS SageMaker JumpStart Foundation Models | AWS Machine Learning Blog Skip to Main Content Click here to return to Amazon Web Services homepage Contact Us Support English My Account Sign In Create an AWS Account Products Solutions Pricing Documentation Learn Partner Network AWS Marketplace Customer Enablement Events Explore More Close عربي Bahasa Indonesia Deutsch English Español Français Italiano Português Tiếng Việt Türkçe Ρусский ไทย 日本語 한국어 中文 (简体) 中文 (繁體) Close My Profile Sign out of AWS Builder ID AWS Management Console Account Settings Billing & Cost Management Security Credentials AWS Personal Health Dashboard Close Support Center Expert Help Knowledge Center AWS Support Overview AWS re:Post Click here to return to Amazon Web Services homepage Get Started for Free Contact Us Products Solutions Pricing Introduction to AWS Getting Started Documentation Training and Certification Developer Center Customer Success Partner Network AWS Marketplace Support AWS re:Post Log into Console Download the Mobile App AWS Blog Home Blogs Editions Close Architecture AWS Cloud Operations & Migrations AWS for Games AWS Insights AWS Marketplace AWS News AWS Partner Network AWS Smart Business Big Data Business Intelligence Business Productivity Cloud Enterprise Strategy Cloud Financial Management Compute Contact Center Containers Database Desktop & Application Streaming Developer Tools DevOps Front-End Web & Mobile HPC Industries Integration & Automation Internet of Things Machine Learning Media Messaging & Targeting Microsoft Workloads on AWS .NET on AWS Networking & Content Delivery Open Source Public Sector Quantum Computing Robotics SAP Security Spatial Computing Startups Storage Supply Chain & Logistics Training & Certification Close 中国版 日本版 한국 에디션 기술 블로그 Edisi Bahasa Indonesia AWS Thai Blog Édition Française Deutsche Edition Edição em Português Edición en Español Версия на русском Türkçe Sürüm AWS Machine Learning Blog Learn how to build and deploy tool-using LLM agents using AWS SageMaker JumpStart Foundation Models by John Hwang | on 15 SEP 2023 | in Advanced (300) , Amazon SageMaker JumpStart , Technical How-to | Permalink | Comments | Share Large language model (LLM) agents are programs that extend the capabilities of standalone LLMs with 1) access to external tools (APIs, functions, webhooks, plugins, and so on), and 2) the ability to plan and execute tasks in a self-directed fashion. Often, LLMs need to interact with other software, databases, or APIs to accomplish complex tasks. For example, an administrative chatbot that schedules meetings would require access to employees’ calendars and email. With access to tools, LLM agents can become more powerful—at the cost of additional complexity. In this post, we introduce LLM agents and demonstrate how to build and deploy an e-commerce LLM agent using Amazon SageMaker JumpStart and AWS Lambda. The agent will use tools to provide new capabilities, such as answering questions about returns (“Is my return rtn001 processed?”) and providing updates about orders (“Could you tell me if order 123456 has shipped?”). These new capabilities require LLMs to fetch data from multiple data sources ( orders , returns ) and perform retrieval augmented generation (RAG). To power the LLM agent, we use a Flan-UL2 model deployed as a SageMaker endpoint and use data retrieval tools built with AWS Lambda. The agent can subsequently be integrated with Amazon Lex and used as a chatbot inside websites or AWS Connect . We conclude the post with items to consider before deploying LLM agents to production. For a fully managed experience for building LLM agents, AWS also provides the agents for Amazon Bedrock feature (in preview) . A brief overview of LLM agent architectures LLM agents are programs that use LLMs to decide when and how to use tools as necessary to complete complex tasks. With tools and task planning abilities, LLM agents can interact with outside systems and overcome traditional limitations of LLMs, such as knowledge cutoffs, hallucinations, and imprecise calculations. Tools can take a variety of forms, such as API calls, Python functions, or webhook-based plugins. For example, an LLM can use a “retrieval plugin” to fetch relevant context and perform RAG. So what does it mean for an LLM to pick tools and plan tasks? There are numerous approaches (such as ReAct , MRKL , Toolformer, HuggingGPT , and Transformer Agent s) to using LLMs with tools, and advancements are happening rapidly. But one simple way is to prompt an LLM with a list of tools and ask it to determine 1) if a tool is needed to satisfy the user query, and if so, 2) select the appropriate tool. Such a prompt typically looks like the following example and may include few-shot examples to improve the LLM’s reliability in picking the right tool. ‘’’\n",
      "Your task is to select a tool to answer a user question. You have access to the following tools.\n",
      "\n",
      "search: search for an answer in FAQs\n",
      "order: order items\n",
      "noop: no tool is needed\n",
      "\n",
      "{few shot examples}\n",
      "\n",
      "Question: {input}\n",
      "Tool:\n",
      "‘’’ More complex approaches involve using a specialized LLM that can directly decode “API calls” or “tool use,” such as GorillaLLM . Such finetuned LLMs are trained on API specification datasets to recognize and predict API calls based on instruction. Often, these LLMs require some metadata about available tools (descriptions, yaml, or JSON schema for their input parameters) in order to output tool invocations. This approach is taken by agents for Amazon Bedrock and OpenAI function calls . Note that LLMs generally need to be sufficiently large and complex in order to show tool selection ability. Assuming task planning and tool selection mechanisms are chosen, a typical LLM agent program works in the following sequence: User request – The program takes a user input such as “Where is my order 123456 ?” from some client application. Plan next action(s) and select tool(s) to use – Next, the program uses a prompt to have the LLM generate the next action, for example, “Look up the orders table using OrdersAPI .” The LLM is prompted to suggest a tool name such as OrdersAPI from a predefined list of available tools and their descriptions. Alternatively, the LLM could be instructed to directly generate an API call with input parameters such as OrdersAPI(12345) . Note that the next action may or may not involve using a tool or API. If not, the LLM would respond to user input without incorporating additional context from tools or simply return a canned response such as, “I cannot answer this question.” Parse tool request – Next, we need to parse out and validate the tool/action prediction suggested by the LLM. Validation is needed to ensure tool names, APIs, and request parameters aren’t hallucinated and that the tools are properly invoked according to specification. This parsing may require a separate LLM call. Invoke tool – Once valid tool name(s) and parameter(s) are ensured, we invoke the tool. This could be an HTTP request, function call, and so on. Parse output – The response from the tool may need additional processing. For example, an API call may result in a long JSON response, where only a subset of fields are of interest to the LLM. Extracting information in a clean, standardized format can help the LLM interpret the result more reliably. Interpret output – Given the output from the tool, the LLM is prompted again to make sense of it and decide whether it can generate the final answer back to the user or whether additional actions are required. Terminate or continue to step 2 – Either return a final answer or a default answer in the case of errors or timeouts. Different agent frameworks execute the previous program flow differently. For example, ReAct combines tool selection and final answer generation into a single prompt, as opposed to using separate prompts for tool selection and answer generation. Also, this logic can be run in a single pass or run in a while statement (the “agent loop”), which terminates when the final answer is generated, an exception is thrown, or timeout occurs. What remains constant is that agents use the LLM as the centerpiece to orchestrate planning and tool invocations until the task terminates. Next, we show how to implement a simple agent loop using AWS services. Solution overview For this blog post, we implement an e-commerce support LLM agent that provides two functionalities powered by tools: Return status retrieval tool – Answer questions about the status of returns such as, “What is happening to my return rtn001 ?” Order status retrieval tool – Track the status of orders such as, “What’s the status of my order 123456 ?” The agent effectively uses the LLM as a query router. Given a query (“What is the status of order 123456 ?”), select the appropriate retrieval tool to query across multiple data sources (that is, returns and orders). We accomplish query routing by having the LLM pick among multiple retrieval tools, which are responsible for interacting with a data source and fetching context. This extends the simple RAG pattern, which assumes a single data source. Both retrieval tools are Lambda functions that take an id ( orderId or returnId ) as input, fetches a JSON object from the data source, and converts the JSON into a human friendly representation string that’s suitable to be used by LLM. The data source in a real-world scenario could be a highly scalable NoSQL database such as DynamoDB , but this solution employs simple Python Dict with sample data for demo purposes. Additional functionalities can be added to the agent by adding Retrieval Tools and modifying prompts accordingly. This agent can be tested a standalone service that integrates with any UI over HTTP, which can be done easily with Amazon Lex . Here are some additional details about the key components: LLM inference endpoint – The core of an agent program is an LLM. We will use SageMaker JumpStar\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `scrape_text` with `{'url': 'https://huyenchip.com/2023/04/11/llm-engineering.html'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mBuilding LLM applications for production Building LLM applications for production | Chip Huyen Chip Huyen Blog Books List 100 MLOps Guide Tiếng Việt Building LLM applications for production Apr 11, 2023 • Chip Huyen [ Hacker News discussion , LinkedIn discussion , Twitter thread ] A question that I’ve been asked a lot recently is how large language models (LLMs) will change machine learning workflows. After working with several companies who are working with LLM applications and personally going down a rabbit hole building my applications, I realized two things: It’s easy to make something cool with LLMs, but very hard to make something production-ready with them. LLM limitations are exacerbated by a lack of engineering rigor in prompt engineering, partially due to the ambiguous nature of natural languages, and partially due to the nascent nature of the field. This post consists of three parts. Part 1 discusses the key challenges of productionizing LLM applications and the solutions that I’ve seen. Part 2 discusses how to compose multiple tasks with control flows (e.g. if statement, for loop) and incorporate tools (e.g. SQL executor, bash, web browsers, third-party APIs) for more complex and powerful applications. Part 3 covers some of the promising use cases that I’ve seen companies building on top of LLMs and how to construct them from smaller tasks. There has been so much written about LLMs, so feel free to skip any section you’re already familiar with. Table of contents Part I. Challenges of productionizing prompt engineering …….. The ambiguity of natural languages ………… Prompt evaluation ………… Prompt versioning ………… Prompt optimization …….. Cost and latency ………… Cost ………… Latency ………… The impossibility of cost + latency analysis for LLMs …….. Prompting vs. finetuning vs. alternatives ………… Prompt tuning ………… Finetuning with distillation …….. Embeddings + vector databases …….. Backward and forward compatibility Part 2. Task composability …….. Applications that consist of multiple tasks …….. Agents, tools, and control flows ………… Tools vs. plugins ………… Control flows: sequential, parallel, if, for loop ………… Control flow with LLM agents ………… Testing an agent Part 3. Promising use cases …….. AI assistant …….. Chatbot …….. Programming and gaming …….. Learning …….. Talk-to-your-data ………… Can LLMs do data analysis for me? …….. Search and recommendation …….. Sales …….. SEO Conclusion Part I. Challenges of productionizing prompt engineering The ambiguity of natural languages For most of the history of computers, engineers have written instructions in programming languages. Programming languages are “mostly” exact. Ambiguity causes frustration and even passionate hatred in developers (think dynamic typing in Python or JavaScript). In prompt engineering, instructions are written in natural languages, which are a lot more flexible than programming languages. This can make for a great user experience, but can lead to a pretty bad developer experience. The flexibility comes from two directions: how users define instructions, and how LLMs respond to these instructions. First, the flexibility in user-defined prompts leads to silent failures. If someone accidentally makes some changes in code, like adding a random character or removing a line, it’ll likely throw an error. However, if someone accidentally changes a prompt, it will still run but give very different outputs. While the flexibility in user-defined prompts is just an annoyance, the ambiguity in LLMs’ generated responses can be a dealbreaker. It leads to two problems: Ambiguous output format : downstream applications on top of LLMs expect outputs in a certain format so that they can parse. We can craft our prompts to be explicit about the output format, but there’s no guarantee that the outputs will always follow this format. Inconsistency in user experience : when using an application, users expect certain consistency. Imagine an insurance company giving you a different quote every time you check on their website. LLMs are stochastic – there’s no guarantee that an LLM will give you the same output for the same input every time. You can force an LLM to give the same response by setting temperature = 0 , which is, in general, a good practice. While it mostly solves the consistency problem , it doesn’t inspire trust in the system. Imagine a teacher who gives you consistent scores only if that teacher sits in one particular room. If that teacher sits in different rooms, that teacher’s scores for you will be wild. How to solve this ambiguity problem? This seems to be a problem that OpenAI is actively trying to mitigate. They have a notebook with tips on how to increase their models’ reliability. A couple of people who’ve worked with LLMs for years told me that they just accepted this ambiguity and built their workflows around that. It’s a different mindset compared to developing deterministic programs, but not something impossible to get used to. This ambiguity can be mitigated by applying as much engineering rigor as possible. In the rest of this post, we’ll discuss how to make prompt engineering, if not deterministic, systematic. Prompt evaluation A common technique for prompt engineering is to provide in the prompt a few examples and hope that the LLM will generalize from these examples (fewshot learners). As an example, consider trying to give a text a controversy score – it was a fun project that I did to find the correlation between a tweet’s popularity and its controversialness. Here is the shortened prompt with 4 fewshot examples: Example: controversy scorer Given a text, give it a controversy score from 0 to 10.\n",
      "\n",
      "Examples:\n",
      "\n",
      "1 + 1 = 2\n",
      "Controversy score: 0\n",
      "\n",
      "Starting April 15th, only verified accounts on Twitter will be eligible to be in For You recommendations\n",
      "Controversy score: 5\n",
      "\n",
      "Everyone has the right to own and use guns\n",
      "Controversy score: 9\n",
      "\n",
      "Immigration should be completely banned to protect our country\n",
      "Controversy score: 10\n",
      "\n",
      "The response should follow the format:\n",
      "\n",
      "Controversy score: { score }\n",
      "Reason: { reason }\n",
      "\n",
      "Here is the text. When doing fewshot learning, two questions to keep in mind: Whether the LLM understands the examples given in the prompt . One way to evaluate this is to input the same examples and see if the model outputs the expected scores. If the model doesn’t perform well on the same examples given in the prompt, it is likely because the prompt isn’t clear – you might want to rewrite the prompt or break the task into smaller tasks (and combine them together, discussed in detail in Part II of this post). Whether the LLM overfits to these fewshot examples. You can evaluate your model on separate examples. One thing I’ve also found useful is to ask models to give examples for which it would give a certain label. For example, I can ask the model to give me examples of texts for which it’d give a score of 4. Then I’d input these examples into the LLM to see if it’ll indeed output 4. from llm import OpenAILLM\n",
      "\n",
      "def eval_prompt(examples_file, eval_file):\n",
      "    prompt = get_prompt(examples_file)\n",
      "    model = OpenAILLM(prompt=prompt, temperature=0)\n",
      "    compute_rmse(model, examples_file)\n",
      "    compute_rmse(model, eval_file)\n",
      "eval_prompt(\"fewshot_examples.txt\", \"eval_examples.txt\") Prompt versioning Small changes to a prompt can lead to very different results. It’s essential to version and track the performance of each prompt. You can use git to version each prompt and its performance, but I wouldn’t be surprised if there will be tools like MLflow or Weights & Biases for prompt experiments. Prompt optimization There have been many papers + blog posts written on how to optimize prompts. I agree with Lilian Weng in her helpful blog post that most papers on prompt engineering are tricks that can be explained in a few sentences. OpenAI has a great notebook that explains many tips with examples . Here are some of them: Prompt the model to explain or explain step-by-step how it arrives at an answer, a technique known as Chain-of-Thought or COT (Wei et al., 2022). Tradeoff : COT can increase both latency and cost due to the increased number of output tokens [see Cost and latency section] Generate many outputs for the same input. Pick the final output by either the majority vote  (also known as self-consistency technique by Wang et al., 2023) or you can ask your LLM to pick the best one. In OpenAI API, you can generate multiple responses for the same input by passing in the argument n (not an ideal API design if you ask me). Break one big prompt into smaller, simpler prompts. Many tools promise to auto-optimize your prompts – they are quite expensive and usually just apply these tricks. One nice thing about these tools is that they’re no code, which makes them appealing to non-coders. Cost and latency Cost The more explicit detail and examples you put into the prompt, the better the model performance (hopefully), and the more expensive your inference will cost. OpenAI API charges for both the input and output tokens. Depending on the task, a simple prompt might be anything between 300 - 1000 tokens. If you want to include more context, e.g. adding your own documents or info retrieved from the Internet to the prompt, it can easily go up to 10k tokens for the prompt alone. The cost with long prompts isn’t in experimentation but in inference. Experimentation-wise, prompt engineering is a cheap and fast way get something up and running . For example, even if you use GPT-4 with the following setting, your experimentation cost will still be just over $300. The traditional ML cost of collecting data and training models is usually much higher and takes much longer. Prompt: 10k tokens ($0.06/1k tokens) Output: 200 tokens ($0.12/1k tokens) Evaluate on 20 examples Experiment with 25 different versions of prompts The cost of LLMOps is in inference. If you use GPT-4 with 10k tokens in input and 200 tokens in output, it’ll be $0.624 / prediction. If you use GPT-3.5-turbo with 4k tok\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `scrape_text` with `{'url': 'https://www.seldon.io/deploying-large-language-models-in-production'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mFailed to retrieve the webpage: Status code 403\u001b[0m\u001b[32;1m\u001b[1;3mI apologize, but I am unable to access the content from the provided URL. However, based on my research, here are some general best practices for deploying LLM agents to production:\n",
      "\n",
      "1. Infrastructure: Ensure that you have a robust and scalable infrastructure to handle the computational requirements of LLM agents. This may involve using cloud-based services or dedicated hardware.\n",
      "\n",
      "2. Model Training: Train your LLM agents on a diverse and representative dataset to improve their performance and generalization capabilities. Consider using transfer learning techniques to leverage pre-trained models.\n",
      "\n",
      "3. Fine-tuning: Fine-tune your LLM agents on specific tasks or domains to improve their performance and adapt them to your specific use case. This may involve using task-specific datasets and optimizing hyperparameters.\n",
      "\n",
      "4. Prompt Engineering: Craft clear and specific prompts to guide the output of LLM agents. Experiment with different prompt formats and techniques, such as few-shot learning, to improve the quality and relevance of the generated responses.\n",
      "\n",
      "5. Model Monitoring: Implement monitoring and logging mechanisms to track the performance and behavior of LLM agents in production. This can help identify issues, detect drift, and ensure the reliability of the deployed models.\n",
      "\n",
      "6. Security and Privacy: Implement appropriate security measures to protect the LLM agents and the data they process. This may involve encryption, access controls, and compliance with data protection regulations.\n",
      "\n",
      "7. Continuous Improvement: Continuously evaluate and iterate on the performance of LLM agents in production. Collect user feedback, monitor metrics, and incorporate updates and improvements to enhance the user experience and achieve better results.\n",
      "\n",
      "Please note that these are general best practices, and the specific deployment process may vary depending on the LLM framework and tools you are using. It is recommended to consult the documentation and resources provided by the specific LLM framework or platform you are working with for more detailed guidance.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What are the current best practices to deploy LLM agents to production?',\n",
       " 'output': 'I apologize, but I am unable to access the content from the provided URL. However, based on my research, here are some general best practices for deploying LLM agents to production:\\n\\n1. Infrastructure: Ensure that you have a robust and scalable infrastructure to handle the computational requirements of LLM agents. This may involve using cloud-based services or dedicated hardware.\\n\\n2. Model Training: Train your LLM agents on a diverse and representative dataset to improve their performance and generalization capabilities. Consider using transfer learning techniques to leverage pre-trained models.\\n\\n3. Fine-tuning: Fine-tune your LLM agents on specific tasks or domains to improve their performance and adapt them to your specific use case. This may involve using task-specific datasets and optimizing hyperparameters.\\n\\n4. Prompt Engineering: Craft clear and specific prompts to guide the output of LLM agents. Experiment with different prompt formats and techniques, such as few-shot learning, to improve the quality and relevance of the generated responses.\\n\\n5. Model Monitoring: Implement monitoring and logging mechanisms to track the performance and behavior of LLM agents in production. This can help identify issues, detect drift, and ensure the reliability of the deployed models.\\n\\n6. Security and Privacy: Implement appropriate security measures to protect the LLM agents and the data they process. This may involve encryption, access controls, and compliance with data protection regulations.\\n\\n7. Continuous Improvement: Continuously evaluate and iterate on the performance of LLM agents in production. Collect user feedback, monitor metrics, and incorporate updates and improvements to enhance the user experience and achieve better results.\\n\\nPlease note that these are general best practices, and the specific deployment process may vary depending on the LLM framework and tools you are using. It is recommended to consult the documentation and resources provided by the specific LLM framework or platform you are working with for more detailed guidance.'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\": \"What are the current best practices to deploy LLM agents to production?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We encounter a few issues when we try to just add the scrape function to the agent without any other consideration. \n",
    "How about we make this better by improving the inner chain of the agent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThe word \"educa\" has 5 letters.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `web_search` with `{'query': 'define educa'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m['https://en.wikipedia.org/wiki/Education', 'https://www.merriam-webster.com/dictionary/education', 'https://www.dictionary.com/browse/education', 'https://www.britannica.com/topic/education', 'https://dictionary.cambridge.org/us/dictionary/english/education', 'https://learn.org/articles/What_is_Education.html', 'https://leverageedu.com/blog/what-is-education/', 'https://kids.wordsmyth.net/we/?rid=13075&ent_l=education', 'https://link.springer.com/chapter/10.1007/978-3-030-83925-3_3', 'https://examplanning.com/definition-of-education-by-different-authors/']\u001b[0m\u001b[32;1m\u001b[1;3mYes, \"educa\" is not a recognized word in the English language. It may be a misspelling or a variation of the word \"education\".\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'is that a real word?',\n",
       " 'chat_history': [HumanMessage(content='how many letters in the word educa?'),\n",
       "  AIMessage(content='The word \"educa\" has 5 letters.')],\n",
       " 'output': 'Yes, \"educa\" is not a recognized word in the English language. It may be a misspelling or a variation of the word \"education\".'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import MessagesPlaceholder\n",
    "\n",
    "MEMORY_KEY = \"chat_history\"\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are very powerful assistant, but bad at calculating lengths of words.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=MEMORY_KEY),\n",
    "        (\"user\", \"{input}\"),\n",
    "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "chat_history = []\n",
    "\n",
    "agent = (\n",
    "    {\n",
    "        \"input\": lambda x: x[\"input\"],\n",
    "        \"agent_scratchpad\": lambda x: format_to_openai_tool_messages(\n",
    "            x[\"intermediate_steps\"]\n",
    "        ),\n",
    "        \"chat_history\": lambda x: x[\"chat_history\"],\n",
    "    }\n",
    "    | prompt\n",
    "    | llm_with_tools\n",
    "    | OpenAIToolsAgentOutputParser()\n",
    ")\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "input1 = \"how many letters in the word educa?\"\n",
    "result = agent_executor.invoke({\"input\": input1, \"chat_history\": chat_history})\n",
    "chat_history.extend(\n",
    "    [\n",
    "        HumanMessage(content=input1),\n",
    "        AIMessage(content=result[\"output\"]),\n",
    "    ]\n",
    ")\n",
    "agent_executor.invoke({\"input\": \"is that a real word?\", \"chat_history\": chat_history})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oreilly-agents",
   "language": "python",
   "name": "oreilly-agents"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
