{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a research Agent with LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chat = ChatOpenAI(temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://en.wikipedia.org/wiki/Artificial_intelligence',\n",
       " 'https://www.ibm.com/topics/artificial-intelligence',\n",
       " 'https://www.techtarget.com/searchenterpriseai/definition/AI-Artificial-Intelligence',\n",
       " 'https://builtin.com/artificial-intelligence',\n",
       " 'https://cloud.google.com/learn/what-is-artificial-intelligence',\n",
       " 'https://www.investopedia.com/terms/a/artificial-intelligence-ai.asp']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.utilities import SerpAPIWrapper\n",
    "from langchain.agents import tool\n",
    "\n",
    "search = SerpAPIWrapper()\n",
    "\n",
    "@tool\n",
    "def web_search(query: str):\n",
    "    \"\"\"Searches the web given a user query and returns the most relevant links\"\"\"\n",
    "    output = search.results(query)\n",
    "    results = output[\"organic_results\"]\n",
    "    return [r[\"link\"] for r in results]\n",
    "\n",
    "web_search(\"Artificial Intelligence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['agent_scratchpad', 'input'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]], 'agent_scratchpad': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a helpful assistant')), MessagesPlaceholder(variable_name='chat_history', optional=True), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}')), MessagesPlaceholder(variable_name='agent_scratchpad')])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import hub\n",
    "\n",
    "prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['agent_scratchpad', 'input']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.input_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]],\n",
       " 'agent_scratchpad': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.input_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': None,\n",
       " 'input_variables': ['agent_scratchpad', 'input'],\n",
       " 'input_types': {'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]],\n",
       "  'agent_scratchpad': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]},\n",
       " 'output_parser': None,\n",
       " 'partial_variables': {},\n",
       " 'messages': [SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a helpful assistant')),\n",
       "  MessagesPlaceholder(variable_name='chat_history', optional=True),\n",
       "  HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}')),\n",
       "  MessagesPlaceholder(variable_name='agent_scratchpad')],\n",
       " 'validate_template': False}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build an agent I need to do 2 essential things:\n",
    "\n",
    "1. Connect a model to tools\n",
    "2. Set up a loop in which the model continuously tries to solve the problem by either just calling the llm or calling a tool, observing the result and repeating until finished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['agent_scratchpad', 'input'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]], 'agent_scratchpad': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a helpful assistant')), MessagesPlaceholder(variable_name='chat_history', optional=True), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}')), MessagesPlaceholder(variable_name='agent_scratchpad')])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_openai_tools_agent\n",
    "\n",
    "tools = [web_search]\n",
    "\n",
    "react_agent = create_openai_tools_agent(llm_chat, tools, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor\n",
    "\n",
    "\n",
    "react_agent_executor = AgentExecutor(agent=react_agent, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThe best way to learn a new language may vary depending on your learning style and preferences. However, here are some effective strategies that can help:\n",
      "\n",
      "1. Immerse yourself in the language: Surround yourself with the language as much as possible. Watch movies, TV shows, and videos in the target language, listen to podcasts or music, and try to have conversations with native speakers.\n",
      "\n",
      "2. Take a structured course: Enroll in a language course, either online or in-person, that provides a structured curriculum and guidance from a qualified instructor. This can help you build a strong foundation and ensure you cover all the necessary aspects of the language.\n",
      "\n",
      "3. Practice regularly: Consistency is key when learning a new language. Set aside dedicated time each day or week to practice listening, speaking, reading, and writing in the target language. Use language learning apps, flashcards, or language exchange platforms to practice regularly.\n",
      "\n",
      "4. Use a variety of resources: Explore different resources such as textbooks, online courses, language learning apps, and websites to expose yourself to different learning materials and approaches. Find resources that suit your learning style and interests.\n",
      "\n",
      "5. Engage in conversation: Find opportunities to practice speaking with native speakers or language learners. Join language exchange groups, attend language meetups, or find a language partner online. Speaking with others will help improve your fluency and confidence.\n",
      "\n",
      "6. Set realistic goals: Set specific, achievable goals for your language learning journey. Whether it's mastering a certain number of vocabulary words, completing a language level, or having a conversation in the target language, setting goals can keep you motivated and focused.\n",
      "\n",
      "7. Be patient and persistent: Learning a new language takes time and effort. Don't get discouraged by setbacks or slow progress. Stay motivated, be patient with yourself, and celebrate small victories along the way.\n",
      "\n",
      "Remember, everyone learns differently, so it's important to find what works best for you. Experiment with different methods and strategies to discover your own effective learning approach.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What is the best way to learn a new language?',\n",
       " 'agent_scratchpad': [],\n",
       " 'output': \"The best way to learn a new language may vary depending on your learning style and preferences. However, here are some effective strategies that can help:\\n\\n1. Immerse yourself in the language: Surround yourself with the language as much as possible. Watch movies, TV shows, and videos in the target language, listen to podcasts or music, and try to have conversations with native speakers.\\n\\n2. Take a structured course: Enroll in a language course, either online or in-person, that provides a structured curriculum and guidance from a qualified instructor. This can help you build a strong foundation and ensure you cover all the necessary aspects of the language.\\n\\n3. Practice regularly: Consistency is key when learning a new language. Set aside dedicated time each day or week to practice listening, speaking, reading, and writing in the target language. Use language learning apps, flashcards, or language exchange platforms to practice regularly.\\n\\n4. Use a variety of resources: Explore different resources such as textbooks, online courses, language learning apps, and websites to expose yourself to different learning materials and approaches. Find resources that suit your learning style and interests.\\n\\n5. Engage in conversation: Find opportunities to practice speaking with native speakers or language learners. Join language exchange groups, attend language meetups, or find a language partner online. Speaking with others will help improve your fluency and confidence.\\n\\n6. Set realistic goals: Set specific, achievable goals for your language learning journey. Whether it's mastering a certain number of vocabulary words, completing a language level, or having a conversation in the target language, setting goals can keep you motivated and focused.\\n\\n7. Be patient and persistent: Learning a new language takes time and effort. Don't get discouraged by setbacks or slow progress. Stay motivated, be patient with yourself, and celebrate small victories along the way.\\n\\nRemember, everyone learns differently, so it's important to find what works best for you. Experiment with different methods and strategies to discover your own effective learning approach.\"}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "react_agent_executor.invoke({\"input\": \"What is the best way to learn a new language?\",\n",
    "                             \"agent_scratchpad\": []})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThe best way to learn good programming practices is through a combination of studying and hands-on practice. Here are some steps you can take:\n",
      "\n",
      "1. Read books and online resources: There are many books and online resources available that cover programming best practices. Some popular books include \"Clean Code\" by Robert C. Martin and \"The Pragmatic Programmer\" by Andrew Hunt and David Thomas. These resources can provide you with a solid foundation in programming principles and best practices.\n",
      "\n",
      "2. Take online courses or tutorials: Online courses and tutorials can provide structured learning experiences and help you understand programming best practices in a practical way. Platforms like Coursera, Udemy, and Codecademy offer a wide range of programming courses that cover best practices.\n",
      "\n",
      "3. Join coding communities: Participating in coding communities, such as forums, online communities, and coding meetups, can expose you to different perspectives and approaches to programming. You can learn from experienced programmers and get feedback on your code, which can help you improve your programming practices.\n",
      "\n",
      "4. Practice coding regularly: The more you practice coding, the better you will become at applying programming best practices. Try to work on small coding projects or solve coding challenges regularly. This will help you reinforce your understanding of best practices and improve your coding skills.\n",
      "\n",
      "5. Review and refactor your code: As you gain more experience, regularly review and refactor your code. This involves going back to your previous projects and improving the code by applying the best practices you have learned. This iterative process will help you develop a habit of writing clean and maintainable code.\n",
      "\n",
      "6. Learn from experienced programmers: Seek opportunities to learn from experienced programmers. This can be through mentorship programs, internships, or working on open-source projects. Observing how experienced programmers approach problems and write code can provide valuable insights into good programming practices.\n",
      "\n",
      "Remember, learning good programming practices is an ongoing process. It takes time and practice to develop the skills and habits necessary to write clean, efficient, and maintainable code.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What is the best way to learn good programming practices?',\n",
       " 'agent_scratchpad': [],\n",
       " 'output': 'The best way to learn good programming practices is through a combination of studying and hands-on practice. Here are some steps you can take:\\n\\n1. Read books and online resources: There are many books and online resources available that cover programming best practices. Some popular books include \"Clean Code\" by Robert C. Martin and \"The Pragmatic Programmer\" by Andrew Hunt and David Thomas. These resources can provide you with a solid foundation in programming principles and best practices.\\n\\n2. Take online courses or tutorials: Online courses and tutorials can provide structured learning experiences and help you understand programming best practices in a practical way. Platforms like Coursera, Udemy, and Codecademy offer a wide range of programming courses that cover best practices.\\n\\n3. Join coding communities: Participating in coding communities, such as forums, online communities, and coding meetups, can expose you to different perspectives and approaches to programming. You can learn from experienced programmers and get feedback on your code, which can help you improve your programming practices.\\n\\n4. Practice coding regularly: The more you practice coding, the better you will become at applying programming best practices. Try to work on small coding projects or solve coding challenges regularly. This will help you reinforce your understanding of best practices and improve your coding skills.\\n\\n5. Review and refactor your code: As you gain more experience, regularly review and refactor your code. This involves going back to your previous projects and improving the code by applying the best practices you have learned. This iterative process will help you develop a habit of writing clean and maintainable code.\\n\\n6. Learn from experienced programmers: Seek opportunities to learn from experienced programmers. This can be through mentorship programs, internships, or working on open-source projects. Observing how experienced programmers approach problems and write code can provide valuable insights into good programming practices.\\n\\nRemember, learning good programming practices is an ongoing process. It takes time and practice to develop the skills and habits necessary to write clean, efficient, and maintainable code.'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "react_agent_executor.invoke({\"input\": \"What is the best way to learn good programming practices?\",\n",
    "                             \"agent_scratchpad\": []})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `web_search` with `{'query': 'most popular open source frameworks to build llm-based agents in Python'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m['https://github.com/kaushikb11/awesome-llm-agents', 'https://www.taskade.com/blog/top-autonomous-agents/', 'https://www.unite.ai/best-open-source-llms/', 'https://www.reddit.com/r/LocalLLaMA/comments/17m2lql/best_framework_for_llm_based_applications_in/', 'https://github.com/tensorchord/Awesome-LLMOps', 'https://pub.towardsai.net/inside-agents-the-new-open-source-framework-for-building-semi-autonomous-llm-agents-a78ffa172902', 'https://www.marktechpost.com/2024/01/03/10-open-source-tools-for-llm-applications-development/', 'https://sourceforge.net/directory/large-language-models-llm/python/', 'https://medium.com/@bijit211987/top-llm-dev-tool-and-when-to-use-them-in-your-ai-stack-622a651ece06']\u001b[0m\u001b[32;1m\u001b[1;3mHere are 5 relevant URLs for the most popular open source frameworks to build LLM-based agents in Python:\n",
      "\n",
      "1. [Awesome LLM Agents](https://github.com/kaushikb11/awesome-llm-agents)\n",
      "2. [Top Autonomous Agents](https://www.taskade.com/blog/top-autonomous-agents/)\n",
      "3. [Best Open Source LLMs](https://www.unite.ai/best-open-source-llms/)\n",
      "4. [Best Framework for LLM-based Applications](https://www.reddit.com/r/LocalLLaMA/comments/17m2lql/best_framework_for_llm_based_applications_in/)\n",
      "5. [Awesome LLMOps](https://github.com/tensorchord/Awesome-LLMOps)\n",
      "\n",
      "You can visit these URLs to learn more about the frameworks and choose the one that suits your needs.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Research the current most popular open source frameworks to build llm-based agents in Python and return 5 releavnt urls.',\n",
       " 'agent_scratchpad': [],\n",
       " 'output': 'Here are 5 relevant URLs for the most popular open source frameworks to build LLM-based agents in Python:\\n\\n1. [Awesome LLM Agents](https://github.com/kaushikb11/awesome-llm-agents)\\n2. [Top Autonomous Agents](https://www.taskade.com/blog/top-autonomous-agents/)\\n3. [Best Open Source LLMs](https://www.unite.ai/best-open-source-llms/)\\n4. [Best Framework for LLM-based Applications](https://www.reddit.com/r/LocalLLaMA/comments/17m2lql/best_framework_for_llm_based_applications_in/)\\n5. [Awesome LLMOps](https://github.com/tensorchord/Awesome-LLMOps)\\n\\nYou can visit these URLs to learn more about the frameworks and choose the one that suits your needs.'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "react_agent_executor.invoke({\"input\": \"Research the current most popular open source frameworks to build llm-based agents in Python and return 5 releavnt urls.\",\n",
    "                             \"agent_scratchpad\": []})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's build this same agent but using a more in depth approach going one level down in the abstraction layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's go down the latter of abstraction and build this same agent but leveraging the lower-level capabilities of LangChain's LCEL language. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chat = ChatOpenAI(model=\"gpt-3.5-turbo-16k\",temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import tool\n",
    "from langchain.utilities import SerpAPIWrapper\n",
    "\n",
    "search = SerpAPIWrapper()\n",
    "\n",
    "@tool\n",
    "def web_search(query: str):\n",
    "    \"\"\"Searches the web given a user query and returns the most relevant links\"\"\"\n",
    "    output = search.results(query)\n",
    "    results = output[\"organic_results\"]\n",
    "    return [r[\"link\"] for r in results]\n",
    "\n",
    "\n",
    "tools = [web_search]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\", \"You are a powerful research assistant. Given a user query you will perform research on the topic using the tools available.\"\n",
    "        ),\n",
    "        (\"user\", \"{input}\"),\n",
    "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bind tools to the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.convert_to_openai import format_tool_to_openai_tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_with_tools = llm_chat.bind(tools=[format_tool_to_openai_tool(tool) for tool in tools])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, all we have to do is import two last utility functions:\n",
    "- a component for formatting intermediate steps (agent action, tool output pairs) to input messages that can be sent to the model, \n",
    "- and a component for converting the output message into an agent action/agent finish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.format_scratchpad.openai_tools import format_to_openai_tool_messages\n",
    "from langchain.agents.output_parsers.openai_tools import OpenAIToolsAgentOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = (\n",
    "    {\n",
    "        \"input\": lambda x: x[\"input\"], \n",
    "        \"agent_scratchpad\": lambda x: format_to_openai_tool_messages(\n",
    "            x[\"intermediate_steps\"]\n",
    "        ),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm_with_tools\n",
    "    | OpenAIToolsAgentOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor\n",
    "\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `web_search` with `{'query': 'latest open source LLMs'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m['https://github.com/eugeneyan/open-llms', 'https://www.datacamp.com/blog/top-open-source-llms', 'https://www.unite.ai/best-open-source-llms/', 'https://thenewstack.io/large-language-models-open-source-llms-in-2023/', 'https://medium.datadriveninvestor.com/list-of-open-source-large-language-models-llms-4eac551bda2e', 'https://github.com/Hannibal046/Awesome-LLM', 'https://www.techopedia.com/6-best-open-source-llms-to-watch-out-for-in-2024', 'https://www.upstage.ai/feed/insight/top-open-source-llms-2024', 'https://www.simform.com/blog/open-source-llm/', 'https://blog.n8n.io/open-source-llm/']\u001b[0m\u001b[32;1m\u001b[1;3mHere are some of the latest open source LLMs available:\n",
      "\n",
      "1. [Open-LLMs](https://github.com/eugeneyan/open-llms): A collection of open source LLMs on GitHub.\n",
      "\n",
      "2. [Top Open Source LLMs](https://www.datacamp.com/blog/top-open-source-llms): A blog post that lists some of the top open source LLMs.\n",
      "\n",
      "3. [Best Open Source LLMs](https://www.unite.ai/best-open-source-llms/): An article that highlights the best open source LLMs available.\n",
      "\n",
      "4. [Large Language Models: Open Source LLMs in 2023](https://thenewstack.io/large-language-models-open-source-llms-in-2023/): A blog post that discusses the state of open source LLMs in 2023.\n",
      "\n",
      "5. [List of Open Source Large Language Models (LLMs)](https://medium.datadriveninvestor.com/list-of-open-source-large-language-models-llms-4eac551bda2e): A Medium article that provides a list of open source LLMs.\n",
      "\n",
      "Please note that the availability and popularity of open source LLMs may change over time, so it's always a good idea to check the latest resources and repositories for the most up-to-date information.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'actions': [OpenAIToolAgentAction(tool='web_search', tool_input={'query': 'latest open source LLMs'}, log=\"\\nInvoking: `web_search` with `{'query': 'latest open source LLMs'}`\\n\\n\\n\", message_log=[AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_Hj4El1AUkwDYVWnwuFZ3LqVh', 'function': {'arguments': '{\\n  \"query\": \"latest open source LLMs\"\\n}', 'name': 'web_search'}, 'type': 'function'}]})], tool_call_id='call_Hj4El1AUkwDYVWnwuFZ3LqVh')],\n",
       "  'messages': [AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_Hj4El1AUkwDYVWnwuFZ3LqVh', 'function': {'arguments': '{\\n  \"query\": \"latest open source LLMs\"\\n}', 'name': 'web_search'}, 'type': 'function'}]})]},\n",
       " {'steps': [AgentStep(action=OpenAIToolAgentAction(tool='web_search', tool_input={'query': 'latest open source LLMs'}, log=\"\\nInvoking: `web_search` with `{'query': 'latest open source LLMs'}`\\n\\n\\n\", message_log=[AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_Hj4El1AUkwDYVWnwuFZ3LqVh', 'function': {'arguments': '{\\n  \"query\": \"latest open source LLMs\"\\n}', 'name': 'web_search'}, 'type': 'function'}]})], tool_call_id='call_Hj4El1AUkwDYVWnwuFZ3LqVh'), observation=['https://github.com/eugeneyan/open-llms', 'https://www.datacamp.com/blog/top-open-source-llms', 'https://www.unite.ai/best-open-source-llms/', 'https://thenewstack.io/large-language-models-open-source-llms-in-2023/', 'https://medium.datadriveninvestor.com/list-of-open-source-large-language-models-llms-4eac551bda2e', 'https://github.com/Hannibal046/Awesome-LLM', 'https://www.techopedia.com/6-best-open-source-llms-to-watch-out-for-in-2024', 'https://www.upstage.ai/feed/insight/top-open-source-llms-2024', 'https://www.simform.com/blog/open-source-llm/', 'https://blog.n8n.io/open-source-llm/'])],\n",
       "  'messages': [FunctionMessage(content='[\"https://github.com/eugeneyan/open-llms\", \"https://www.datacamp.com/blog/top-open-source-llms\", \"https://www.unite.ai/best-open-source-llms/\", \"https://thenewstack.io/large-language-models-open-source-llms-in-2023/\", \"https://medium.datadriveninvestor.com/list-of-open-source-large-language-models-llms-4eac551bda2e\", \"https://github.com/Hannibal046/Awesome-LLM\", \"https://www.techopedia.com/6-best-open-source-llms-to-watch-out-for-in-2024\", \"https://www.upstage.ai/feed/insight/top-open-source-llms-2024\", \"https://www.simform.com/blog/open-source-llm/\", \"https://blog.n8n.io/open-source-llm/\"]', name='web_search')]},\n",
       " {'output': \"Here are some of the latest open source LLMs available:\\n\\n1. [Open-LLMs](https://github.com/eugeneyan/open-llms): A collection of open source LLMs on GitHub.\\n\\n2. [Top Open Source LLMs](https://www.datacamp.com/blog/top-open-source-llms): A blog post that lists some of the top open source LLMs.\\n\\n3. [Best Open Source LLMs](https://www.unite.ai/best-open-source-llms/): An article that highlights the best open source LLMs available.\\n\\n4. [Large Language Models: Open Source LLMs in 2023](https://thenewstack.io/large-language-models-open-source-llms-in-2023/): A blog post that discusses the state of open source LLMs in 2023.\\n\\n5. [List of Open Source Large Language Models (LLMs)](https://medium.datadriveninvestor.com/list-of-open-source-large-language-models-llms-4eac551bda2e): A Medium article that provides a list of open source LLMs.\\n\\nPlease note that the availability and popularity of open source LLMs may change over time, so it's always a good idea to check the latest resources and repositories for the most up-to-date information.\",\n",
       "  'messages': [AIMessage(content=\"Here are some of the latest open source LLMs available:\\n\\n1. [Open-LLMs](https://github.com/eugeneyan/open-llms): A collection of open source LLMs on GitHub.\\n\\n2. [Top Open Source LLMs](https://www.datacamp.com/blog/top-open-source-llms): A blog post that lists some of the top open source LLMs.\\n\\n3. [Best Open Source LLMs](https://www.unite.ai/best-open-source-llms/): An article that highlights the best open source LLMs available.\\n\\n4. [Large Language Models: Open Source LLMs in 2023](https://thenewstack.io/large-language-models-open-source-llms-in-2023/): A blog post that discusses the state of open source LLMs in 2023.\\n\\n5. [List of Open Source Large Language Models (LLMs)](https://medium.datadriveninvestor.com/list-of-open-source-large-language-models-llms-4eac551bda2e): A Medium article that provides a list of open source LLMs.\\n\\nPlease note that the availability and popularity of open source LLMs may change over time, so it's always a good idea to check the latest resources and repositories for the most up-to-date information.\")]}]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(agent_executor.stream({\"input\": \"What are the latest open source LLMs available?\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, what if we want the agent to actually read the links and return a research report rather than just the urls?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "@tool\n",
    "def scrape_text(url: str):\n",
    "    \"\"\"Scrapes the text from a given URL and returns it.\"\"\"\n",
    "    # Send a GET request to the webpage\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "\n",
    "        # Check if the request was successful\n",
    "        if response.status_code == 200:\n",
    "            # Parse the content of the request with BeautifulSoup\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "            # Extract all text from the webpage\n",
    "            page_text = soup.get_text(separator=\" \", strip=True)\n",
    "\n",
    "            # Print the extracted text\n",
    "            return page_text[:10000]\n",
    "        else:\n",
    "            return f\"Failed to retrieve the webpage: Status code {response.status_code}\"\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return f\"Failed to retrieve the webpage: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [web_search, scrape_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_with_tools = llm_chat.bind(tools=[format_tool_to_openai_tool(tool) for tool in tools])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\", \"You are a powerful research assistant. Given a user query you will perform research on the topic using the tools available.\"\n",
    "        ),\n",
    "        (\"user\", \"Scrape and summarize the contents from these urls: {urls}\"),\n",
    "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = (\n",
    "    {\n",
    "        \"urls\": lambda x: x[\"input\"], \n",
    "        \"agent_scratchpad\": lambda x: format_to_openai_tool_messages(\n",
    "            x[\"intermediate_steps\"]\n",
    "        ),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm_with_tools\n",
    "    | OpenAIToolsAgentOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor\n",
    "\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `scrape_text` with `{'url': 'https://towardsdatascience.com/deploying-llm-agents-to-production-best-practices-7e4e8e8e8e8e'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mMedium Open in app Sign up Sign in Write Sign up Sign in PAGE NOT FOUND 404 Out of nothing, something. You can find (just about) anything on Medium — apparently even a page that doesn’t exist. Maybe these stories about finding what you didn’t know you were looking for will take you somewhere new? Home How To Get Lost Paul Greenberg Feb 22, 2022 · 11 min read Member-only How To Get Lost Paul Greenberg Feb 22, 2022 · 11 min read Member-only Lost in Conversation Omar Merza in The Coffeelicious Sep 19, 2017 · 5 min read Lost in Conversation Omar Merza in The Coffeelicious Sep 19, 2017 · 5 min read The Lost Photographs of Penang Ted Anthony in Stuff Dot Life Sep 19, 2017 · 10 min read Member-only The Lost Photographs of Penang Ted Anthony in Stuff Dot Life Sep 19, 2017 · 10 min read Member-only To Anyone Who Has Lost Themselves Jamie Varon Jun 20, 2018 · 6 min read To Anyone Who Has Lost Themselves Jamie Varon Jun 20, 2018 · 6 min read\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `web_search` with `{'query': 'current best practices to deploy LLM agents to production'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m['https://medium.com/@aigeek_/best-practices-for-deploying-large-language-models-llms-in-production-fdc5bf240d6a', 'https://www.linkedin.com/pulse/deploying-llms-production-anatomy-llm-applications-xenonstack', 'https://arize.com/blog-course/large-language-model-llm-deployment/', 'https://www.youtube.com/watch?v=NRjUMB9BrhQ', 'https://aws.amazon.com/blogs/machine-learning/learn-how-to-build-and-deploy-tool-using-llm-agents-using-aws-sagemaker-jumpstart-foundation-models/', 'https://huyenchip.com/2023/04/11/llm-engineering.html', 'https://www.seldon.io/orchestrating-llms-in-production', 'https://www.qwak.com/post/best-practices-when-deploying-llms-in-production', 'https://www.reddit.com/r/LangChain/comments/14g7qui/discussion_biggest_roadblocks_to_deploy_llms_to/', 'https://superwise.ai/blog/considerations-best-practices-for-llm-architectures/']\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `scrape_text` with `{'url': 'https://medium.com/@aigeek_/best-practices-for-deploying-large-language-models-llms-in-production-fdc5bf240d6a'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mBest Practices for Deploying Large Language Models (LLMs) in Production | by ai geek (wishesh) | Medium Open in app Sign up Sign in Write Sign up Sign in Best Practices for Deploying Large Language Models (LLMs) in Production ai geek (wishesh) · Follow 10 min read · Jun 26, 2023 -- 2 Listen Share Large Language Models (LLMs) have revolutionized the field of natural language processing and understanding, enabling a wide range of AI applications across various domains. However, deploying LLM applications in production comes with its own set of challenges. From addressing the ambiguity of natural languages to managing costs and latency, there are several factors that require careful consideration. The ambiguity of natural languages poses a significant challenge when working with LLMs. Despite their impressive capabilities, LLMs can sometimes produce inconsistent and unexpected outputs, leading to silent failures. Prompt evaluation becomes crucial to ensure that the model understands the given examples and doesn’t overfit to them. Additionally, prompt versioning and optimization play vital roles in maintaining performance and cost-effectiveness. Cost and latency considerations are paramount when deploying LLM applications. Longer prompts increase the cost of inference , while the length of the output directly impacts latency. However, it is essential to note that cost and latency analysis for LLMs can quickly become outdated due to the rapid evolution of the field. Different approaches can be employed while working with LLMs, such as prompting , finetuning , and prompt tuning . Prompting is a quick and easy method that requires only a few examples, while finetuning enhances model performance but demands a larger volume of data. The combination of prompting and finetuning, known as prompt tuning, offers a promising approach to strike a balance. What Is a Language Model? | deepset Building LLM applications for production (huyenchip.com) Task composability is another critical aspect of building LLM applications. Many applications involve executing multiple tasks sequentially, in parallel, or based on conditions. LLM agents can be utilized to control task flow, while incorporating tools or plugins enables specific actions to be performed efficiently. Building LLM applications for production (huyenchip.com) LLMs have found promising use cases in various domains, including AI assistants, chatbots, programming and gaming, learning, talk-to-your-data applications, search and recommendation systems, sales, and SEO. These applications leverage the capabilities of LLMs to provide personalized and interactive experiences, enhancing user engagement. Understanding the strengths and limitations of LLMs and effectively leveraging their capabilities can lead to the development of innovative and impactful applications in diverse fields. In this article, we will delve deeper into the best practices for deploying LLMs, considering factors such as importance of data, cost effectiveness, prompt engineering, fine-tuning, task composability, and user experience. These best practices were suggested during the recent conference on LLM in Production by leading MLOps practitioner and researchers in the LLM space. By embracing these practices, developers and organizations can navigate the complexities of LLM deployment and unlock the full potential of these powerful language models. Data remains a vital resource in the era of LLMs In the world of language models, LLMs (Large Language Models) have gained significant attention and popularity. However, it’s important to remember that data is still king. No matter how powerful and sophisticated an LLM may be, without quality clean data, it won’t be able to perform at its best. In fact, the success of an LLM heavily relies on the quality and relevance of the training data it is exposed to. When training an LLM for production purposes, it’s crucial to ensure that the data used for training is clean and well-structured. This means removing any noise, inconsistencies, or biases that might exist within the dataset. It also involves carefully curating the data to ensure its relevance to the specific task at hand. By investing time and effort into data preprocessing and cleaning, you lay a solid foundation for your LLM, enabling it to provide accurate and reliable results. Emerging Architectures for LLM Applications | Andreessen Horowitz (a16z.com) Smaller LLMs are both efficient and cost-effective Contrary to popular belief, bigger doesn’t always mean better when it comes to LLMs . Smaller models can be just as effective, if not more so, when it comes to specific tasks. In fact, using smaller models tailored to a specific task can offer several advantages. First and foremost, smaller models are often more cost-effective to train and deploy . They require fewer computational resources, making them an attractive option, especially for resource-constrained projects. Moreover, smaller models tend to have a lower inference time, resulting in faster response rates, which is crucial for applications that require real-time or near-real-time processing. By utilizing smaller models, you can achieve comparable performance to larger general models while optimizing cost and efficiency. Cost of fine-tuning LLMs is going down Fine-tuning, the process of adapting a pre-trained language model to a specific task or domain, has traditionally been considered an expensive endeavor. However, recent advancements have made fine-tuning more affordable and accessible. With the availability of pre-trained models and transfer learning techniques, the cost and effort required for fine-tuning have significantly reduced. By leveraging pre-trained models as a starting point and fine-tuning them on task-specific data, you can accelerate the training process and achieve good performance with fewer resources . This approach not only saves time and money but also allows you to benefit from the general knowledge and language understanding already embedded in the pre-trained models. Evaluating LLM performance is challenging Evaluating the performance of LLMs is an ongoing challenge in the field. Despite the progress made, evaluation metrics for LLMs are still subjective to some extent . The traditional metrics used in machine learning, such as precision, recall, and F1 score, may not fully capture the intricacies of language understanding and generation. As a result, it is important to approach the evaluation process with caution and consider multiple perspectives. Human evaluation, where human annotators assess the outputs of the LLM, can provide valuable insights into the quality of the model’s responses. Additionally, it’s essential to establish specific evaluation criteria tailored to the task at hand, taking into account factors like coherence, relevance, and context-awareness. Managed services like OpenAI are expensive at scale Managed APIs, which provide access to pre-trained LLMs via an API interface such as OpenAI APIs, offer a convenient way to integrate language capabilities into your applications. However, it’s important to note that utilizing managed APIs can come at a significant cost . These services often have usage-based pricing models, meaning that the more you rely on them, the higher your expenses will be. While managed APIs can be a convenient option for rapid prototyping or small-scale projects, it’s crucial to consider the long-term costs and evaluate whether it makes financial sense to rely on them for large-scale production deployments. In some cases, building and fine-tuning your own LLM may be a more cost-effective alternative. Old school machine learning is still important Despite the emergence of powerful LLMs, “traditional” machine learning techniques still have their place in the production landscape . LLMs excel at tasks that require language generation, context understanding, and large-scale pre-training. However, for tasks that involve structured data, feature engineering, and well-defined problem spaces, traditional ML approaches can still be highly effective and efficient. In many scenarios, a combination of LLMs and traditional ML techniques can deliver optimal results. Leveraging the strengths of both approaches can lead to more robust and accurate models, especially when it comes to complex tasks that require a deep understanding of both language and data patterns. LLM memory management is critical for successful deployment Memory considerations play a crucial role in deploying and training LLMs . When it comes to serving an LLM in production, memory efficiency is vital for maintaining low latency and ensuring a smooth user experience. Optimizing memory usage during inference can help reduce response times and enable real-time or near-real-time interactions. Similarly, during the training process, memory management is essential for efficient model training. As LLMs require significant computational resources, managing memory usage becomes critical to avoid resource limitations and bottlenecks. Techniques such as gradient checkpointing and memory optimization strategies can help mitigate memory-related challenges and enable successful LLM training. Vector databases are becoming standards for developing data aware AI apps Information retrieval is a fundamental aspect of many applications that leverage LLMs. Traditionally, information retrieval has been performed using techniques like keyword matching or TF-IDF scoring. However, with the rise of LLMs, a new standard pattern is emerging — information retrieval with vector databases. Vector databases, such as FAISS , ChromaDB and Pinecone , allow for efficient and scalable similarity search in large collections of documents. By encoding documents and queries as dense vectors, you can leverage the power of LLMs for information retrieval tasks. This approach enables fast and accurate search capabilities, allowing users to find relevant information within va\u001b[0m\u001b[32;1m\u001b[1;3mHere are some best practices for deploying LLM agents to production:\n",
      "\n",
      "1. Addressing the ambiguity of natural languages: LLMs can sometimes produce inconsistent and unexpected outputs, so prompt evaluation becomes crucial to ensure that the model understands the given examples and doesn't overfit to them. Prompt versioning and optimization also play vital roles in maintaining performance and cost-effectiveness.\n",
      "\n",
      "2. Cost and latency considerations: Longer prompts increase the cost of inference, while the length of the output directly impacts latency. It's important to analyze and optimize cost and latency for LLM applications, keeping in mind that the analysis can quickly become outdated due to the rapid evolution of the field.\n",
      "\n",
      "3. Different approaches: Prompting, finetuning, and prompt tuning are different approaches that can be employed while working with LLMs. Prompting requires only a few examples and is quick and easy, while finetuning enhances model performance but demands a larger volume of data. Prompt tuning, which combines prompting and finetuning, offers a promising approach to strike a balance.\n",
      "\n",
      "4. Task composability: LLM agents can be utilized to control task flow in applications that involve executing multiple tasks sequentially, in parallel, or based on conditions. Incorporating tools or plugins can enable specific actions to be performed efficiently.\n",
      "\n",
      "5. Understanding use cases: LLMs have found promising use cases in various domains, including AI assistants, chatbots, programming and gaming, learning, talk-to-your-data applications, search and recommendation systems, sales, and SEO. Understanding the strengths and limitations of LLMs and effectively leveraging their capabilities can lead to the development of innovative and impactful applications in diverse fields.\n",
      "\n",
      "6. Data quality: The success of an LLM heavily relies on the quality and relevance of the training data. It's crucial to ensure that the data used for training is clean, well-structured, and relevant to the specific task at hand. Data preprocessing and cleaning lay a solid foundation for the LLM, enabling it to provide accurate and reliable results.\n",
      "\n",
      "7. Smaller LLMs: Smaller models tailored to specific tasks can be just as effective, if not more so, than larger models. Smaller models are often more cost-effective to train and deploy, require fewer computational resources, and have lower inference times, resulting in faster response rates.\n",
      "\n",
      "8. Cost of fine-tuning: Fine-tuning, the process of adapting a pre-trained language model to a specific task or domain, has become more affordable and accessible with the availability of pre-trained models and transfer learning techniques. Leveraging pre-trained models as a starting point and fine-tuning them on task-specific data can accelerate the training process and achieve good performance with fewer resources.\n",
      "\n",
      "9. Evaluation challenges: Evaluating the performance of LLMs is an ongoing challenge. Traditional evaluation metrics may not fully capture the intricacies of language understanding and generation. Human evaluation and specific evaluation criteria tailored to the task at hand can provide valuable insights into the quality of the model's responses.\n",
      "\n",
      "10. Managed services vs. building your own LLM: Managed APIs that provide access to pre-trained LLMs via an API interface offer convenience but can come at a significant cost. It's important to consider the long-term costs and evaluate whether it makes financial sense to rely on managed APIs for large-scale production deployments. Building and fine-tuning your own LLM may be a more cost-effective alternative.\n",
      "\n",
      "11. Combination with traditional ML techniques: Traditional machine learning techniques still have their place in the production landscape. LLMs excel at tasks that require language generation and context understanding, while traditional ML approaches can be highly effective and efficient for tasks that involve structured data, feature engineering, and well-defined problem spaces. Leveraging the strengths of both approaches can lead to more robust and accurate models.\n",
      "\n",
      "12. Memory management: Memory considerations play a crucial role in deploying and training LLMs. Optimizing memory usage during inference and training can help reduce response times, enable real-time interactions, and avoid resource limitations and bottlenecks.\n",
      "\n",
      "13. Vector databases for information retrieval: Vector databases allow for efficient and scalable similarity search in large collections of documents. By encoding documents and queries as dense vectors, LLMs can be leveraged for information retrieval tasks, enabling fast and accurate search capabilities.\n",
      "\n",
      "These best practices were suggested during a recent conference on LLM in Production by leading MLOps practitioners and researchers in the LLM space. By embracing these practices, developers and organizations can navigate the complexities of LLM deployment and unlock the full potential of these powerful language models.\n",
      "\n",
      "Sources:\n",
      "- [Medium: Best Practices for Deploying Large Language Models (LLMs) in Production](https://medium.com/@aigeek_/best-practices-for-deploying-large-language-models-llms-in-production-fdc5bf240d6a)\n",
      "- [LinkedIn: Deploying LLMs in Production: The Anatomy of LLM Applications](https://www.linkedin.com/pulse/deploying-llms-production-anatomy-llm-applications-xenonstack)\n",
      "- [Arize: Large Language Model (LLM) Deployment](https://arize.com/blog-course/large-language-model-llm-deployment/)\n",
      "- [YouTube: Deploying LLMs to Production](https://www.youtube.com/watch?v=NRjUMB9BrhQ)\n",
      "- [AWS Machine Learning Blog: Learn How to Build and Deploy Tool Using LLM Agents Using AWS SageMaker JumpStart Foundation Models](https://aws.amazon.com/blogs/machine-learning/learn-how-to-build-and-deploy-tool-using-llm-agents-using-aws-sagemaker-jumpstart-foundation-models/)\n",
      "- [Huyen Chip: LLM Engineering](https://huyenchip.com/2023/04/11/llm-engineering.html)\n",
      "- [Seldon: Orchestrating LLMs in Production](https://www.seldon.io/orchestrating-llms-in-production)\n",
      "- [Qwak: Best Practices When Deploying LLMs in Production](https://www.qwak.com/post/best-practices-when-deploying-llms-in-production)\n",
      "- [Reddit: Discussion - Biggest Roadblocks to Deploy LLMs to Production](https://www.reddit.com/r/LangChain/comments/14g7qui/discussion_biggest_roadblocks_to_deploy_llms_to/)\n",
      "- [Superwise: Considerations & Best Practices for LLM Architectures](https://superwise.ai/blog/considerations-best-practices-for-llm-architectures/)\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What are the current best practices to deploy LLM agents to production?',\n",
       " 'output': \"Here are some best practices for deploying LLM agents to production:\\n\\n1. Addressing the ambiguity of natural languages: LLMs can sometimes produce inconsistent and unexpected outputs, so prompt evaluation becomes crucial to ensure that the model understands the given examples and doesn't overfit to them. Prompt versioning and optimization also play vital roles in maintaining performance and cost-effectiveness.\\n\\n2. Cost and latency considerations: Longer prompts increase the cost of inference, while the length of the output directly impacts latency. It's important to analyze and optimize cost and latency for LLM applications, keeping in mind that the analysis can quickly become outdated due to the rapid evolution of the field.\\n\\n3. Different approaches: Prompting, finetuning, and prompt tuning are different approaches that can be employed while working with LLMs. Prompting requires only a few examples and is quick and easy, while finetuning enhances model performance but demands a larger volume of data. Prompt tuning, which combines prompting and finetuning, offers a promising approach to strike a balance.\\n\\n4. Task composability: LLM agents can be utilized to control task flow in applications that involve executing multiple tasks sequentially, in parallel, or based on conditions. Incorporating tools or plugins can enable specific actions to be performed efficiently.\\n\\n5. Understanding use cases: LLMs have found promising use cases in various domains, including AI assistants, chatbots, programming and gaming, learning, talk-to-your-data applications, search and recommendation systems, sales, and SEO. Understanding the strengths and limitations of LLMs and effectively leveraging their capabilities can lead to the development of innovative and impactful applications in diverse fields.\\n\\n6. Data quality: The success of an LLM heavily relies on the quality and relevance of the training data. It's crucial to ensure that the data used for training is clean, well-structured, and relevant to the specific task at hand. Data preprocessing and cleaning lay a solid foundation for the LLM, enabling it to provide accurate and reliable results.\\n\\n7. Smaller LLMs: Smaller models tailored to specific tasks can be just as effective, if not more so, than larger models. Smaller models are often more cost-effective to train and deploy, require fewer computational resources, and have lower inference times, resulting in faster response rates.\\n\\n8. Cost of fine-tuning: Fine-tuning, the process of adapting a pre-trained language model to a specific task or domain, has become more affordable and accessible with the availability of pre-trained models and transfer learning techniques. Leveraging pre-trained models as a starting point and fine-tuning them on task-specific data can accelerate the training process and achieve good performance with fewer resources.\\n\\n9. Evaluation challenges: Evaluating the performance of LLMs is an ongoing challenge. Traditional evaluation metrics may not fully capture the intricacies of language understanding and generation. Human evaluation and specific evaluation criteria tailored to the task at hand can provide valuable insights into the quality of the model's responses.\\n\\n10. Managed services vs. building your own LLM: Managed APIs that provide access to pre-trained LLMs via an API interface offer convenience but can come at a significant cost. It's important to consider the long-term costs and evaluate whether it makes financial sense to rely on managed APIs for large-scale production deployments. Building and fine-tuning your own LLM may be a more cost-effective alternative.\\n\\n11. Combination with traditional ML techniques: Traditional machine learning techniques still have their place in the production landscape. LLMs excel at tasks that require language generation and context understanding, while traditional ML approaches can be highly effective and efficient for tasks that involve structured data, feature engineering, and well-defined problem spaces. Leveraging the strengths of both approaches can lead to more robust and accurate models.\\n\\n12. Memory management: Memory considerations play a crucial role in deploying and training LLMs. Optimizing memory usage during inference and training can help reduce response times, enable real-time interactions, and avoid resource limitations and bottlenecks.\\n\\n13. Vector databases for information retrieval: Vector databases allow for efficient and scalable similarity search in large collections of documents. By encoding documents and queries as dense vectors, LLMs can be leveraged for information retrieval tasks, enabling fast and accurate search capabilities.\\n\\nThese best practices were suggested during a recent conference on LLM in Production by leading MLOps practitioners and researchers in the LLM space. By embracing these practices, developers and organizations can navigate the complexities of LLM deployment and unlock the full potential of these powerful language models.\\n\\nSources:\\n- [Medium: Best Practices for Deploying Large Language Models (LLMs) in Production](https://medium.com/@aigeek_/best-practices-for-deploying-large-language-models-llms-in-production-fdc5bf240d6a)\\n- [LinkedIn: Deploying LLMs in Production: The Anatomy of LLM Applications](https://www.linkedin.com/pulse/deploying-llms-production-anatomy-llm-applications-xenonstack)\\n- [Arize: Large Language Model (LLM) Deployment](https://arize.com/blog-course/large-language-model-llm-deployment/)\\n- [YouTube: Deploying LLMs to Production](https://www.youtube.com/watch?v=NRjUMB9BrhQ)\\n- [AWS Machine Learning Blog: Learn How to Build and Deploy Tool Using LLM Agents Using AWS SageMaker JumpStart Foundation Models](https://aws.amazon.com/blogs/machine-learning/learn-how-to-build-and-deploy-tool-using-llm-agents-using-aws-sagemaker-jumpstart-foundation-models/)\\n- [Huyen Chip: LLM Engineering](https://huyenchip.com/2023/04/11/llm-engineering.html)\\n- [Seldon: Orchestrating LLMs in Production](https://www.seldon.io/orchestrating-llms-in-production)\\n- [Qwak: Best Practices When Deploying LLMs in Production](https://www.qwak.com/post/best-practices-when-deploying-llms-in-production)\\n- [Reddit: Discussion - Biggest Roadblocks to Deploy LLMs to Production](https://www.reddit.com/r/LangChain/comments/14g7qui/discussion_biggest_roadblocks_to_deploy_llms_to/)\\n- [Superwise: Considerations & Best Practices for LLM Architectures](https://superwise.ai/blog/considerations-best-practices-for-llm-architectures/)\"}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\": \"What are the current best practices to deploy LLM agents to production?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We encounter a few issues when we try to just add the scrape function to the agent without any other consideration. \n",
    "How about we make this better by improving the inner chain of the agent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThe word \"educa\" has 5 letters.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `web_search` with `{'query': 'define educa'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m['https://www.merriam-webster.com/dictionary/education', 'https://en.wikipedia.org/wiki/Education', 'https://www.dictionary.com/browse/education', 'https://www.britannica.com/topic/education', 'https://dictionary.cambridge.org/us/dictionary/english/education', 'https://www.oxfordlearnersdictionaries.com/us/definition/english/education', 'https://learn.org/articles/What_is_Education.html', 'https://en.wikipedia.org/wiki/Definitions_of_education', 'https://link.springer.com/chapter/10.1007/978-3-030-83925-3_3', 'https://www.collinsdictionary.com/us/dictionary/english/education']\u001b[0m\u001b[32;1m\u001b[1;3mYes, \"educa\" is not a recognized word in the English language. It seems to be a misspelling or a truncated form of the word \"education\".\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'is that a real word?',\n",
       " 'chat_history': [HumanMessage(content='how many letters in the word educa?'),\n",
       "  AIMessage(content='The word \"educa\" has 5 letters.')],\n",
       " 'output': 'Yes, \"educa\" is not a recognized word in the English language. It seems to be a misspelling or a truncated form of the word \"education\".'}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import MessagesPlaceholder\n",
    "\n",
    "MEMORY_KEY = \"chat_history\"\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are very powerful assistant, but bad at calculating lengths of words.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=MEMORY_KEY),\n",
    "        (\"user\", \"{input}\"),\n",
    "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "chat_history = []\n",
    "\n",
    "agent = (\n",
    "    {\n",
    "        \"input\": lambda x: x[\"input\"],\n",
    "        \"agent_scratchpad\": lambda x: format_to_openai_tool_messages(\n",
    "            x[\"intermediate_steps\"]\n",
    "        ),\n",
    "        \"chat_history\": lambda x: x[\"chat_history\"],\n",
    "    }\n",
    "    | prompt\n",
    "    | llm_with_tools\n",
    "    | OpenAIToolsAgentOutputParser()\n",
    ")\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "input1 = \"how many letters in the word educa?\"\n",
    "result = agent_executor.invoke({\"input\": input1, \"chat_history\": chat_history})\n",
    "chat_history.extend(\n",
    "    [\n",
    "        HumanMessage(content=input1),\n",
    "        AIMessage(content=result[\"output\"]),\n",
    "    ]\n",
    ")\n",
    "agent_executor.invoke({\"input\": \"is that a real word?\", \"chat_history\": chat_history})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oreilly-langchain",
   "language": "python",
   "name": "oreilly-langchain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
