{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a research Agent with LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install openai\n",
    "# !pip install \"langchain[all]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Or if you are in Colab, uncoment below and add your api key\n",
    "import os\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"your-api-key\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chat = ChatOpenAI(temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.quora.com/Why-do-pancakes-from-restaurants-taste-so-much-better-than-the-pancakes-I-make-at-home',\n",
       " 'https://www.prnewswire.com/news-releases/packaged-facts-still-room-for-pancakes-and-waffles-at-americas-breakfast-table-300317750.html',\n",
       " 'https://www.fitday.com/fitness-articles/nutrition/healthy-eating/healthy-breakfast-pancakes-vs-waffles.html',\n",
       " 'https://www.reddit.com/r/Cooking/comments/wr3ngc/how_do_i_have_pancakes_for_breakfast_without/',\n",
       " 'https://www.smithsonianmag.com/history/a-brief-history-of-pancakes-180981667/',\n",
       " 'https://www.kingarthurbaking.com/blog/2021/10/26/best-pancake-tips',\n",
       " 'https://www.snackandbakery.com/articles/94837-state-of-the-industry-2020-breakfast-products-combine-nutrition-and-good-taste',\n",
       " 'https://www.turnipseedtravel.com/blog/the-best-pancakes-around-the-world',\n",
       " 'https://shscourier.com/45785/commentary/which-breakfast-food-reigns-superior-pancakes-or-waffles/']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.utilities import SerpAPIWrapper\n",
    "from langchain.agents import tool\n",
    "\n",
    "search = SerpAPIWrapper()\n",
    "\n",
    "@tool\n",
    "def web_search(query: str):\n",
    "    \"\"\"Searches the web given a user query and returns the most relevant links\"\"\"\n",
    "    output = search.results(query)\n",
    "    results = output[\"organic_results\"]\n",
    "    return [r[\"link\"] for r in results]\n",
    "\n",
    "web_search(\"Current resources on why pancakes are the best breakfast\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['agent_scratchpad', 'input'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]], 'agent_scratchpad': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a helpful assistant')), MessagesPlaceholder(variable_name='chat_history', optional=True), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}')), MessagesPlaceholder(variable_name='agent_scratchpad')])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import hub\n",
    "\n",
    "prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['agent_scratchpad', 'input']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.input_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]],\n",
       " 'agent_scratchpad': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.input_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': None,\n",
       " 'input_variables': ['agent_scratchpad', 'input'],\n",
       " 'input_types': {'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]],\n",
       "  'agent_scratchpad': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]},\n",
       " 'output_parser': None,\n",
       " 'partial_variables': {},\n",
       " 'metadata': None,\n",
       " 'tags': None,\n",
       " 'messages': [SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a helpful assistant')),\n",
       "  MessagesPlaceholder(variable_name='chat_history', optional=True),\n",
       "  HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}')),\n",
       "  MessagesPlaceholder(variable_name='agent_scratchpad')],\n",
       " 'validate_template': False}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build an agent I need to do 2 essential things:\n",
    "\n",
    "1. Connect a model to tools\n",
    "2. Set up a loop in which the model continuously tries to solve the problem by either just calling the llm or calling a tool, observing the result and repeating until finished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['agent_scratchpad', 'input'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]], 'agent_scratchpad': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a helpful assistant')), MessagesPlaceholder(variable_name='chat_history', optional=True), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}')), MessagesPlaceholder(variable_name='agent_scratchpad')])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_openai_tools_agent\n",
    "\n",
    "tools = [web_search]\n",
    "\n",
    "agent = create_openai_tools_agent(llm_chat, tools, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor\n",
    "\n",
    "\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `web_search` with `{'query': 'resources to learn new languages'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m['https://nymag.com/strategist/article/how-to-learn-languages-at-home.html', 'https://www.theintrepidguide.com/top-language-learning-resources/', 'https://www.lingualift.com/blog/best-language-learning-resources/', 'https://www.reddit.com/r/languagelearning/comments/uhiuab/what_are_apps_or_websites_where_you_can_learn/', 'https://www.quora.com/What-resources-do-you-use-to-learn-languages', 'https://libguides.csudh.edu/modernlanguages/freeresources', 'https://www.alllanguageresources.com/', 'https://www.fluentu.com/blog/free-language-learning-websites/']\u001b[0m\u001b[32;1m\u001b[1;3mI found some resources that can help you learn new languages:\n",
      "\n",
      "1. [How to Learn Languages at Home - NYMag](https://nymag.com/strategist/article/how-to-learn-languages-at-home.html)\n",
      "2. [Top Language Learning Resources - The Intrepid Guide](https://www.theintrepidguide.com/top-language-learning-resources/)\n",
      "3. [Best Language Learning Resources - Lingualift](https://www.lingualift.com/blog/best-language-learning-resources/)\n",
      "4. [Apps or Websites to Learn Languages - Reddit](https://www.reddit.com/r/languagelearning/comments/uhiuab/what_are_apps_or_websites_where_you_can_learn/)\n",
      "5. [Resources for Learning Languages - Quora](https://www.quora.com/What-resources-do-you-use-to-learn-languages)\n",
      "6. [Free Language Learning Resources - CSUDH LibGuides](https://libguides.csudh.edu/modernlanguages/freeresources)\n",
      "7. [All Language Resources - All Language Resources](https://www.alllanguageresources.com/)\n",
      "8. [Free Language Learning Websites - FluentU](https://www.fluentu.com/blog/free-language-learning-websites/)\n",
      "\n",
      "You can explore these resources to find the best fit for your language learning journey.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What are good resources to learn about learning new languages?',\n",
       " 'agent_scratchpad': [],\n",
       " 'output': 'I found some resources that can help you learn new languages:\\n\\n1. [How to Learn Languages at Home - NYMag](https://nymag.com/strategist/article/how-to-learn-languages-at-home.html)\\n2. [Top Language Learning Resources - The Intrepid Guide](https://www.theintrepidguide.com/top-language-learning-resources/)\\n3. [Best Language Learning Resources - Lingualift](https://www.lingualift.com/blog/best-language-learning-resources/)\\n4. [Apps or Websites to Learn Languages - Reddit](https://www.reddit.com/r/languagelearning/comments/uhiuab/what_are_apps_or_websites_where_you_can_learn/)\\n5. [Resources for Learning Languages - Quora](https://www.quora.com/What-resources-do-you-use-to-learn-languages)\\n6. [Free Language Learning Resources - CSUDH LibGuides](https://libguides.csudh.edu/modernlanguages/freeresources)\\n7. [All Language Resources - All Language Resources](https://www.alllanguageresources.com/)\\n8. [Free Language Learning Websites - FluentU](https://www.fluentu.com/blog/free-language-learning-websites/)\\n\\nYou can explore these resources to find the best fit for your language learning journey.'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\": \"What are good resources to learn about learning new languages?\",\n",
    "                             \"agent_scratchpad\": []})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mIt's important to gather information from reliable sources to learn about good programming practices.\n",
      "Action: web_search\n",
      "Action Input: \"best way to learn good programming practices\"\u001b[0m\u001b[36;1m\u001b[1;3m['https://www.reddit.com/r/learnprogramming/comments/6bv3ux/how_do_you_practice_programming/', 'https://www.quora.com/How-can-I-learn-the-best-practices-in-programming', 'https://www.thinkful.com/blog/coding-best-practices/', 'https://www.educative.io/blog/coding-best-practices', 'https://www.datacamp.com/tutorial/coding-best-practices-and-guidelines', 'https://flatironschool.com/blog/best-websites-to-practice-coding-for-beginners/', 'https://codesigningstore.com/ultimate-programming-best-practices-guide', 'https://www.browserstack.com/guide/coding-standards-best-practices', 'https://medium.com/@josueparra2892/20-best-programming-practices-407df688b96e']\u001b[0m\u001b[32;1m\u001b[1;3mThere are many resources available to learn good programming practices.\n",
      "Action: web_search\n",
      "Action Input: \"coding best practices\"\u001b[0m\u001b[36;1m\u001b[1;3m['https://www.thinkful.com/blog/coding-best-practices/', 'https://www.datacamp.com/tutorial/coding-best-practices-and-guidelines', 'https://www.browserstack.com/guide/coding-standards-best-practices', 'https://www.reddit.com/r/java/comments/2z58la/what_are_good_coding_practices_that_you_wish/', 'https://en.wikipedia.org/wiki/Coding_best_practices', 'https://www.educative.io/blog/coding-best-practices', 'https://softwareengineering.stackexchange.com/questions/196706/creating-a-coding-standards-document', 'https://www.appacademy.io/blog/python-coding-best-practices', 'https://mitcommlab.mit.edu/broad/commkit/best-practices-for-coding-organization-and-documentation/']\u001b[0m\u001b[32;1m\u001b[1;3mThere are specific websites and articles dedicated to coding best practices.\n",
      "Action: web_search\n",
      "Action Input: \"coding best practices article\"\u001b[0m\u001b[36;1m\u001b[1;3m['https://www.thinkful.com/blog/coding-best-practices/', 'https://www.datacamp.com/tutorial/coding-best-practices-and-guidelines', 'https://www.browserstack.com/guide/coding-standards-best-practices', 'https://en.wikipedia.org/wiki/Coding_best_practices', 'https://www.educative.io/blog/coding-best-practices', 'https://mitcommlab.mit.edu/broad/commkit/best-practices-for-coding-organization-and-documentation/', 'https://www.qulix.com/about/blog/good-coding-practices/', 'https://codesigningstore.com/ultimate-programming-best-practices-guide', 'https://medium.com/codex/15-best-coding-practices-to-follow-421501bbf24d']\u001b[0m\u001b[32;1m\u001b[1;3mThere are various articles and guides available online that provide information on coding best practices.\n",
      "Final Answer: The best way to learn good programming practices is to read articles and guides dedicated to coding best practices.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What is the best way to learn good programming practices?',\n",
       " 'agent_scratchpad': [],\n",
       " 'output': 'The best way to learn good programming practices is to read articles and guides dedicated to coding best practices.'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.agents import AgentExecutor, create_react_agent\n",
    "\n",
    "prompt = hub.pull(\"hwchase17/react\")\n",
    "\n",
    "agent = create_react_agent(llm_chat, tools, prompt)\n",
    "\n",
    "react_agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "\n",
    "react_agent_executor.invoke({\"input\": \"What is the best way to learn good programming practices?\",\n",
    "                             \"agent_scratchpad\": []})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's build this same agent but using a more in depth approach going one level down in the abstraction layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's go down the latter of abstraction and build this same agent but leveraging the lower-level capabilities of LangChain's LCEL language. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chat = ChatOpenAI(model=\"gpt-3.5-turbo-16k\",temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import tool\n",
    "from langchain.utilities import SerpAPIWrapper\n",
    "\n",
    "search = SerpAPIWrapper()\n",
    "\n",
    "@tool\n",
    "def web_search(query: str):\n",
    "    \"\"\"Searches the web given a user query and returns the most relevant links\"\"\"\n",
    "    output = search.results(query)\n",
    "    results = output[\"organic_results\"]\n",
    "    return [r[\"link\"] for r in results]\n",
    "\n",
    "\n",
    "tools = [web_search]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\", \"You are a powerful research assistant.\\\n",
    "            Given a user query you will perform research on the \\\n",
    "                topic using the tools available.\"\n",
    "        ),\n",
    "        (\"user\", \"{input}\"),\n",
    "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bind tools to the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.convert_to_openai import format_tool_to_openai_tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_with_tools = llm_chat.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, all we have to do is import two last utility functions:\n",
    "- a component for formatting intermediate steps (agent action, tool output pairs) to input messages that can be sent to the model, \n",
    "- and a component for converting the output message into an agent action/agent finish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.format_scratchpad.openai_tools import format_to_openai_tool_messages\n",
    "from langchain.agents.output_parsers.openai_tools import OpenAIToolsAgentOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = (\n",
    "    {\n",
    "        \"input\": lambda x: x[\"input\"], \n",
    "        \"agent_scratchpad\": lambda x: format_to_openai_tool_messages(\n",
    "            x[\"intermediate_steps\"]\n",
    "        ),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm_with_tools\n",
    "    | OpenAIToolsAgentOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor\n",
    "\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `web_search` with `{'query': 'best open source LLM'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m['https://www.datacamp.com/blog/top-open-source-llms', 'https://github.com/Hannibal046/Awesome-LLM', 'https://www.reddit.com/r/LocalLLaMA/comments/17jmpn8/what_are_top_open_source_projects_in_llm_space/', 'https://www.width.ai/post/the-best-open-source-llms', 'https://blog.n8n.io/open-source-llm/', 'https://klu.ai/blog/open-source-llm-models', 'https://github.com/eugeneyan/open-llms', 'https://www.techopedia.com/6-best-open-source-llms-to-watch-out-for-in-2024', 'https://www.youtube.com/watch?v=L6nnNTVX-VI']\u001b[0m\u001b[32;1m\u001b[1;3mThere are several open source LLM (Language Model) projects available. Here are some resources that can help you find the best open source LLM:\n",
      "\n",
      "1. [DataCamp Blog](https://www.datacamp.com/blog/top-open-source-llms): This blog post provides a list of top open source LLM projects.\n",
      "\n",
      "2. [Awesome-LLM GitHub Repository](https://github.com/Hannibal046/Awesome-LLM): This GitHub repository contains a curated list of open source LLM projects.\n",
      "\n",
      "3. [Reddit Thread](https://www.reddit.com/r/LocalLLaMA/comments/17jmpn8/what_are_top_open_source_projects_in_llm_space/): This Reddit thread discusses the top open source projects in the LLM space.\n",
      "\n",
      "4. [Width.ai Blog](https://www.width.ai/post/the-best-open-source-llms): This blog post highlights the best open source LLM projects.\n",
      "\n",
      "5. [n8n.io Blog](https://blog.n8n.io/open-source-llm/): This blog post discusses open source LLM models and their applications.\n",
      "\n",
      "6. [Klu.ai Blog](https://klu.ai/blog/open-source-llm-models): This blog post explores open source LLM models and their features.\n",
      "\n",
      "7. [Open-LLMs GitHub Repository](https://github.com/eugeneyan/open-llms): This GitHub repository contains open source LLM models.\n",
      "\n",
      "8. [Techopedia Article](https://www.techopedia.com/6-best-open-source-llms-to-watch-out-for-in-2024): This article lists the 6 best open source LLMs to watch out for in 2024.\n",
      "\n",
      "Please note that the \"best\" open source LLM may vary depending on your specific requirements and use case. It is recommended to explore these resources and evaluate the projects based on your needs.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'actions': [OpenAIToolAgentAction(tool='web_search', tool_input={'query': 'best open source LLM'}, log=\"\\nInvoking: `web_search` with `{'query': 'best open source LLM'}`\\n\\n\\n\", message_log=[AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_HiVnPSa0aULBRoUBQ8nBiVss', 'function': {'arguments': '{\\n  \"query\": \"best open source LLM\"\\n}', 'name': 'web_search'}, 'type': 'function'}]})], tool_call_id='call_HiVnPSa0aULBRoUBQ8nBiVss')],\n",
       "  'messages': [AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_HiVnPSa0aULBRoUBQ8nBiVss', 'function': {'arguments': '{\\n  \"query\": \"best open source LLM\"\\n}', 'name': 'web_search'}, 'type': 'function'}]})]},\n",
       " {'steps': [AgentStep(action=OpenAIToolAgentAction(tool='web_search', tool_input={'query': 'best open source LLM'}, log=\"\\nInvoking: `web_search` with `{'query': 'best open source LLM'}`\\n\\n\\n\", message_log=[AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_HiVnPSa0aULBRoUBQ8nBiVss', 'function': {'arguments': '{\\n  \"query\": \"best open source LLM\"\\n}', 'name': 'web_search'}, 'type': 'function'}]})], tool_call_id='call_HiVnPSa0aULBRoUBQ8nBiVss'), observation=['https://www.datacamp.com/blog/top-open-source-llms', 'https://github.com/Hannibal046/Awesome-LLM', 'https://www.reddit.com/r/LocalLLaMA/comments/17jmpn8/what_are_top_open_source_projects_in_llm_space/', 'https://www.width.ai/post/the-best-open-source-llms', 'https://blog.n8n.io/open-source-llm/', 'https://klu.ai/blog/open-source-llm-models', 'https://github.com/eugeneyan/open-llms', 'https://www.techopedia.com/6-best-open-source-llms-to-watch-out-for-in-2024', 'https://www.youtube.com/watch?v=L6nnNTVX-VI'])],\n",
       "  'messages': [FunctionMessage(content='[\"https://www.datacamp.com/blog/top-open-source-llms\", \"https://github.com/Hannibal046/Awesome-LLM\", \"https://www.reddit.com/r/LocalLLaMA/comments/17jmpn8/what_are_top_open_source_projects_in_llm_space/\", \"https://www.width.ai/post/the-best-open-source-llms\", \"https://blog.n8n.io/open-source-llm/\", \"https://klu.ai/blog/open-source-llm-models\", \"https://github.com/eugeneyan/open-llms\", \"https://www.techopedia.com/6-best-open-source-llms-to-watch-out-for-in-2024\", \"https://www.youtube.com/watch?v=L6nnNTVX-VI\"]', name='web_search')]},\n",
       " {'output': 'There are several open source LLM (Language Model) projects available. Here are some resources that can help you find the best open source LLM:\\n\\n1. [DataCamp Blog](https://www.datacamp.com/blog/top-open-source-llms): This blog post provides a list of top open source LLM projects.\\n\\n2. [Awesome-LLM GitHub Repository](https://github.com/Hannibal046/Awesome-LLM): This GitHub repository contains a curated list of open source LLM projects.\\n\\n3. [Reddit Thread](https://www.reddit.com/r/LocalLLaMA/comments/17jmpn8/what_are_top_open_source_projects_in_llm_space/): This Reddit thread discusses the top open source projects in the LLM space.\\n\\n4. [Width.ai Blog](https://www.width.ai/post/the-best-open-source-llms): This blog post highlights the best open source LLM projects.\\n\\n5. [n8n.io Blog](https://blog.n8n.io/open-source-llm/): This blog post discusses open source LLM models and their applications.\\n\\n6. [Klu.ai Blog](https://klu.ai/blog/open-source-llm-models): This blog post explores open source LLM models and their features.\\n\\n7. [Open-LLMs GitHub Repository](https://github.com/eugeneyan/open-llms): This GitHub repository contains open source LLM models.\\n\\n8. [Techopedia Article](https://www.techopedia.com/6-best-open-source-llms-to-watch-out-for-in-2024): This article lists the 6 best open source LLMs to watch out for in 2024.\\n\\nPlease note that the \"best\" open source LLM may vary depending on your specific requirements and use case. It is recommended to explore these resources and evaluate the projects based on your needs.',\n",
       "  'messages': [AIMessage(content='There are several open source LLM (Language Model) projects available. Here are some resources that can help you find the best open source LLM:\\n\\n1. [DataCamp Blog](https://www.datacamp.com/blog/top-open-source-llms): This blog post provides a list of top open source LLM projects.\\n\\n2. [Awesome-LLM GitHub Repository](https://github.com/Hannibal046/Awesome-LLM): This GitHub repository contains a curated list of open source LLM projects.\\n\\n3. [Reddit Thread](https://www.reddit.com/r/LocalLLaMA/comments/17jmpn8/what_are_top_open_source_projects_in_llm_space/): This Reddit thread discusses the top open source projects in the LLM space.\\n\\n4. [Width.ai Blog](https://www.width.ai/post/the-best-open-source-llms): This blog post highlights the best open source LLM projects.\\n\\n5. [n8n.io Blog](https://blog.n8n.io/open-source-llm/): This blog post discusses open source LLM models and their applications.\\n\\n6. [Klu.ai Blog](https://klu.ai/blog/open-source-llm-models): This blog post explores open source LLM models and their features.\\n\\n7. [Open-LLMs GitHub Repository](https://github.com/eugeneyan/open-llms): This GitHub repository contains open source LLM models.\\n\\n8. [Techopedia Article](https://www.techopedia.com/6-best-open-source-llms-to-watch-out-for-in-2024): This article lists the 6 best open source LLMs to watch out for in 2024.\\n\\nPlease note that the \"best\" open source LLM may vary depending on your specific requirements and use case. It is recommended to explore these resources and evaluate the projects based on your needs.')]}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(agent_executor.stream({\"input\": \"What is the current best open source LLM available?\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, what if we want the agent to actually read the links and return a research report rather than just the urls?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "@tool\n",
    "def scrape_text(url: str):\n",
    "    \"\"\"Scrapes the text from a given URL and returns it.\"\"\"\n",
    "    # Send a GET request to the webpage\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "\n",
    "        # Check if the request was successful\n",
    "        if response.status_code == 200:\n",
    "            # Parse the content of the request with BeautifulSoup\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "            # Extract all text from the webpage\n",
    "            page_text = soup.get_text(separator=\" \", strip=True)\n",
    "\n",
    "            # Print the extracted text\n",
    "            return page_text[:10000]\n",
    "        else:\n",
    "            return f\"Failed to retrieve the webpage: Status code {response.status_code}\"\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return f\"Failed to retrieve the webpage: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [web_search, scrape_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_with_tools = llm_chat.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\", \"You are a powerful research assistant. Given a user query you will perform research on the topic using the tools available.\"\n",
    "        ),\n",
    "        (\"user\", \"Scrape and summarize the contents from these urls: {input}\"),\n",
    "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = (\n",
    "    {\n",
    "        \"input\": lambda x: x[\"input\"], \n",
    "        \"agent_scratchpad\": lambda x: format_to_openai_tool_messages(\n",
    "            x[\"intermediate_steps\"]\n",
    "        ),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm_with_tools\n",
    "    | OpenAIToolsAgentOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor\n",
    "\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `scrape_text` with `{'url': 'https://towardsdatascience.com/deploying-llm-agents-to-production-best-practices-7e4e8e8e8e8e'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mFailed to retrieve the webpage: Status code 403\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `scrape_text` with `{'url': 'https://towardsdatascience.com/deploying-llm-agents-to-production-best-practices-7e4e8e8e8e8e'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mFailed to retrieve the webpage: Status code 403\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `scrape_text` with `{'url': 'https://towardsdatascience.com/deploying-llm-agents-to-production-best-practices-7e4e8e8e8e8e'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mFailed to retrieve the webpage: Status code 403\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `web_search` with `{'query': 'best practices to deploy LLM agents to production'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m['https://www.qwak.com/post/best-practices-when-deploying-llms-in-production', 'https://www.linkedin.com/pulse/deploying-llms-production-anatomy-llm-applications-xenonstack', 'https://www.reddit.com/r/MachineLearning/comments/16fklqj/discussion_how_to_deploy_llm_in_production/', 'https://arize.com/blog-course/large-language-model-llm-deployment/', 'https://www.youtube.com/watch?v=NRjUMB9BrhQ', 'https://huyenchip.com/2023/04/11/llm-engineering.html', 'https://aws.amazon.com/blogs/machine-learning/learn-how-to-build-and-deploy-tool-using-llm-agents-using-aws-sagemaker-jumpstart-foundation-models/', 'https://www.seldon.io/deploying-large-language-models-in-production']\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `scrape_text` with `{'url': 'https://www.qwak.com/post/best-practices-when-deploying-llms-in-production'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mBest Practices for Deploying LLMs in Production | Qwak ð£ MONTHLY LIVE DEMO: See Qwak in ActionÂ Â |Â Â Apr. 3, 2024, 11am EST -> x Platform Our Products Model Registry Model Training Model Serving Model Monitoring Feature Store Vector Store Feature + Vector Pipeline Managed Notebooks Solutions Use Cases LLMOps Recommender Systems Search & Ranking Fraud Detection Resources Blog Events & Webinars Compare Qwak Customer Stories Model Examples Integrations Virtual Conference Join Infer Spring â24: The engineering behind AI and ML. Documentation Pricing Company About us Partners Careers Contact us Follow us Log in Book Demo Start Free Book a Demo Start for Free Back to Blog LLMops Best Practices for Deploying LLMs in Production Large Language Models (LLMs) have taken the world by storm. Thanks to powerful AI models such as OpenAI's GPT-4 and the popularity of products such as ChatGPT, companies are now racing to integrate LLMs into their products. Guy Eshet Product Manager at Qwak April 30, 2023 Table of contents Introduction Large Language Models (LLMs) have taken the world by storm. Thanks to powerful AI models such as OpenAI's GPT-4 and the popularity of products such as ChatGPT, companies are now racing to integrate LLMs into their products. Launching exciting prototypes with generative models may seem easy, however deploying LLMs in production can be a complex process that requires robust infrastructure and proper tools.Â In this article, we will explore the challenges and considerations for deploying LLMs in production, and learn how Qwak is helping customers in various industries to streamline their MLOps workflows and deploy LLMs in production. What are LLMs? LLMs are large machine learning models that use deep learning to generate, summarize and understand natural language, text and code. LLMs are trained on vast amounts of data, and demand immense compute power to train and maintain.Â In recent years, there has been a significant advancement in the inference abilities of LLMs. These models are now capable of performing complex tasks such as creative writing, coding, information retrieval, summarization, and many more. LLMs have become valuable tools for a wide range of applications, including chatbots, voice assistants, search and automated content generation, among others. Closed-source and open-source LLMs LLMs can be broadly classified into two categories: closed-source and open-source models.Â Closed-source models, such as GPT-4 by OpenAI, LaMDA by Google, and Jurassic by AI21, are proprietary models that are only available through APIs. These models offer limited customization options and control over your data, as the source code and model weights are not available to the public. However, closed-source models are often more powerful than their open-source counterparts, as they are developed by industry leaders with access to vast resources and expertise. On the other hand, open-source models, such as BLOOM, Alpaca, LLaMA, OpenAssistant, and BERT, are rapidly evolving and catching up. They offer more flexibility and can be deployed on your own infrastructure while being customized to your specific needs. Make sure to check the model licenses before using them. Sme models are available for research purposes only and cannot be used for commercial applications.Â Customizing large language models Prompt engineering, fine-tuning, and creating chains are three common techniques used to generate output from LLMs. Prompt Engineering Prompt engineering involves the art of providing a prompt to the model. This can include a question, sentence, or topic for the model to focus on. The purpose is to guide the model output towards the desired direction. Prompt engineering can help to control the output and ensure that it's relevant and useful. Model Chains Creating chains involves using the output of one model as the input for another model, creating a chain of models that work together to generate the final output. This technique is often used to improve the quality and relevance of the output. As an example, LangChain is one of the most common LLM frameworks supporting these use cases. Additionally, AI-Agent frameworks such as Auto-GPT use similar mechanisms to use LLMs to complete complex tasks. Model Fine Tuning Fine-tuning involves training an LLM on a specific dataset or task to improve its performance on that particular task. Fine-tuning can help to optimize the model's output for a specific use case or application. For instance, a fine-tuned LLM could be designed to process financial data or expertly handle customer requests. Fine-tuning models requires significant infrastructure and can be done on managed models like GPT-4, although the inference cost is generally higher. Fine Tuning on Qwak To make the process easier, Qwak enables our customers to fine-tune, tag, and track their fine-tuned models more effectively. This helps to ensure better control over the various model versions between different fine-tuning processes. Additionally, Qwak makes it easier to connect your existing data pipelines with the model fine-tuning process, allowing data science teams to operate faster.Â Integrating LLMs into your products Incorporating LLMs into your products can be challenging, as it involves considerations such as privacy, security, and legal issues. These concerns vary widely when choosing to deploy in-house LLMs or when using external models. In the near future, it's possible that companies will use external GPT-4-like models for early prototyping and high-stakes applications, while opting for smaller, more fine-tuned open-source, self hosted LLMs for other use cases. This approach will likely lead to a mix of both large and small models in the production pipeline. About Qwak Qwak is a fully managed, accessible, and reliable ML Platform. It allows builders to transform and store data, build, train, and deploy models, and monitor the entire Machine Learning pipeline. Pay-as-you-go pricing makes it easy to scale when needed. Infer Virtual Conference by Qwak March 20th, 11AM EST -> Say goodbye to complex MLOps Chat with us to see the platform live and discover how we can help simplify your ML journey. Book a Meeting Related posts LLMops Integrating Vector Databases with LLMs: A Hands-On Guide Ran Romano February 20, 2024 LLMops RAG and LLM: A New Frontier in Dynamic Language Modeling Pavel Klushin December 28, 2023 LLMops The Costs of Leveraging the Power of GPT4 Lior Penso March 19, 2023 Get started with Qwak today. See how the best ML engineers and data scientists deploy models in production. Say goodbye to complex MLOps and start delivering today. Try Qwak free Book a Demo Build high-quality ML simply. Â© 2024 Qwak. All rights reserved. Terms & Conditions Privacy Policy Products Model Registry Model Training Model Serving Model Monitoring Feature Store Vector Store Feature + Vector Pipeline Managed Notebooks Company Blog Pricing Case Studies Partners Careers Compare Qwak Qwak vs. SageMaker Qwak vs. Vertex AI Qwak vs. Databricks Follow us Sign up for Qwak updates\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `scrape_text` with `{'url': 'https://www.linkedin.com/pulse/deploying-llms-production-anatomy-llm-applications-xenonstack'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mDeploying LLMS in Production: The Anatomy of LLM Applications LinkedIn and 3rd parties use essential and non-essential cookies to provide, secure, analyze and improve our Services, and to show you relevant ads (including professional and job ads ) on and off LinkedIn. Learn more in our Cookie Policy . Select Accept to consent or Reject to decline non-essential cookies for this use. You can update your choices at any time in your settings . Accept Reject Agree & Join LinkedIn By clicking Continue, you agree to LinkedIn’s User Agreement , Privacy Policy , and Cookie Policy . Skip to main content LinkedIn Articles People Learning Jobs Join now Sign in Anatomy of LLM Applications Deploying LLMs in Production: The Anatomy of LLM Applications Report this article XenonStack XenonStack Platform Engineering for Autonomous Operations #autonomousops Published Oct 10, 2023 + Follow Introduction Large Language Models (LLMs) are deep learning algorithms that can be used for a wide range of NLP (Natural Language Processing). Large language models use transformer models and are trained using massive datasets — hence, large. This enables them to recognize, translate, predict, or generate text or other content. Large language models are also referred to as neural networks (NNs), which are computing systems inspired by the human brain. These neural networks work using a network of nodes that are layered, much like neurons. Subscribe the Newsletter LLM applications and use cases LLMs have opened a new class of enterprise use cases. Being trained on such a large amount of information, these models provide a strong foundation to build off. This makes them incredibly versatile and easily adaptable to suit a variety of applications. Whether leveraging the built-in knowledge of the model or fine-tuning to your use case, there are many ways to provide a useful and intuitive user experience. Some LLM use applications are as follows: - Chatbots It provides customer support, product recommendations and conversational search. Document Understanding Summarize, exact key information and analyse sentiments. Code completion Explain and document code, suggest improvement and auto generate code. Content generation Write a blog post, emails, ad copies and outline based on prompts. Search Answer natural language questions on top of a large collection of texts. Translation Translate between languages and perform style transfer to change the tone and reading level. LLM components and reference architecture LLM Models The LLM model is the heart of the LLM application and selecting a model depends on a variety of factors. The primary elements are the size of the model (# of parameters) and whether it is open sourced or behind an API. Some models may have tokenizers and embedding models built in, while others will require you to run these steps yourself. Tokenizers split input text into smaller chunks while embedding models convert the text into numeric vectors which an LLM can understand. The easiest place to begin prototyping is with OpenAI’s GPT-4, as it is quick to get up and running given how adaptable the model is. In some cases, a long context window may be required if performing a lot of in-context learning, making Claude 2 an excellent choice. However, when using such large models, both the cost and latency could become prohibitive. Prompt Template Much like the experimentation process that has evolved in traditional Machine Learning model training, prompt engineering has emerged as a core activity in LLM application development. There are several prompting techniques that lead to better results. The first technique is to write clear instructions and specify the steps required to complete the task. Most tasks can be done using a zero-shot prompt but adding some examples with a few-shot prompt can further improve and tailor the response. Even simply including phrases such as show your work or let’s think step by step leads the model to iteratively develop a solution iteratively, increasing the chances of a correct answer. This chain-of-thought prompting is even more powerful if a few examples of reasoning logic are provided. Prompts can then be templated and reused by an LLM application. They can be integrated into your code with a simple f-string or str. format(), but there are also libraries like LangChain, Guidance, and LMQL that offer more control. For chat completion APIs, there is generally a system prompt to assign the task, followed by alternating user and assistant messages. You can seed the chat with examples of user questions and assistant answers before running it. Experimenting and iterating on these prompt templates in a structured way will lead to improved model performance. The outputs of the model should be evaluated by either a scoring function or human feedback. Vector Database Many use cases will require access to information that the LLM has not been trained on. This may be a private knowledge base (e.g., a company wiki), recent information (e.g., events this weekend), or a specific document (e.g., a research paper). There might be other cases where the context window of the model is too small to include everything in the prompt (e.g., an entire book). Adding an external database can provide access to supplementary information in a technique known as retrieval augmented generation (RAG). Vector databases have been around for some time but have surged in popularity recently as they make their way into LLM applications. Relevant external information is first chunked into blocks (e.g., sentences or paragraphs), tokenized, and run through an embedding model. These are then fed into a vector database, such as Pinecone, Chrome, Qdrant, or pgvector (open source). When a prompt is made, it is also vectorized and a similarity search is performed to retrieve relevant entries from the database. These entries are then fed in alongside the original prompt, providing the context needed to provide a coherent response. The LLM may cite which chunks it used in its response, which adds a degree of trust to counteract potential hallucinations. Agents and Tools Using LLMs alone can enable powerful applications, but there are inherent limitations. For example, they cannot continually prompt themselves, make external API calls, nor retrieve a web page. An LLM agent has access to tools that can perform actions beyond text generation. For example, an LLM agent could perform a web search, execute code, perform a database lookup, or do maths calculations. OpenAI language models can decide which tools to use and return a JSON object with the arguments to call the function. This enables a whole new suite of use cases, such as booking a flight, generating an image, or sending an email. Some frameworks enable agents to execute iteratively to complete a task. This could be as simple as breaking down a task into several subtasks or “self-asking” to continue gathering relevant information for the task. These can then be executed in a chain to work towards a correct answer iteratively. The idea of combining this reasoning alongside taking actions using tools led to the emergence of a framework called React. ReAct enters a loop of thinking about which action to take, taking that action, observing the result, and deciding on a subsequent action until a solution is found. Although this approach has proved to outperform baseline LLMs, it should be noted that evaluating the performance of these systems and achieving reliable results is still a major challenge. Security is also a concern as LLMs are given the ability to take actions, such as posting on the web. Orchestrator Orchestrators provide a framework to tie these components together. They create abstractions on top of LLMs, prompt templates, data sources, agents, and tools. Templating frameworks like Guidance and LMQL enable complex prompts that specify inputs, outputs, and constraints. In addition, these tools can improve performance by providing memory management, token healing, beam search, session management, error handling, and more. Being able to create a logical control flow on top of LLMs enables developers to build differentiated, specialized, and hardened use cases. Monitoring Given the stochastic nature and rapid evolution of LLM models, your application should be monitored in production. Standard monitoring of CPU, GPU, memory usage, latency, and throughput should be tracked. Drift and outlier detectors can also be deployed to alert you of changing or anomalous inputs over time. Requests and responses should also be logged so that unexpected or harmful outputs can be evaluated. With complex chains, or graphs, of LLM agents strung together, it can be difficult to track exactly how the prompt is evolving. Tools like LangSmith and Seldon Core v2 provide the ability to trace the flow of data, providing visibility into the behaviour of your LLM application. Not only that, but Seldon Core v2 provides a data-centric deployment graph that enables advanced monitoring with drift detectors and explainers. Conclusion Deploying and managing LLMs in a production environment can be challenging due to resource management, model performance, model versioning, and infrastructure issues. LLMs are simple to deploy and administer in a production setting using MLflow’s tools and APIs for managing the model lifecycle. In this blog, we discussed how to use MLflow to deploy and manage LLMs in a production environment, along with support for Hugging Face transformers, Open AI, and Lang Chain models. The collaboration between data scientists, engineers, and other stakeholders in the machine learning lifecycle can be improved by using MLflow. Help improve contributions Mark contributions as unhelpful if you find them irrelevant or not valuable to the article. This feedback is private to you and won’t be shared publicly. Got it Contribution hidden for you This feedback is never shared publicly, we’ll use it to show better\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `scrape_text` with `{'url': 'https://www.reddit.com/r/MachineLearning/comments/16fklqj/discussion_how_to_deploy_llm_in_production/'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mReddit - Dive into anything Skip to main content Reddit and its partners use cookies and similar technologies to provide you with a better experience. By accepting all cookies, you agree to our use of cookies to deliver and maintain our services and site, improve the quality of Reddit, personalize Reddit content and advertising, and measure the effectiveness of advertising. By rejecting non-essential cookies, Reddit may still use certain cookies to ensure the proper functionality of our platform. For more information, please see our Cookie Notice and our Privacy Policy . Open menu Open navigation Go to Reddit Home r/MachineLearning A chip A close button Get app Get the Reddit app Log In Log in to Reddit Expand user menu Open settings menu Log In / Sign Up Advertise on Reddit Shop Collectible Avatars Get the Reddit app Scan this QR code to download the app now Or check it out in the app stores Go to MachineLearning r/MachineLearning r/MachineLearning ml Members Online • unknow_from_vietnam ADMIN MOD [Discussion] How to deploy LLM in production Discussion hello everyone, I am currently developing a response generator using GPT2, I use weights and code from Huggingface. Currently, I am having some problems deploying the model to production, with the need to handle many requests within the allowed time. I used ONNX to convert but it encountered many problems when having to separate the modules. Is there any other way for me to convert quickly and more conveniently, or is there a way to serve LLM models in production? Read more Top 1% Rank by size &nbsp; TOPICS Gaming Valheim Genshin Impact Minecraft Pokimane Halo Infinite Call of Duty: Warzone Path of Exile Hollow Knight: Silksong Escape from Tarkov Watch Dogs: Legion Sports NFL NBA Megan Anderson Atlanta Hawks Los Angeles Lakers Boston Celtics Arsenal F.C. Philadelphia 76ers Premier League UFC Business GameStop Moderna Pfizer Johnson & Johnson AstraZeneca Walgreens Best Buy Novavax SpaceX Tesla Crypto Cardano Dogecoin Algorand Bitcoin Litecoin Basic Attention Token Bitcoin Cash Television The Real Housewives of Atlanta The Bachelor Sister Wives 90 Day Fiance Wife Swap The Amazing Race Australia Married at First Sight The Real Housewives of Dallas My 600-lb Life Last Week Tonight with John Oliver Celebrity Kim Kardashian Doja Cat Iggy Azalea Anya Taylor-Joy Jamie Lee Curtis Natalie Portman Henry Cavill Millie Bobby Brown Tom Hiddleston Keanu Reeves RESOURCES About Reddit Advertise Help Blog Careers Press Communities Best of Reddit Topics Content Policy Privacy Policy User Agreement Reddit, Inc. © 2024. All rights reserved.\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `scrape_text` with `{'url': 'https://arize.com/blog-course/large-language-model-llm-deployment/'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mBest Practices for Large Language Model (LLM) Deployment - Arize AI Join us on July 11th for Arize:Observe @ Shack15 → Platform Overview Left Column Monitors Dashboards Eval & Performance Tracing Explainability & Fairness Right Column Embeddings & RAG Analyzer LLM Tracing Fine Tune Phoenix OSS Arize Product Demo: See the Platform in Action Watch now Solutions Overview Left Column Use Cases Computer Vision Recommender Systems Regression & Classification Forecasting Right Column Industries Financial Services eCommerce Media & Entertainment Autonomous Vehicles Manufacturing Biotechnology & Pharmaceutical Research Customers Learn more Pricing Learn Left Column Resources Blog Podcast Events Videos Paper Readings Right Column Arize University Certification Basic and advanced course certification LLMOps Self-guided LLMops learning ML Observability Self-guided observability learning Arize Community Great discussions, support, and random acts of swag await! Join now Docs Company About Partners Careers Press Security Arize AI Introduces Next Generation of Its ML Observability Platform, Goes Self-Serve Learn more Book a Demo Sign In Sign Up × ARIZE GENERATIVE AI COURSE Intro To LLMOps Intro To LLMOps and LLM Observability What is LLM Observability? Introduction To LLMOps Foundation Models OpenAI API LLM Deployment LLM Evaluation LLM Evaluation: Build and Benchmark Evals What To Avoid The Needle In a Haystack Test Evals from OpenAI Traces and Spans Traces and Spans in Orchestration Frameworks Search and Retrieval: Build Your Own Chatbot Retrieval Augmented Generation (RAG) Benchmarking Evaluation of LLM RAG RAG Evaluation Prompt Engineering Evaluating Prompts: A Developer’s Guide Chains and Agents AI Agents: When and How To Implement LLM Agent: Set Up ReAct Architecture | Simple AI Agents LLM Guardrails and Controls for Deployment Guardrails Use Cases Structured Data Extraction Certifications LLM Certification Agents, Tools, and Chains LLM Evaluations Traces and Spans ML Observability Fundamentals Advanced Model Performance Metrics New to Arize? Arize onboarding Best Practices for Large Language Model (LLM) Deployment Sally-Ann DeLucia, ML Solutions Engineer | Published June 06, 2023 Large language models (LLMs) are revolutionizing the AI landscape. Despite being relatively new, many teams are jumping at the opportunity to deploy these powerful models in their organizations. While excitement is high, knowing where to start can be a daunting task. This post covers the ins-and-outs of deploying LLMs, including: tradeoffs between models, optimization — prompt engineering, fine tuning, and context retrieval — deployment strategies, and what to do after launch. Tradeoffs Between Custom and Commercial Models (Build Versus Buy LLMs) Benefits of Building One of the first decisions to make when deciding to deploy a LLM is whether to build a LLM from scratch or use a commercial model. Both options have their advantages, and the answer might actually be both. Let’s get into it. One benefit of building an LLM from scratch is that you have complete data ownership and privacy. Not only will your proprietary data be kept private but you can also make the most out of it. By leveraging your own data, you have the ability to control where the data powering your LLM comes from. This allows you to make sure the LLM is trained on reliable sources that won’t contribute to bias. You also don’t have to worry about your proprietary data being inadvertently used by a third party for training or leaked . Using your own data could also lead to superior performance, potentially giving you a competitive advantage. You can also decide exactly what content filters to use based on your specific business use case. For example, you might need a longer sequence length than what a commercial model offers or the ability to add specific content filters to your training data. When you opt to use a commercial LLM, you have to work around the sequence limits and have no visibility of the data used for training. Commercial models, like GPT-4, are known to have a delay ranging from 5 to 30 seconds depending on the query. This latency issue is not ideal for applications that require real-time responses. When you build your LLM from scratch, you can opt for a smaller model with fewer layers and less parameters which tend to have faster inference speeds than their larger peers. There’s growing evidence that smaller models can be just as powerful as larger models for domain specific tasks. BioMedLM , a biomedical-specific LLM using a mere 2.7 billion parameters, performed just as well if not better than a 120 billion parameter competitor. Another benefit: your model is smaller this will save you a bunch in training and serving costs. Since you have complete model ownership when you opt to build from scratch, you also have better introspection and explainability . A commercial model is a black box; since you have little to no access to the inner workings of the model, understanding why the model is behaving the way it does is extremely difficult. Benefits of Commercial Models One of the biggest challenges when building an LLM from scratch is the cost. Building a LLM can be extremely expensive. GPT-4, for example, reportedly cost $100 million to train. There’s also a lot of risk. With little room for error, you could end up wasting thousands or even millions of dollars — leaving you with only a suboptimal model. You also need a large volume of high quality and diverse data in order for the model to gain the required generalization capabilities to power your system. Using a commercial model, on the other hand, means far less training costs. Since you do not have to worry about hosting or pre-training a commercial LLM, the only cost occurs at inference. Training costs would only be acquired from doing tests and experiments during development. Another benefit of commercial LLMs is they require less technical expertise. Finally, commercial models can also are a great tool for prototyping and exploring. So…Which? Overall, the choice of whether or not to build or buy an LLM comes down to your specific application, resources, traffic, and data ownership concerns. Teams with domain-specific applications might opt to build a model from scratch whereas teams looking to leverage the latest and greatest to build downstream applications might use a commercial model. Prior to heavily investing in either option, you may want to experiment with the technology to understand what’s possible and carefully consider your specific requirements. What Are the Benefits of Open Source Over ChatGPT and Other Commercial LLMs? While build-versus-commercial is the mainstream debate, let’s not forget about the open source options. There have been some impressive open source models that are available for commercial use. Dolly, MPT-7B, and RedPajama are just a few examples of open source models with commercial licenses that are rivaling popular commercial models like GPT-4 and LLaMA. Open source LLMs allow you to leverage powerful models that have already learned a vast amount of data without having to be dependent on a service. Since you are not starting completely from scratch, there can be huge savings on training time and budget. This allows you to get your model in production sooner. Just like building and using commercial LLMs, open source LLMs have downsides as well. While you save on costs at inference time by not having to pay a service provider, if you have a low usage then using a commercial model might actually lead to a cost savings. The cost benefits of open source models is seen when the requested volume is more than one million (see: Skanda Vivek’s great piece on LLM economics). The main cost associated with using open source is hosting and deploying these large models. When you have thousands of requests a day, paying a service provider often works out cheaper than paying at inference. In addition to cost, open source models, while less demanding than building from scratch, still require substantial lift. Similar domain expertise is needed to train, fine-tune, and host an open source LLM. There is also evidence to support that reproducibility is still an issue for open source LLMs. Without the proper expertise you risk wasting time and resources. A Practical Guide to Optimization of Foundation Models The steps to deployment will differ depending on your use case and model choice. Commercial models, for example, do not allow for fine tuning. Instead, you opt for prompt engineering and context retrieval to optimize your LLM. When using an open source or your own LLM, you’ll likely use a combination of these techniques to refine the model’s performance. Prompt Engineering — Shaping Performance with the Right Instructions Prompt engineering is a new field in AI where engineers focus on crafting prompts to feed the model, with prompts defined as the set of instructions the LLM will use to perform the task. Prompts play a huge role in optimizing and controlling the performance of LLMs. Prompt engineering allows developers to provide the LLM with context to perform a specific task. A good prompt will assist the model in providing high quality responses that are relevant and accurate. This allows you to harness the power of LLMs without needing extensive fine-tuning. Since prompts are at the center of guiding model output towards the desired direction, prompt engineering is a crucial step in deploying an LLM. An effective prompt will allow your LLM to correctly and reliably respond to your query. You want your prompt to be clear and include specific instructions. Some best practices for prompts include: Write clear and specific instructions Use delimiters to indicate specific pieces of the prompt Outline the structure of the desired output Use guidelines to check if certain conditions are met Leverage few-shot prompting to give successful examples of completing the task Prompt engineering is a\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `scrape_text` with `{'url': 'https://www.youtube.com/watch?v=NRjUMB9BrhQ'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mA Must for Deploying LLM applications to Production - YouTube Acerca de Imprensa Direitos de autor Contactar-nos Criadores Publicidade Programadores Termos Privacidade Política e segurança Como funciona o YouTube Testar novas funcionalidades © 2024 Google LLC\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `scrape_text` with `{'url': 'https://huyenchip.com/2023/04/11/llm-engineering.html'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mBuilding LLM applications for production Building LLM applications for production | Chip Huyen Chip Huyen Blog Books List 100 MLOps Guide Tiếng Việt Building LLM applications for production Apr 11, 2023 • Chip Huyen [ Hacker News discussion , LinkedIn discussion , Twitter thread ] A question that I’ve been asked a lot recently is how large language models (LLMs) will change machine learning workflows. After working with several companies who are working with LLM applications and personally going down a rabbit hole building my applications, I realized two things: It’s easy to make something cool with LLMs, but very hard to make something production-ready with them. LLM limitations are exacerbated by a lack of engineering rigor in prompt engineering, partially due to the ambiguous nature of natural languages, and partially due to the nascent nature of the field. This post consists of three parts. Part 1 discusses the key challenges of productionizing LLM applications and the solutions that I’ve seen. Part 2 discusses how to compose multiple tasks with control flows (e.g. if statement, for loop) and incorporate tools (e.g. SQL executor, bash, web browsers, third-party APIs) for more complex and powerful applications. Part 3 covers some of the promising use cases that I’ve seen companies building on top of LLMs and how to construct them from smaller tasks. There has been so much written about LLMs, so feel free to skip any section you’re already familiar with. Table of contents Part I. Challenges of productionizing prompt engineering …….. The ambiguity of natural languages ………… Prompt evaluation ………… Prompt versioning ………… Prompt optimization …….. Cost and latency ………… Cost ………… Latency ………… The impossibility of cost + latency analysis for LLMs …….. Prompting vs. finetuning vs. alternatives ………… Prompt tuning ………… Finetuning with distillation …….. Embeddings + vector databases …….. Backward and forward compatibility Part 2. Task composability …….. Applications that consist of multiple tasks …….. Agents, tools, and control flows ………… Tools vs. plugins ………… Control flows: sequential, parallel, if, for loop ………… Control flow with LLM agents ………… Testing an agent Part 3. Promising use cases …….. AI assistant …….. Chatbot …….. Programming and gaming …….. Learning …….. Talk-to-your-data ………… Can LLMs do data analysis for me? …….. Search and recommendation …….. Sales …….. SEO Conclusion Part I. Challenges of productionizing prompt engineering The ambiguity of natural languages For most of the history of computers, engineers have written instructions in programming languages. Programming languages are “mostly” exact. Ambiguity causes frustration and even passionate hatred in developers (think dynamic typing in Python or JavaScript). In prompt engineering, instructions are written in natural languages, which are a lot more flexible than programming languages. This can make for a great user experience, but can lead to a pretty bad developer experience. The flexibility comes from two directions: how users define instructions, and how LLMs respond to these instructions. First, the flexibility in user-defined prompts leads to silent failures. If someone accidentally makes some changes in code, like adding a random character or removing a line, it’ll likely throw an error. However, if someone accidentally changes a prompt, it will still run but give very different outputs. While the flexibility in user-defined prompts is just an annoyance, the ambiguity in LLMs’ generated responses can be a dealbreaker. It leads to two problems: Ambiguous output format : downstream applications on top of LLMs expect outputs in a certain format so that they can parse. We can craft our prompts to be explicit about the output format, but there’s no guarantee that the outputs will always follow this format. Inconsistency in user experience : when using an application, users expect certain consistency. Imagine an insurance company giving you a different quote every time you check on their website. LLMs are stochastic – there’s no guarantee that an LLM will give you the same output for the same input every time. You can force an LLM to give the same response by setting temperature = 0 , which is, in general, a good practice. While it mostly solves the consistency problem , it doesn’t inspire trust in the system. Imagine a teacher who gives you consistent scores only if that teacher sits in one particular room. If that teacher sits in different rooms, that teacher’s scores for you will be wild. How to solve this ambiguity problem? This seems to be a problem that OpenAI is actively trying to mitigate. They have a notebook with tips on how to increase their models’ reliability. A couple of people who’ve worked with LLMs for years told me that they just accepted this ambiguity and built their workflows around that. It’s a different mindset compared to developing deterministic programs, but not something impossible to get used to. This ambiguity can be mitigated by applying as much engineering rigor as possible. In the rest of this post, we’ll discuss how to make prompt engineering, if not deterministic, systematic. Prompt evaluation A common technique for prompt engineering is to provide in the prompt a few examples and hope that the LLM will generalize from these examples (fewshot learners). As an example, consider trying to give a text a controversy score – it was a fun project that I did to find the correlation between a tweet’s popularity and its controversialness. Here is the shortened prompt with 4 fewshot examples: Example: controversy scorer Given a text, give it a controversy score from 0 to 10.\n",
      "\n",
      "Examples:\n",
      "\n",
      "1 + 1 = 2\n",
      "Controversy score: 0\n",
      "\n",
      "Starting April 15th, only verified accounts on Twitter will be eligible to be in For You recommendations\n",
      "Controversy score: 5\n",
      "\n",
      "Everyone has the right to own and use guns\n",
      "Controversy score: 9\n",
      "\n",
      "Immigration should be completely banned to protect our country\n",
      "Controversy score: 10\n",
      "\n",
      "The response should follow the format:\n",
      "\n",
      "Controversy score: { score }\n",
      "Reason: { reason }\n",
      "\n",
      "Here is the text. When doing fewshot learning, two questions to keep in mind: Whether the LLM understands the examples given in the prompt . One way to evaluate this is to input the same examples and see if the model outputs the expected scores. If the model doesn’t perform well on the same examples given in the prompt, it is likely because the prompt isn’t clear – you might want to rewrite the prompt or break the task into smaller tasks (and combine them together, discussed in detail in Part II of this post). Whether the LLM overfits to these fewshot examples. You can evaluate your model on separate examples. One thing I’ve also found useful is to ask models to give examples for which it would give a certain label. For example, I can ask the model to give me examples of texts for which it’d give a score of 4. Then I’d input these examples into the LLM to see if it’ll indeed output 4. from llm import OpenAILLM\n",
      "\n",
      "def eval_prompt(examples_file, eval_file):\n",
      "    prompt = get_prompt(examples_file)\n",
      "    model = OpenAILLM(prompt=prompt, temperature=0)\n",
      "    compute_rmse(model, examples_file)\n",
      "    compute_rmse(model, eval_file)\n",
      "eval_prompt(\"fewshot_examples.txt\", \"eval_examples.txt\") Prompt versioning Small changes to a prompt can lead to very different results. It’s essential to version and track the performance of each prompt. You can use git to version each prompt and its performance, but I wouldn’t be surprised if there will be tools like MLflow or Weights & Biases for prompt experiments. Prompt optimization There have been many papers + blog posts written on how to optimize prompts. I agree with Lilian Weng in her helpful blog post that most papers on prompt engineering are tricks that can be explained in a few sentences. OpenAI has a great notebook that explains many tips with examples . Here are some of them: Prompt the model to explain or explain step-by-step how it arrives at an answer, a technique known as Chain-of-Thought or COT (Wei et al., 2022). Tradeoff : COT can increase both latency and cost due to the increased number of output tokens [see Cost and latency section] Generate many outputs for the same input. Pick the final output by either the majority vote  (also known as self-consistency technique by Wang et al., 2023) or you can ask your LLM to pick the best one. In OpenAI API, you can generate multiple responses for the same input by passing in the argument n (not an ideal API design if you ask me). Break one big prompt into smaller, simpler prompts. Many tools promise to auto-optimize your prompts – they are quite expensive and usually just apply these tricks. One nice thing about these tools is that they’re no code, which makes them appealing to non-coders. Cost and latency Cost The more explicit detail and examples you put into the prompt, the better the model performance (hopefully), and the more expensive your inference will cost. OpenAI API charges for both the input and output tokens. Depending on the task, a simple prompt might be anything between 300 - 1000 tokens. If you want to include more context, e.g. adding your own documents or info retrieved from the Internet to the prompt, it can easily go up to 10k tokens for the prompt alone. The cost with long prompts isn’t in experimentation but in inference. Experimentation-wise, prompt engineering is a cheap and fast way get something up and running . For example, even if you use GPT-4 with the following setting, your experimentation cost will still be just over $300. The traditional ML cost of collecting data and training models is usually much higher and takes much longer. Prompt: 10k tokens ($0.06/1k tokens) Output: 200 tokens ($0.12/1k tokens) Evaluate on 20 examples Experiment with 25 different versions of prompts The cost of LLMOps is in inference. If you use GPT-4 with 10k tokens in input and 200 tokens in output, it’ll be $0.624 / prediction. If you use GPT-3.5-turbo with 4k tok\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `scrape_text` with `{'url': 'https://www.seldon.io/deploying-large-language-models-in-production'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mFailed to retrieve the webpage: Status code 403\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `web_search` with `{'query': 'best practices to deploy LLM agents to production'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m['https://www.qwak.com/post/best-practices-when-deploying-llms-in-production', 'https://www.linkedin.com/pulse/deploying-llms-production-anatomy-llm-applications-xenonstack', 'https://www.reddit.com/r/MachineLearning/comments/16fklqj/discussion_how_to_deploy_llm_in_production/', 'https://arize.com/blog-course/large-language-model-llm-deployment/', 'https://www.youtube.com/watch?v=NRjUMB9BrhQ', 'https://huyenchip.com/2023/04/11/llm-engineering.html', 'https://aws.amazon.com/blogs/machine-learning/learn-how-to-build-and-deploy-tool-using-llm-agents-using-aws-sagemaker-jumpstart-foundation-models/', 'https://www.seldon.io/deploying-large-language-models-in-production']\u001b[0m\u001b[32;1m\u001b[1;3mHere are some resources that provide best practices for deploying LLM agents to production:\n",
      "\n",
      "1. [Best Practices for Deploying LLMs in Production](https://www.qwak.com/post/best-practices-when-deploying-llms-in-production): This article discusses the challenges and considerations for deploying LLMs in production and how Qwak helps streamline MLOps workflows.\n",
      "\n",
      "2. [Deploying LLMs in Production: The Anatomy of LLM Applications](https://www.linkedin.com/pulse/deploying-llms-production-anatomy-llm-applications-xenonstack): This LinkedIn article explores the deployment of LLMs in production and the components and reference architecture involved.\n",
      "\n",
      "3. [Discussion: How to deploy LLM in production](https://www.reddit.com/r/MachineLearning/comments/16fklqj/discussion_how_to_deploy_llm_in_production/): This Reddit thread discusses the challenges and approaches to deploying LLMs in production, with insights from the machine learning community.\n",
      "\n",
      "4. [Best Practices for Large Language Model (LLM) Deployment](https://arize.com/blog-course/large-language-model-llm-deployment/): This blog post covers the best practices for deploying LLMs, including tradeoffs between custom and commercial models, prompt engineering, embeddings and vector databases, and backward and forward compatibility.\n",
      "\n",
      "5. [A Must for Deploying LLM applications to Production](https://www.youtube.com/watch?v=NRjUMB9BrhQ): This YouTube video provides insights into deploying LLM applications to production and discusses the challenges and considerations involved.\n",
      "\n",
      "6. [Building LLM applications for production](https://huyenchip.com/2023/04/11/llm-engineering.html): This blog post discusses the challenges of productionizing LLM applications and provides solutions and best practices for prompt engineering, cost and latency optimization, and task composability.\n",
      "\n",
      "Please note that some of the links may require further exploration to access the full content.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What are the current best practices to deploy LLM agents to production?',\n",
       " 'output': 'Here are some resources that provide best practices for deploying LLM agents to production:\\n\\n1. [Best Practices for Deploying LLMs in Production](https://www.qwak.com/post/best-practices-when-deploying-llms-in-production): This article discusses the challenges and considerations for deploying LLMs in production and how Qwak helps streamline MLOps workflows.\\n\\n2. [Deploying LLMs in Production: The Anatomy of LLM Applications](https://www.linkedin.com/pulse/deploying-llms-production-anatomy-llm-applications-xenonstack): This LinkedIn article explores the deployment of LLMs in production and the components and reference architecture involved.\\n\\n3. [Discussion: How to deploy LLM in production](https://www.reddit.com/r/MachineLearning/comments/16fklqj/discussion_how_to_deploy_llm_in_production/): This Reddit thread discusses the challenges and approaches to deploying LLMs in production, with insights from the machine learning community.\\n\\n4. [Best Practices for Large Language Model (LLM) Deployment](https://arize.com/blog-course/large-language-model-llm-deployment/): This blog post covers the best practices for deploying LLMs, including tradeoffs between custom and commercial models, prompt engineering, embeddings and vector databases, and backward and forward compatibility.\\n\\n5. [A Must for Deploying LLM applications to Production](https://www.youtube.com/watch?v=NRjUMB9BrhQ): This YouTube video provides insights into deploying LLM applications to production and discusses the challenges and considerations involved.\\n\\n6. [Building LLM applications for production](https://huyenchip.com/2023/04/11/llm-engineering.html): This blog post discusses the challenges of productionizing LLM applications and provides solutions and best practices for prompt engineering, cost and latency optimization, and task composability.\\n\\nPlease note that some of the links may require further exploration to access the full content.'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\": \"What are the current best practices to deploy LLM agents to production?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We encounter a few issues when we try to just add the scrape function to the agent without any other consideration. \n",
    "How about we make this better by improving the inner chain of the agent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `web_search` with `{'query': 'best practices to deploy LLM agents to production'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m['https://www.qwak.com/post/best-practices-when-deploying-llms-in-production', 'https://www.linkedin.com/pulse/deploying-llms-production-anatomy-llm-applications-xenonstack', 'https://www.reddit.com/r/MachineLearning/comments/16fklqj/discussion_how_to_deploy_llm_in_production/', 'https://arize.com/blog-course/large-language-model-llm-deployment/', 'https://www.youtube.com/watch?v=NRjUMB9BrhQ', 'https://huyenchip.com/2023/04/11/llm-engineering.html', 'https://aws.amazon.com/blogs/machine-learning/learn-how-to-build-and-deploy-tool-using-llm-agents-using-aws-sagemaker-jumpstart-foundation-models/', 'https://www.seldon.io/deploying-large-language-models-in-production']\u001b[0m\u001b[32;1m\u001b[1;3mHere are some current best practices for deploying LLM agents to production:\n",
      "\n",
      "1. Understand the requirements: Before deploying an LLM agent, it is important to understand the specific requirements of your application. This includes considering factors such as the size of the model, the computational resources needed, and the expected workload.\n",
      "\n",
      "2. Model selection and fine-tuning: Choose the right LLM model for your application. Consider factors such as model size, performance, and compatibility with your infrastructure. Fine-tune the model on your specific dataset to improve its accuracy and performance.\n",
      "\n",
      "3. Infrastructure considerations: Ensure that your infrastructure can handle the computational requirements of the LLM agent. This may involve using high-performance GPUs or distributed computing resources. Consider using cloud-based solutions for scalability and flexibility.\n",
      "\n",
      "4. Data preprocessing and storage: Preprocess your data to ensure it is in the right format for the LLM agent. Consider using efficient data storage solutions to handle large amounts of data.\n",
      "\n",
      "5. Model deployment: Deploy the LLM agent in a production environment. This may involve setting up a server or cloud-based infrastructure to host the model. Consider using containerization technologies such as Docker for easy deployment and scalability.\n",
      "\n",
      "6. Monitoring and performance optimization: Monitor the performance of the LLM agent in production and optimize it for better efficiency. This may involve fine-tuning hyperparameters, optimizing resource allocation, and implementing caching mechanisms.\n",
      "\n",
      "7. Security and privacy considerations: Ensure that the deployed LLM agent follows best practices for security and privacy. This includes protecting sensitive data, implementing access controls, and regularly updating the model to address security vulnerabilities.\n",
      "\n",
      "8. Continuous improvement: Continuously monitor and evaluate the performance of the LLM agent in production. Collect feedback from users and use it to improve the model and its deployment.\n",
      "\n",
      "Please note that these are general best practices, and the specific requirements and considerations may vary depending on your application and infrastructure. It is recommended to consult with experts and refer to the documentation and resources provided by the LLM framework you are using.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import MessagesPlaceholder\n",
    "\n",
    "MEMORY_KEY = \"chat_history\"\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are very powerful assistant, but bad at calculating lengths of words.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=MEMORY_KEY),\n",
    "        (\"user\", \"{input}\"),\n",
    "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "chat_history = []\n",
    "\n",
    "agent = (\n",
    "    {\n",
    "        \"input\": lambda x: x[\"input\"],\n",
    "        \"agent_scratchpad\": lambda x: format_to_openai_tool_messages(\n",
    "            x[\"intermediate_steps\"]\n",
    "        ),\n",
    "        \"chat_history\": lambda x: x[\"chat_history\"],\n",
    "    }\n",
    "    | prompt\n",
    "    | llm_with_tools\n",
    "    | OpenAIToolsAgentOutputParser()\n",
    ")\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "input1 = \"What are the current best practices to deploy LLM agents to production?\"\n",
    "result = agent_executor.invoke({\"input\": input1, \"chat_history\": chat_history})\n",
    "chat_history.extend(\n",
    "    [\n",
    "        HumanMessage(content=input1),\n",
    "        AIMessage(content=result[\"output\"]),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'What are the current best practices to deploy LLM agents to production?',\n",
       " 'chat_history': [HumanMessage(content='What are the current best practices to deploy LLM agents to production?'),\n",
       "  AIMessage(content='Here are some current best practices for deploying LLM agents to production:\\n\\n1. Understand the requirements: Before deploying an LLM agent, it is important to understand the specific requirements of your application. This includes considering factors such as the size of the model, the computational resources needed, and the expected workload.\\n\\n2. Model selection and fine-tuning: Choose the right LLM model for your application. Consider factors such as model size, performance, and compatibility with your infrastructure. Fine-tune the model on your specific dataset to improve its accuracy and performance.\\n\\n3. Infrastructure considerations: Ensure that your infrastructure can handle the computational requirements of the LLM agent. This may involve using high-performance GPUs or distributed computing resources. Consider using cloud-based solutions for scalability and flexibility.\\n\\n4. Data preprocessing and storage: Preprocess your data to ensure it is in the right format for the LLM agent. Consider using efficient data storage solutions to handle large amounts of data.\\n\\n5. Model deployment: Deploy the LLM agent in a production environment. This may involve setting up a server or cloud-based infrastructure to host the model. Consider using containerization technologies such as Docker for easy deployment and scalability.\\n\\n6. Monitoring and performance optimization: Monitor the performance of the LLM agent in production and optimize it for better efficiency. This may involve fine-tuning hyperparameters, optimizing resource allocation, and implementing caching mechanisms.\\n\\n7. Security and privacy considerations: Ensure that the deployed LLM agent follows best practices for security and privacy. This includes protecting sensitive data, implementing access controls, and regularly updating the model to address security vulnerabilities.\\n\\n8. Continuous improvement: Continuously monitor and evaluate the performance of the LLM agent in production. Collect feedback from users and use it to improve the model and its deployment.\\n\\nPlease note that these are general best practices, and the specific requirements and considerations may vary depending on your application and infrastructure. It is recommended to consult with experts and refer to the documentation and resources provided by the LLM framework you are using.')],\n",
       " 'output': 'Here are some current best practices for deploying LLM agents to production:\\n\\n1. Understand the requirements: Before deploying an LLM agent, it is important to understand the specific requirements of your application. This includes considering factors such as the size of the model, the computational resources needed, and the expected workload.\\n\\n2. Model selection and fine-tuning: Choose the right LLM model for your application. Consider factors such as model size, performance, and compatibility with your infrastructure. Fine-tune the model on your specific dataset to improve its accuracy and performance.\\n\\n3. Infrastructure considerations: Ensure that your infrastructure can handle the computational requirements of the LLM agent. This may involve using high-performance GPUs or distributed computing resources. Consider using cloud-based solutions for scalability and flexibility.\\n\\n4. Data preprocessing and storage: Preprocess your data to ensure it is in the right format for the LLM agent. Consider using efficient data storage solutions to handle large amounts of data.\\n\\n5. Model deployment: Deploy the LLM agent in a production environment. This may involve setting up a server or cloud-based infrastructure to host the model. Consider using containerization technologies such as Docker for easy deployment and scalability.\\n\\n6. Monitoring and performance optimization: Monitor the performance of the LLM agent in production and optimize it for better efficiency. This may involve fine-tuning hyperparameters, optimizing resource allocation, and implementing caching mechanisms.\\n\\n7. Security and privacy considerations: Ensure that the deployed LLM agent follows best practices for security and privacy. This includes protecting sensitive data, implementing access controls, and regularly updating the model to address security vulnerabilities.\\n\\n8. Continuous improvement: Continuously monitor and evaluate the performance of the LLM agent in production. Collect feedback from users and use it to improve the model and its deployment.\\n\\nPlease note that these are general best practices, and the specific requirements and considerations may vary depending on your application and infrastructure. It is recommended to consult with experts and refer to the documentation and resources provided by the LLM framework you are using.'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oreilly-agents",
   "language": "python",
   "name": "oreilly-agents"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
