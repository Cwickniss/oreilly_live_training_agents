{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a research Agent with LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chat = ChatOpenAI(temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://en.wikipedia.org/wiki/Artificial_intelligence',\n",
       " 'https://www.ibm.com/topics/artificial-intelligence',\n",
       " 'https://www.techtarget.com/searchenterpriseai/definition/AI-Artificial-Intelligence',\n",
       " 'https://builtin.com/artificial-intelligence',\n",
       " 'https://cloud.google.com/learn/what-is-artificial-intelligence',\n",
       " 'https://www.investopedia.com/terms/a/artificial-intelligence-ai.asp',\n",
       " 'https://www.britannica.com/technology/artificial-intelligence']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.utilities import SerpAPIWrapper\n",
    "from langchain.agents import tool\n",
    "\n",
    "search = SerpAPIWrapper()\n",
    "\n",
    "@tool\n",
    "def web_search(query: str):\n",
    "    \"\"\"Searches the web given a user query and returns the most relevant links\"\"\"\n",
    "    output = search.results(query)\n",
    "    results = output[\"organic_results\"]\n",
    "    return [r[\"link\"] for r in results]\n",
    "\n",
    "web_search(\"Artificial Intelligence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['agent_scratchpad', 'input'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]], 'agent_scratchpad': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a helpful assistant')), MessagesPlaceholder(variable_name='chat_history', optional=True), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}')), MessagesPlaceholder(variable_name='agent_scratchpad')])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import hub\n",
    "\n",
    "prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['agent_scratchpad', 'input']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.input_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]],\n",
       " 'agent_scratchpad': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.input_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': None,\n",
       " 'input_variables': ['agent_scratchpad', 'input'],\n",
       " 'input_types': {'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]],\n",
       "  'agent_scratchpad': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]},\n",
       " 'output_parser': None,\n",
       " 'partial_variables': {},\n",
       " 'messages': [SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a helpful assistant')),\n",
       "  MessagesPlaceholder(variable_name='chat_history', optional=True),\n",
       "  HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}')),\n",
       "  MessagesPlaceholder(variable_name='agent_scratchpad')],\n",
       " 'validate_template': False}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build an agent I need to do 2 essential things:\n",
    "\n",
    "1. Connect a model to tools\n",
    "2. Set up a loop in which the model continuously tries to solve the problem by either just calling the llm or calling a tool, observing the result and repeating until finished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['agent_scratchpad', 'input'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]], 'agent_scratchpad': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a helpful assistant')), MessagesPlaceholder(variable_name='chat_history', optional=True), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}')), MessagesPlaceholder(variable_name='agent_scratchpad')])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_openai_tools_agent\n",
    "\n",
    "tools = [web_search]\n",
    "\n",
    "react_agent = create_openai_tools_agent(llm_chat, tools, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor\n",
    "\n",
    "\n",
    "react_agent_executor = AgentExecutor(agent=react_agent, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mTo learn a new language effectively, there are several strategies and techniques that can be helpful. Here are some of the best ways to learn a new language:\n",
      "\n",
      "1. Immerse Yourself in the Language: One of the most effective ways to learn a new language is to immerse yourself in it as much as possible. This can be done by living in a country where the language is spoken, or by surrounding yourself with native speakers of the language. Immersion helps you to pick up the language naturally and develop your listening and speaking skills.\n",
      "\n",
      "2. Take Language Classes: Taking language classes can provide you with a structured learning environment and guidance from a qualified teacher. Language classes can help you learn the grammar, vocabulary, and pronunciation of the language. Look for classes that focus on conversation practice to improve your speaking skills.\n",
      "\n",
      "3. Practice Regularly: Consistency is key when learning a new language. Set aside regular time each day or week to practice the language. This can include activities such as reading books or articles in the language, watching movies or TV shows in the language, or practicing with language learning apps or websites.\n",
      "\n",
      "4. Use Language Learning Apps and Websites: There are many language learning apps and websites available that can supplement your learning. These platforms often provide interactive exercises, vocabulary lists, and pronunciation practice. Some popular language learning apps include Duolingo, Babbel, and Rosetta Stone.\n",
      "\n",
      "5. Find a Language Exchange Partner: Language exchange partners are native speakers of the language you are learning who are also interested in learning your native language. This can be a great way to practice speaking and listening skills. You can find language exchange partners through online platforms or language exchange meetups in your area.\n",
      "\n",
      "6. Immerse Yourself in Language Resources: Surround yourself with resources in the language you are learning. This can include books, movies, music, podcasts, and online articles. Consuming media in the language will help you become familiar with the culture and improve your language skills.\n",
      "\n",
      "7. Travel to a Country Where the Language is Spoken: If possible, travel to a country where the language is spoken. This will give you the opportunity to practice your language skills in real-life situations and immerse yourself in the culture. Language immersion programs or study abroad opportunities can provide a structured learning experience while abroad.\n",
      "\n",
      "8. Practice Speaking with Native Speakers: Speaking with native speakers is crucial for improving your language skills. Look for opportunities to practice speaking with native speakers, such as language exchange meetups, conversation groups, or online language communities.\n",
      "\n",
      "9. Set Realistic Goals: Set realistic goals for your language learning journey. Break down your goals into smaller, achievable tasks. This will help you stay motivated and track your progress.\n",
      "\n",
      "10. Be Patient and Persistent: Learning a new language takes time and effort. Be patient with yourself and celebrate small victories along the way. Stay persistent and keep practicing, even when it feels challenging.\n",
      "\n",
      "Remember, everyone learns languages differently, so it's important to find the methods that work best for you. Experiment with different strategies and techniques to find what helps you learn and retain the language most effectively.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'What is the best way to learn a new language?',\n",
       " 'agent_scratchpad': [],\n",
       " 'output': \"To learn a new language effectively, there are several strategies and techniques that can be helpful. Here are some of the best ways to learn a new language:\\n\\n1. Immerse Yourself in the Language: One of the most effective ways to learn a new language is to immerse yourself in it as much as possible. This can be done by living in a country where the language is spoken, or by surrounding yourself with native speakers of the language. Immersion helps you to pick up the language naturally and develop your listening and speaking skills.\\n\\n2. Take Language Classes: Taking language classes can provide you with a structured learning environment and guidance from a qualified teacher. Language classes can help you learn the grammar, vocabulary, and pronunciation of the language. Look for classes that focus on conversation practice to improve your speaking skills.\\n\\n3. Practice Regularly: Consistency is key when learning a new language. Set aside regular time each day or week to practice the language. This can include activities such as reading books or articles in the language, watching movies or TV shows in the language, or practicing with language learning apps or websites.\\n\\n4. Use Language Learning Apps and Websites: There are many language learning apps and websites available that can supplement your learning. These platforms often provide interactive exercises, vocabulary lists, and pronunciation practice. Some popular language learning apps include Duolingo, Babbel, and Rosetta Stone.\\n\\n5. Find a Language Exchange Partner: Language exchange partners are native speakers of the language you are learning who are also interested in learning your native language. This can be a great way to practice speaking and listening skills. You can find language exchange partners through online platforms or language exchange meetups in your area.\\n\\n6. Immerse Yourself in Language Resources: Surround yourself with resources in the language you are learning. This can include books, movies, music, podcasts, and online articles. Consuming media in the language will help you become familiar with the culture and improve your language skills.\\n\\n7. Travel to a Country Where the Language is Spoken: If possible, travel to a country where the language is spoken. This will give you the opportunity to practice your language skills in real-life situations and immerse yourself in the culture. Language immersion programs or study abroad opportunities can provide a structured learning experience while abroad.\\n\\n8. Practice Speaking with Native Speakers: Speaking with native speakers is crucial for improving your language skills. Look for opportunities to practice speaking with native speakers, such as language exchange meetups, conversation groups, or online language communities.\\n\\n9. Set Realistic Goals: Set realistic goals for your language learning journey. Break down your goals into smaller, achievable tasks. This will help you stay motivated and track your progress.\\n\\n10. Be Patient and Persistent: Learning a new language takes time and effort. Be patient with yourself and celebrate small victories along the way. Stay persistent and keep practicing, even when it feels challenging.\\n\\nRemember, everyone learns languages differently, so it's important to find the methods that work best for you. Experiment with different strategies and techniques to find what helps you learn and retain the language most effectively.\"}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "react_agent_executor.invoke({\"query\": \"What is the best way to learn a new language?\",\n",
    "                             \"agent_scratchpad\": []})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mTo learn good programming practices, there are several approaches you can take. Here are some of the best ways to learn and improve your programming skills:\n",
      "\n",
      "1. Read Books and Online Resources: There are many books and online resources available that cover programming best practices. Some popular books include \"Clean Code\" by Robert C. Martin and \"The Pragmatic Programmer\" by Andrew Hunt and David Thomas. These resources provide valuable insights and guidelines on writing clean, maintainable, and efficient code.\n",
      "\n",
      "2. Take Online Courses: Online learning platforms like Coursera, Udemy, and edX offer a wide range of programming courses that cover best practices. Look for courses that focus on software engineering principles, code quality, and design patterns. These courses often include practical exercises and assignments to help you apply what you've learned.\n",
      "\n",
      "3. Practice Regularly: The more you practice programming, the better you will become. Set aside dedicated time each day or week to work on coding projects. Practice writing clean and efficient code, and challenge yourself to refactor and improve your existing code.\n",
      "\n",
      "4. Join Coding Communities: Engaging with other programmers and joining coding communities can provide valuable insights and feedback on your code. Participate in online forums, discussion boards, and coding communities like Stack Overflow and GitHub. Collaborating with others and reviewing their code can also help you learn from different programming styles and practices.\n",
      "\n",
      "5. Learn from Open Source Projects: Open source projects are a great way to learn from experienced developers and see how they apply best practices. Explore popular open source projects on platforms like GitHub and contribute to them. Reading and understanding well-written code can help you improve your own coding practices.\n",
      "\n",
      "6. Code Reviews: Seek feedback from experienced programmers by asking for code reviews. Code reviews can help identify areas for improvement and provide suggestions for better coding practices. You can ask for code reviews from colleagues, mentors, or online coding communities.\n",
      "\n",
      "7. Stay Updated: Programming languages and best practices evolve over time. Stay updated with the latest trends, tools, and frameworks in the programming community. Follow programming blogs, subscribe to newsletters, and attend conferences or meetups to stay informed about new techniques and best practices.\n",
      "\n",
      "Remember, learning good programming practices is an ongoing process. Continuously strive to improve your skills, seek feedback, and stay updated with the latest industry standards.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'What is the best way to learn good programming practices?',\n",
       " 'agent_scratchpad': [],\n",
       " 'output': 'To learn good programming practices, there are several approaches you can take. Here are some of the best ways to learn and improve your programming skills:\\n\\n1. Read Books and Online Resources: There are many books and online resources available that cover programming best practices. Some popular books include \"Clean Code\" by Robert C. Martin and \"The Pragmatic Programmer\" by Andrew Hunt and David Thomas. These resources provide valuable insights and guidelines on writing clean, maintainable, and efficient code.\\n\\n2. Take Online Courses: Online learning platforms like Coursera, Udemy, and edX offer a wide range of programming courses that cover best practices. Look for courses that focus on software engineering principles, code quality, and design patterns. These courses often include practical exercises and assignments to help you apply what you\\'ve learned.\\n\\n3. Practice Regularly: The more you practice programming, the better you will become. Set aside dedicated time each day or week to work on coding projects. Practice writing clean and efficient code, and challenge yourself to refactor and improve your existing code.\\n\\n4. Join Coding Communities: Engaging with other programmers and joining coding communities can provide valuable insights and feedback on your code. Participate in online forums, discussion boards, and coding communities like Stack Overflow and GitHub. Collaborating with others and reviewing their code can also help you learn from different programming styles and practices.\\n\\n5. Learn from Open Source Projects: Open source projects are a great way to learn from experienced developers and see how they apply best practices. Explore popular open source projects on platforms like GitHub and contribute to them. Reading and understanding well-written code can help you improve your own coding practices.\\n\\n6. Code Reviews: Seek feedback from experienced programmers by asking for code reviews. Code reviews can help identify areas for improvement and provide suggestions for better coding practices. You can ask for code reviews from colleagues, mentors, or online coding communities.\\n\\n7. Stay Updated: Programming languages and best practices evolve over time. Stay updated with the latest trends, tools, and frameworks in the programming community. Follow programming blogs, subscribe to newsletters, and attend conferences or meetups to stay informed about new techniques and best practices.\\n\\nRemember, learning good programming practices is an ongoing process. Continuously strive to improve your skills, seek feedback, and stay updated with the latest industry standards.'}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "react_agent_executor.invoke({\"query\": \"What is the best way to learn good programming practices?\",\n",
    "                             \"agent_scratchpad\": []})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `web_search` with `{'query': 'current most popular open source frameworks to build llm-based agents in Python'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m['https://github.com/Hannibal046/Awesome-LLM', 'https://www.simform.com/blog/open-source-llm/', 'https://odsc.medium.com/9-open-source-llms-and-agents-to-watch-728049d77060', 'https://www.unite.ai/best-open-source-llms/', 'https://neptune.ai/blog/best-open-source-mlops-tools', 'https://www.reddit.com/r/LocalLLaMA/comments/17m2lql/best_framework_for_llm_based_applications_in/', 'https://www.taskade.com/blog/top-autonomous-agents/', 'https://www.marktechpost.com/2024/01/03/10-open-source-tools-for-llm-applications-development/', 'https://www.datacamp.com/blog/top-open-source-llms', 'https://eng-shamsan.medium.com/large-language-model-evaluation-tools-2023-open-source-and-commercial-567525c57f82']\u001b[0m\u001b[32;1m\u001b[1;3mI have found some relevant URLs for you to explore:\n",
      "\n",
      "1. [Awesome-LLM](https://github.com/Hannibal046/Awesome-LLM)\n",
      "2. [Open Source LLM](https://www.simform.com/blog/open-source-llm/)\n",
      "3. [9 Open Source LLMs and Agents to Watch](https://odsc.medium.com/9-open-source-llms-and-agents-to-watch-728049d77060)\n",
      "4. [Best Open Source LLMs](https://www.unite.ai/best-open-source-llms/)\n",
      "5. [Best Open Source MLOps Tools](https://neptune.ai/blog/best-open-source-mlops-tools)\n",
      "\n",
      "You can visit these URLs to learn more about the current most popular open source frameworks to build LLM-based agents in Python.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'Research the current most popular open source frameworks to build llm-based agents in Python and return 5 releavnt urls.',\n",
       " 'agent_scratchpad': [],\n",
       " 'output': 'I have found some relevant URLs for you to explore:\\n\\n1. [Awesome-LLM](https://github.com/Hannibal046/Awesome-LLM)\\n2. [Open Source LLM](https://www.simform.com/blog/open-source-llm/)\\n3. [9 Open Source LLMs and Agents to Watch](https://odsc.medium.com/9-open-source-llms-and-agents-to-watch-728049d77060)\\n4. [Best Open Source LLMs](https://www.unite.ai/best-open-source-llms/)\\n5. [Best Open Source MLOps Tools](https://neptune.ai/blog/best-open-source-mlops-tools)\\n\\nYou can visit these URLs to learn more about the current most popular open source frameworks to build LLM-based agents in Python.'}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "react_agent_executor.invoke({\"query\": \"Research the current most popular open source frameworks to build llm-based agents in Python and return 5 releavnt urls.\",\n",
    "                             \"agent_scratchpad\": []})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's build this same agent but using a more in depth approach going one level down in the abstraction layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's go down the latter of abstraction and build this same agent but leveraging the lower-level capabilities of LangChain's LCEL language. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chat = ChatOpenAI(model=\"gpt-3.5-turbo-16k\",temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import tool\n",
    "from langchain.utilities import SerpAPIWrapper\n",
    "\n",
    "search = SerpAPIWrapper()\n",
    "\n",
    "@tool\n",
    "def web_search(query: str):\n",
    "    \"\"\"Searches the web given a user query and returns the most relevant links\"\"\"\n",
    "    output = search.results(query)\n",
    "    results = output[\"organic_results\"]\n",
    "    return [r[\"link\"] for r in results]\n",
    "\n",
    "\n",
    "tools = [web_search]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\", \"You are a powerful research assistant. Given a user query you will perform research on the topic using the tools available.\"\n",
    "        ),\n",
    "        (\"user\", \"{input}\"),\n",
    "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bind tools to the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.convert_to_openai import format_tool_to_openai_tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_with_tools = llm_chat.bind(tools=[format_tool_to_openai_tool(tool) for tool in tools])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, all we have to do is import two last utility functions:\n",
    "- a component for formatting intermediate steps (agent action, tool output pairs) to input messages that can be sent to the model, \n",
    "- and a component for converting the output message into an agent action/agent finish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.format_scratchpad.openai_tools import format_to_openai_tool_messages\n",
    "from langchain.agents.output_parsers.openai_tools import OpenAIToolsAgentOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = (\n",
    "    {\n",
    "        \"input\": lambda x: x[\"input\"], \n",
    "        \"agent_scratchpad\": lambda x: format_to_openai_tool_messages(\n",
    "            x[\"intermediate_steps\"]\n",
    "        ),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm_with_tools\n",
    "    | OpenAIToolsAgentOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor\n",
    "\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `web_search` with `{'query': 'latest open source LLMs'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m['https://github.com/eugeneyan/open-llms', 'https://www.datacamp.com/blog/top-open-source-llms', 'https://www.unite.ai/best-open-source-llms/', 'https://thenewstack.io/large-language-models-open-source-llms-in-2023/', 'https://github.com/Hannibal046/Awesome-LLM', 'https://medium.datadriveninvestor.com/list-of-open-source-large-language-models-llms-4eac551bda2e', 'https://www.techopedia.com/6-best-open-source-llms-to-watch-out-for-in-2024', 'https://www.upstage.ai/feed/insight/top-open-source-llms-2024', 'https://www.simform.com/blog/open-source-llm/', 'https://blog.n8n.io/open-source-llm/']\u001b[0m\u001b[32;1m\u001b[1;3mHere are some of the latest open source LLMs available:\n",
      "\n",
      "1. [Open-LLMs](https://github.com/eugeneyan/open-llms): A collection of open source LLMs for various tasks.\n",
      "2. [Top Open Source LLMs](https://www.datacamp.com/blog/top-open-source-llms): A blog post that lists some of the top open source LLMs.\n",
      "3. [Best Open Source LLMs](https://www.unite.ai/best-open-source-llms/): An article that highlights the best open source LLMs.\n",
      "4. [Large Language Models: Open Source LLMs in 2023](https://thenewstack.io/large-language-models-open-source-llms-in-2023/): A blog post that discusses the state of open source LLMs in 2023.\n",
      "5. [Awesome-LLM](https://github.com/Hannibal046/Awesome-LLM): A curated list of open source LLMs.\n",
      "6. [List of Open Source Large Language Models (LLMs)](https://medium.datadriveninvestor.com/list-of-open-source-large-language-models-llms-4eac551bda2e): A Medium article that provides a list of open source LLMs.\n",
      "7. [6 Best Open Source LLMs to Watch Out for in 2024](https://www.techopedia.com/6-best-open-source-llms-to-watch-out-for-in-2024): An article that highlights the best open source LLMs to watch out for in 2024.\n",
      "8. [Top Open Source LLMs 2024](https://www.upstage.ai/feed/insight/top-open-source-llms-2024): A blog post that lists the top open source LLMs in 2024.\n",
      "9. [Open Source LLM](https://www.simform.com/blog/open-source-llm/): A blog post that discusses open source LLMs and their benefits.\n",
      "10. [Open Source LLM](https://blog.n8n.io/open-source-llm/): A blog post that explores the concept of open source LLMs.\n",
      "\n",
      "You can visit these links to learn more about the latest open source LLMs.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'actions': [OpenAIToolAgentAction(tool='web_search', tool_input={'query': 'latest open source LLMs'}, log=\"\\nInvoking: `web_search` with `{'query': 'latest open source LLMs'}`\\n\\n\\n\", message_log=[AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_vtpKs3zgTOcLOKczRz9poRpt', 'function': {'arguments': '{\\n  \"query\": \"latest open source LLMs\"\\n}', 'name': 'web_search'}, 'type': 'function'}]})], tool_call_id='call_vtpKs3zgTOcLOKczRz9poRpt')],\n",
       "  'messages': [AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_vtpKs3zgTOcLOKczRz9poRpt', 'function': {'arguments': '{\\n  \"query\": \"latest open source LLMs\"\\n}', 'name': 'web_search'}, 'type': 'function'}]})]},\n",
       " {'steps': [AgentStep(action=OpenAIToolAgentAction(tool='web_search', tool_input={'query': 'latest open source LLMs'}, log=\"\\nInvoking: `web_search` with `{'query': 'latest open source LLMs'}`\\n\\n\\n\", message_log=[AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_vtpKs3zgTOcLOKczRz9poRpt', 'function': {'arguments': '{\\n  \"query\": \"latest open source LLMs\"\\n}', 'name': 'web_search'}, 'type': 'function'}]})], tool_call_id='call_vtpKs3zgTOcLOKczRz9poRpt'), observation=['https://github.com/eugeneyan/open-llms', 'https://www.datacamp.com/blog/top-open-source-llms', 'https://www.unite.ai/best-open-source-llms/', 'https://thenewstack.io/large-language-models-open-source-llms-in-2023/', 'https://github.com/Hannibal046/Awesome-LLM', 'https://medium.datadriveninvestor.com/list-of-open-source-large-language-models-llms-4eac551bda2e', 'https://www.techopedia.com/6-best-open-source-llms-to-watch-out-for-in-2024', 'https://www.upstage.ai/feed/insight/top-open-source-llms-2024', 'https://www.simform.com/blog/open-source-llm/', 'https://blog.n8n.io/open-source-llm/'])],\n",
       "  'messages': [FunctionMessage(content='[\"https://github.com/eugeneyan/open-llms\", \"https://www.datacamp.com/blog/top-open-source-llms\", \"https://www.unite.ai/best-open-source-llms/\", \"https://thenewstack.io/large-language-models-open-source-llms-in-2023/\", \"https://github.com/Hannibal046/Awesome-LLM\", \"https://medium.datadriveninvestor.com/list-of-open-source-large-language-models-llms-4eac551bda2e\", \"https://www.techopedia.com/6-best-open-source-llms-to-watch-out-for-in-2024\", \"https://www.upstage.ai/feed/insight/top-open-source-llms-2024\", \"https://www.simform.com/blog/open-source-llm/\", \"https://blog.n8n.io/open-source-llm/\"]', name='web_search')]},\n",
       " {'output': 'Here are some of the latest open source LLMs available:\\n\\n1. [Open-LLMs](https://github.com/eugeneyan/open-llms): A collection of open source LLMs for various tasks.\\n2. [Top Open Source LLMs](https://www.datacamp.com/blog/top-open-source-llms): A blog post that lists some of the top open source LLMs.\\n3. [Best Open Source LLMs](https://www.unite.ai/best-open-source-llms/): An article that highlights the best open source LLMs.\\n4. [Large Language Models: Open Source LLMs in 2023](https://thenewstack.io/large-language-models-open-source-llms-in-2023/): A blog post that discusses the state of open source LLMs in 2023.\\n5. [Awesome-LLM](https://github.com/Hannibal046/Awesome-LLM): A curated list of open source LLMs.\\n6. [List of Open Source Large Language Models (LLMs)](https://medium.datadriveninvestor.com/list-of-open-source-large-language-models-llms-4eac551bda2e): A Medium article that provides a list of open source LLMs.\\n7. [6 Best Open Source LLMs to Watch Out for in 2024](https://www.techopedia.com/6-best-open-source-llms-to-watch-out-for-in-2024): An article that highlights the best open source LLMs to watch out for in 2024.\\n8. [Top Open Source LLMs 2024](https://www.upstage.ai/feed/insight/top-open-source-llms-2024): A blog post that lists the top open source LLMs in 2024.\\n9. [Open Source LLM](https://www.simform.com/blog/open-source-llm/): A blog post that discusses open source LLMs and their benefits.\\n10. [Open Source LLM](https://blog.n8n.io/open-source-llm/): A blog post that explores the concept of open source LLMs.\\n\\nYou can visit these links to learn more about the latest open source LLMs.',\n",
       "  'messages': [AIMessage(content='Here are some of the latest open source LLMs available:\\n\\n1. [Open-LLMs](https://github.com/eugeneyan/open-llms): A collection of open source LLMs for various tasks.\\n2. [Top Open Source LLMs](https://www.datacamp.com/blog/top-open-source-llms): A blog post that lists some of the top open source LLMs.\\n3. [Best Open Source LLMs](https://www.unite.ai/best-open-source-llms/): An article that highlights the best open source LLMs.\\n4. [Large Language Models: Open Source LLMs in 2023](https://thenewstack.io/large-language-models-open-source-llms-in-2023/): A blog post that discusses the state of open source LLMs in 2023.\\n5. [Awesome-LLM](https://github.com/Hannibal046/Awesome-LLM): A curated list of open source LLMs.\\n6. [List of Open Source Large Language Models (LLMs)](https://medium.datadriveninvestor.com/list-of-open-source-large-language-models-llms-4eac551bda2e): A Medium article that provides a list of open source LLMs.\\n7. [6 Best Open Source LLMs to Watch Out for in 2024](https://www.techopedia.com/6-best-open-source-llms-to-watch-out-for-in-2024): An article that highlights the best open source LLMs to watch out for in 2024.\\n8. [Top Open Source LLMs 2024](https://www.upstage.ai/feed/insight/top-open-source-llms-2024): A blog post that lists the top open source LLMs in 2024.\\n9. [Open Source LLM](https://www.simform.com/blog/open-source-llm/): A blog post that discusses open source LLMs and their benefits.\\n10. [Open Source LLM](https://blog.n8n.io/open-source-llm/): A blog post that explores the concept of open source LLMs.\\n\\nYou can visit these links to learn more about the latest open source LLMs.')]}]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(agent_executor.stream({\"input\": \"What are the latest open source LLMs available?\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, what if we want the agent to actually read the links and return a research report rather than just the urls?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "@tool\n",
    "def scrape_text(url: str):\n",
    "    \"\"\"Scrapes the text from a given URL and returns it.\"\"\"\n",
    "    # Send a GET request to the webpage\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "\n",
    "        # Check if the request was successful\n",
    "        if response.status_code == 200:\n",
    "            # Parse the content of the request with BeautifulSoup\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "            # Extract all text from the webpage\n",
    "            page_text = soup.get_text(separator=\" \", strip=True)\n",
    "\n",
    "            # Print the extracted text\n",
    "            return page_text[:10000]\n",
    "        else:\n",
    "            return f\"Failed to retrieve the webpage: Status code {response.status_code}\"\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return f\"Failed to retrieve the webpage: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [web_search, scrape_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_with_tools = llm_chat.bind(tools=[format_tool_to_openai_tool(tool) for tool in tools])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\", \"You are a powerful research assistant. Given a user query you will perform research on the topic using the tools available.\"\n",
    "        ),\n",
    "        (\"user\", \"Scrape and summarize the contents from these urls: {urls}\"),\n",
    "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = (\n",
    "    {\n",
    "        \"urls\": lambda x: x[\"input\"], \n",
    "        \"agent_scratchpad\": lambda x: format_to_openai_tool_messages(\n",
    "            x[\"intermediate_steps\"]\n",
    "        ),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm_with_tools\n",
    "    | OpenAIToolsAgentOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor\n",
    "\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `scrape_text` with `{'url': 'https://towardsdatascience.com/deploying-llm-agents-to-production-best-practices-7e4e8e8e8e8e'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mFailed to retrieve the webpage: Status code 403\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `scrape_text` with `{'url': 'https://towardsdatascience.com/deploying-llm-agents-to-production-best-practices-7e4e8e8e8e8e'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mFailed to retrieve the webpage: Status code 403\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `scrape_text` with `{'url': 'https://towardsdatascience.com/deploying-llm-agents-to-production-best-practices-7e4e8e8e8e8e'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mFailed to retrieve the webpage: Status code 403\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `web_search` with `{'query': 'best practices to deploy LLM agents to production'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m['https://medium.com/@aigeek_/best-practices-for-deploying-large-language-models-llms-in-production-fdc5bf240d6a', 'https://www.qwak.com/post/best-practices-when-deploying-llms-in-production', 'https://www.linkedin.com/pulse/deploying-llms-production-anatomy-llm-applications-xenonstack', 'https://arize.com/blog-course/large-language-model-llm-deployment/', 'https://aws.amazon.com/blogs/machine-learning/learn-how-to-build-and-deploy-tool-using-llm-agents-using-aws-sagemaker-jumpstart-foundation-models/', 'https://huyenchip.com/2023/04/11/llm-engineering.html', 'https://www.seldon.io/deploying-large-language-models-in-production', 'https://python.langchain.com/docs/guides/deployments/', 'https://www.reddit.com/r/LangChain/comments/14g7qui/discussion_biggest_roadblocks_to_deploy_llms_to/', 'https://www.youtube.com/watch?v=t-dMfo2pONs']\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `scrape_text` with `{'url': 'https://medium.com/@aigeek_/best-practices-for-deploying-large-language-models-llms-in-production-fdc5bf240d6a'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mFailed to retrieve the webpage: Status code 403\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `scrape_text` with `{'url': 'https://www.qwak.com/post/best-practices-when-deploying-llms-in-production'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mBest Practices for Deploying LLMs in Production | Qwak Live Webinar Mastering MLOps and ML Engineering: Key Strategies for 2024 January 24th 12:00 PM EST -> x Platform Our Products Model Registry Model Training Model Serving Model Monitoring Feature Store Vector Store Feature + Vector Pipeline Managed Notebooks Resources Blog Events & Webinars Customer Stories Announcements Compare Qwak LLMOps Webinar Mastering MLOps and ML Engineering: Key Strategies for 2024 with Yuval Fernbach Documentation Pricing Company About us Partners Careers Contact us Follow us Log in Book Demo Start Free Book a Demo Start for Free Back to Blog LLMops Best Practices for Deploying LLMs in Production Large Language Models (LLMs) have taken the world by storm. Thanks to powerful AI models such as OpenAI's GPT-4 and the popularity of products such as ChatGPT, companies are now racing to integrate LLMs into their products. Guy Eshet Product Manager at Qwak April 30, 2023 Introduction Large Language Models (LLMs) have taken the world by storm. Thanks to powerful AI models such as OpenAI's GPT-4 and the popularity of products such as ChatGPT, companies are now racing to integrate LLMs into their products. Launching exciting prototypes with generative models may seem easy, however deploying LLMs in production can be a complex process that requires robust infrastructure and proper tools.Â In this article, we will explore the challenges and considerations for deploying LLMs in production, and learn how Qwak is helping customers in various industries to streamline their MLOps workflows and deploy LLMs in production. What are LLMs? LLMs are large machine learning models that use deep learning to generate, summarize and understand natural language, text and code. LLMs are trained on vast amounts of data, and demand immense compute power to train and maintain.Â In recent years, there has been a significant advancement in the inference abilities of LLMs. These models are now capable of performing complex tasks such as creative writing, coding, information retrieval, summarization, and many more. LLMs have become valuable tools for a wide range of applications, including chatbots, voice assistants, search and automated content generation, among others. Closed-source and open-source LLMs LLMs can be broadly classified into two categories: closed-source and open-source models.Â Closed-source models, such as GPT-4 by OpenAI, LaMDA by Google, and Jurassic by AI21, are proprietary models that are only available through APIs. These models offer limited customization options and control over your data, as the source code and model weights are not available to the public. However, closed-source models are often more powerful than their open-source counterparts, as they are developed by industry leaders with access to vast resources and expertise. On the other hand, open-source models, such as BLOOM, Alpaca, LLaMA, OpenAssistant, and BERT, are rapidly evolving and catching up. They offer more flexibility and can be deployed on your own infrastructure while being customized to your specific needs. Make sure to check the model licenses before using them. Sme models are available for research purposes only and cannot be used for commercial applications.Â Customizing large language models Prompt engineering, fine-tuning, and creating chains are three common techniques used to generate output from LLMs. Prompt Engineering Prompt engineering involves the art of providing a prompt to the model. This can include a question, sentence, or topic for the model to focus on. The purpose is to guide the model output towards the desired direction. Prompt engineering can help to control the output and ensure that it's relevant and useful. Model Chains Creating chains involves using the output of one model as the input for another model, creating a chain of models that work together to generate the final output. This technique is often used to improve the quality and relevance of the output. As an example, LangChain is one of the most common LLM frameworks supporting these use cases. Additionally, AI-Agent frameworks such as Auto-GPT use similar mechanisms to use LLMs to complete complex tasks. Model Fine Tuning Fine-tuning involves training an LLM on a specific dataset or task to improve its performance on that particular task. Fine-tuning can help to optimize the model's output for a specific use case or application. For instance, a fine-tuned LLM could be designed to process financial data or expertly handle customer requests. Fine-tuning models requires significant infrastructure and can be done on managed models like GPT-4, although the inference cost is generally higher. Fine Tuning on Qwak To make the process easier, Qwak enables our customers to fine-tune, tag, and track their fine-tuned models more effectively. This helps to ensure better control over the various model versions between different fine-tuning processes. Additionally, Qwak makes it easier to connect your existing data pipelines with the model fine-tuning process, allowing data science teams to operate faster.Â Integrating LLMs into your products Incorporating LLMs into your products can be challenging, as it involves considerations such as privacy, security, and legal issues. These concerns vary widely when choosing to deploy in-house LLMs or when using external models. In the near future, it's possible that companies will use external GPT-4-like models for early prototyping and high-stakes applications, while opting for smaller, more fine-tuned open-source, self hosted LLMs for other use cases. This approach will likely lead to a mix of both large and small models in the production pipeline. About Qwak Qwak is a fully managed, accessible, and reliable ML Platform. It allows builders to transform and store data, build, train, and deploy models, and monitor the entire Machine Learning pipeline. Pay-as-you-go pricing makes it easy to scale when needed. Infer Virtual Conference by Qwak March 20th, 9 am PT -> Say goodbye to complex MLOps Chat with us to see the platform live and discover how we can help simplify your ML journey. Book a Meeting Related posts LLMops RAG and LLM: A New Frontier in Dynamic Language Modeling Pavel Klushin December 28, 2023 LLMops How to Fine-Tune LLMs on Custom Datasets Alon Lev June 11, 2023 LLMops The Costs of Leveraging the Power of GPT4 Lior Penso March 19, 2023 Get started with Qwak today. See how the best ML engineers and data scientists deploy models in production. Say goodbye to complex MLOps and start delivering today. Try Qwak free Book a Demo Build high-quality ML simply. Â© 2024 Qwak. All rights reserved. Terms & Conditions Privacy Policy Products Model Registry Model Training Model Serving Model Monitoring Feature Store Vector Store Feature + Vector Pipeline Managed Notebooks Resources Blog Webinars Case Studies Documentation Announcements Compare Qwak Qwak vs. Amazon SageMaker Qwak vs. Vertex AI Qwak vs. Databricks Company Pricing About us Careers Partners Contact us Follow us Sign up for Qwak updates\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `scrape_text` with `{'url': 'https://www.linkedin.com/pulse/deploying-llms-production-anatomy-llm-applications-xenonstack'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mDeploying LLMS in Production: The Anatomy of LLM Applications LinkedIn and 3rd parties use essential and non-essential cookies to provide, secure, analyze and improve our Services, and to show you relevant ads (including professional and job ads ) on and off LinkedIn. Learn more in our Cookie Policy . Select Accept to consent or Reject to decline non-essential cookies for this use. You can update your choices at any time in your settings . Accept Reject Agree & Join LinkedIn By clicking Continue, you agree to LinkedIn’s User Agreement , Privacy Policy , and Cookie Policy . Sign in to view more content Create your free account or sign in to continue your search Sign in Welcome back Email or phone Password Show Forgot password? Sign in or By clicking Continue, you agree to LinkedIn’s User Agreement , Privacy Policy , and Cookie Policy . New to LinkedIn? Join now or By clicking Continue, you agree to LinkedIn’s User Agreement , Privacy Policy , and Cookie Policy . New to LinkedIn? Join now Skip to main content LinkedIn Articles People Learning Jobs Join now Sign in Anatomy of LLM Applications Deploying LLMs in Production: The Anatomy of LLM Applications Report this article XenonStack XenonStack Real Time Data and AI Company #platformengineering #dataops #generativeai #decisionintelligence #realtimedata Published Oct 10, 2023 + Follow Introduction Large Language Models (LLMs) are deep learning algorithms that can be used for a wide range of NLP (Natural Language Processing). Large language models use transformer models and are trained using massive datasets — hence, large. This enables them to recognize, translate, predict, or generate text or other content. Large language models are also referred to as neural networks (NNs), which are computing systems inspired by the human brain. These neural networks work using a network of nodes that are layered, much like neurons. Subscribe the Newsletter LLM applications and use cases LLMs have opened a new class of enterprise use cases. Being trained on such a large amount of information, these models provide a strong foundation to build off. This makes them incredibly versatile and easily adaptable to suit a variety of applications. Whether leveraging the built-in knowledge of the model or fine-tuning to your use case, there are many ways to provide a useful and intuitive user experience. Some LLM use applications are as follows: - Chatbots It provides customer support, product recommendations and conversational search. Document Understanding Summarize, exact key information and analyse sentiments. Code completion Explain and document code, suggest improvement and auto generate code. Content generation Write a blog post, emails, ad copies and outline based on prompts. Search Answer natural language questions on top of a large collection of texts. Translation Translate between languages and perform style transfer to change the tone and reading level. LLM components and reference architecture LLM Models The LLM model is the heart of the LLM application and selecting a model depends on a variety of factors. The primary elements are the size of the model (# of parameters) and whether it is open sourced or behind an API. Some models may have tokenizers and embedding models built in, while others will require you to run these steps yourself. Tokenizers split input text into smaller chunks while embedding models convert the text into numeric vectors which an LLM can understand. The easiest place to begin prototyping is with OpenAI’s GPT-4, as it is quick to get up and running given how adaptable the model is. In some cases, a long context window may be required if performing a lot of in-context learning, making Claude 2 an excellent choice. However, when using such large models, both the cost and latency could become prohibitive. Prompt Template Much like the experimentation process that has evolved in traditional Machine Learning model training, prompt engineering has emerged as a core activity in LLM application development. There are several prompting techniques that lead to better results. The first technique is to write clear instructions and specify the steps required to complete the task. Most tasks can be done using a zero-shot prompt but adding some examples with a few-shot prompt can further improve and tailor the response. Even simply including phrases such as show your work or let’s think step by step leads the model to iteratively develop a solution iteratively, increasing the chances of a correct answer. This chain-of-thought prompting is even more powerful if a few examples of reasoning logic are provided. Prompts can then be templated and reused by an LLM application. They can be integrated into your code with a simple f-string or str. format(), but there are also libraries like LangChain, Guidance, and LMQL that offer more control. For chat completion APIs, there is generally a system prompt to assign the task, followed by alternating user and assistant messages. You can seed the chat with examples of user questions and assistant answers before running it. Experimenting and iterating on these prompt templates in a structured way will lead to improved model performance. The outputs of the model should be evaluated by either a scoring function or human feedback. Vector Database Many use cases will require access to information that the LLM has not been trained on. This may be a private knowledge base (e.g., a company wiki), recent information (e.g., events this weekend), or a specific document (e.g., a research paper). There might be other cases where the context window of the model is too small to include everything in the prompt (e.g., an entire book). Adding an external database can provide access to supplementary information in a technique known as retrieval augmented generation (RAG). Vector databases have been around for some time but have surged in popularity recently as they make their way into LLM applications. Relevant external information is first chunked into blocks (e.g., sentences or paragraphs), tokenized, and run through an embedding model. These are then fed into a vector database, such as Pinecone, Chrome, Qdrant, or pgvector (open source). When a prompt is made, it is also vectorized and a similarity search is performed to retrieve relevant entries from the database. These entries are then fed in alongside the original prompt, providing the context needed to provide a coherent response. The LLM may cite which chunks it used in its response, which adds a degree of trust to counteract potential hallucinations. Agents and Tools Using LLMs alone can enable powerful applications, but there are inherent limitations. For example, they cannot continually prompt themselves, make external API calls, nor retrieve a web page. An LLM agent has access to tools that can perform actions beyond text generation. For example, an LLM agent could perform a web search, execute code, perform a database lookup, or do maths calculations. OpenAI language models can decide which tools to use and return a JSON object with the arguments to call the function. This enables a whole new suite of use cases, such as booking a flight, generating an image, or sending an email. Some frameworks enable agents to execute iteratively to complete a task. This could be as simple as breaking down a task into several subtasks or “self-asking” to continue gathering relevant information for the task. These can then be executed in a chain to work towards a correct answer iteratively. The idea of combining this reasoning alongside taking actions using tools led to the emergence of a framework called React. ReAct enters a loop of thinking about which action to take, taking that action, observing the result, and deciding on a subsequent action until a solution is found. Although this approach has proved to outperform baseline LLMs, it should be noted that evaluating the performance of these systems and achieving reliable results is still a major challenge. Security is also a concern as LLMs are given the ability to take actions, such as posting on the web. Orchestrator Orchestrators provide a framework to tie these components together. They create abstractions on top of LLMs, prompt templates, data sources, agents, and tools. Templating frameworks like Guidance and LMQL enable complex prompts that specify inputs, outputs, and constraints. In addition, these tools can improve performance by providing memory management, token healing, beam search, session management, error handling, and more. Being able to create a logical control flow on top of LLMs enables developers to build differentiated, specialized, and hardened use cases. Monitoring Given the stochastic nature and rapid evolution of LLM models, your application should be monitored in production. Standard monitoring of CPU, GPU, memory usage, latency, and throughput should be tracked. Drift and outlier detectors can also be deployed to alert you of changing or anomalous inputs over time. Requests and responses should also be logged so that unexpected or harmful outputs can be evaluated. With complex chains, or graphs, of LLM agents strung together, it can be difficult to track exactly how the prompt is evolving. Tools like LangSmith and Seldon Core v2 provide the ability to trace the flow of data, providing visibility into the behaviour of your LLM application. Not only that, but Seldon Core v2 provides a data-centric deployment graph that enables advanced monitoring with drift detectors and explainers. Conclusion Deploying and managing LLMs in a production environment can be challenging due to resource management, model performance, model versioning, and infrastructure issues. LLMs are simple to deploy and administer in a production setting using MLflow’s tools and APIs for managing the model lifecycle. In this blog, we discussed how to use MLflow to deploy and manage LLMs in a production environment, along with support for Hugging Face tra\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `scrape_text` with `{'url': 'https://arize.com/blog-course/large-language-model-llm-deployment/'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mBest Practices for Large Language Model (LLM) Deployment - Arize AI Free LLM Observability Courses | Enroll now! Platform Overview Left Column Monitors Dashboards Eval & Performance Tracing Explainability & Fairness Right Column Embeddings & RAG Analyzer LLM Traces & Spans Fine Tune Phoenix OSS Arize Product Demo: See the Platform in Action Watch now Solutions Overview Left Column Use Cases Computer Vision Recommender Systems Regression & Classification Forecasting Right Column Industries Financial Services eCommerce Media & Entertainment Autonomous Vehicles Manufacturing Biotechnology & Pharmaceutical Research Customers Learn more Pricing Learn Left Column Resources Blog Podcast Events Videos Paper Readings Right Column Arize University Certification Basic and advanced course certification LLMOps Self-guided LLMops learning ML Observability Self-guided observability learning Arize Community Great discussions, support, and random acts of swag await! Join now Docs Company About Partners Careers Press Security Arize AI Introduces Next Generation of Its ML Observability Platform, Goes Self-Serve Learn more Book a Demo Sign In Sign Up × ARIZE GENERATIVE AI COURSE Intro To LLMOps Intro To LLMOps and LLM Observability What is LLM Observability? Introduction To LLMOps Foundation Models OpenAI API LLM Deployment LLM Evaluation LLM Evaluation: Build and Benchmark Evals LLM-Assisted Evals vs Other Methods Navigating LLM Leaderboards Evals from OpenAI Traces and Spans Traces and Spans in Orchestration Frameworks Search and Retrieval: Build Your Own Chatbot Introduction To Retrieval Augmented Generation Benchmarking Evaluation of LLM RAG 👀 More Coming Soon Prompt Engineering Evaluating Prompts: A Developer’s Guide Chains and Agents AI Agents: When and How To Implement LLM Agent: Set Up ReAct Architecture | Simple AI Agents LLM Guardrails and Controls for Deployment Guardrails Use Cases Structured Data Extraction Certifications Arize University New to Arize? Arize onboarding Glossary See all terms Best Practices for Large Language Model (LLM) Deployment Sally-Ann DeLucia, ML Solutions Engineer | Published June 14, 2023 Large language models (LLMs) are revolutionizing the AI landscape. Despite being relatively new, many teams are jumping at the opportunity to deploy these powerful models in their organizations. While excitement is high, knowing where to start can be a daunting task. This post covers the ins-and-outs of deploying LLMs, including: tradeoffs between models, optimization — prompt engineering, fine tuning, and context retrieval — deployment strategies, and what to do after launch. Tradeoffs Between Custom and Commercial Models (Build Versus Buy LLMs) Benefits of Building One of the first decisions to make when deciding to deploy a LLM is whether to build a LLM from scratch or use a commercial model. Both options have their advantages, and the answer might actually be both. Let’s get into it. One benefit of building an LLM from scratch is that you have complete data ownership and privacy. Not only will your proprietary data be kept private but you can also make the most out of it. By leveraging your own data, you have the ability to control where the data powering your LLM comes from. This allows you to make sure the LLM is trained on reliable sources that won’t contribute to bias. You also don’t have to worry about your proprietary data being inadvertently used by a third party for training or leaked . Using your own data could also lead to superior performance, potentially giving you a competitive advantage. You can also decide exactly what content filters to use based on your specific business use case. For example, you might need a longer sequence length than what a commercial model offers or the ability to add specific content filters to your training data. When you opt to use a commercial LLM, you have to work around the sequence limits and have no visibility of the data used for training. Commercial models, like GPT-4, are known to have a delay ranging from 5 to 30 seconds depending on the query. This latency issue is not ideal for applications that require real-time responses. When you build your LLM from scratch, you can opt for a smaller model with fewer layers and less parameters which tend to have faster inference speeds than their larger peers. There’s growing evidence that smaller models can be just as powerful as larger models for domain specific tasks. BioMedLM , a biomedical-specific LLM using a mere 2.7 billion parameters, performed just as well if not better than a 120 billion parameter competitor. Another benefit: your model is smaller this will save you a bunch in training and serving costs. Since you have complete model ownership when you opt to build from scratch, you also have better introspection and explainability . A commercial model is a black box; since you have little to no access to the inner workings of the model, understanding why the model is behaving the way it does is extremely difficult. Benefits of Commercial Models One of the biggest challenges when building an LLM from scratch is the cost. Building a LLM can be extremely expensive. GPT-4, for example, reportedly cost $100 million to train. There’s also a lot of risk. With little room for error, you could end up wasting thousands or even millions of dollars — leaving you with only a suboptimal model. You also need a large volume of high quality and diverse data in order for the model to gain the required generalization capabilities to power your system. Using a commercial model, on the other hand, means far less training costs. Since you do not have to worry about hosting or pre-training a commercial LLM, the only cost occurs at inference. Training costs would only be acquired from doing tests and experiments during development. Another benefit of commercial LLMs is they require less technical expertise. Finally, commercial models can also are a great tool for prototyping and exploring. So…Which? Overall, the choice of whether or not to build or buy an LLM comes down to your specific application, resources, traffic, and data ownership concerns. Teams with domain-specific applications might opt to build a model from scratch whereas teams looking to leverage the latest and greatest to build downstream applications might use a commercial model. Prior to heavily investing in either option, you may want to experiment with the technology to understand what’s possible and carefully consider your specific requirements. What Are the Benefits of Open Source Over ChatGPT and Other Commercial LLMs? While build-versus-commercial is the mainstream debate, let’s not forget about the open source options. There have been some impressive open source models that are available for commercial use. Dolly, MPT-7B, and RedPajama are just a few examples of open source models with commercial licenses that are rivaling popular commercial models like GPT-4 and LLaMA. Open source LLMs allow you to leverage powerful models that have already learned a vast amount of data without having to be dependent on a service. Since you are not starting completely from scratch, there can be huge savings on training time and budget. This allows you to get your model in production sooner. Just like building and using commercial LLMs, open source LLMs have downsides as well. While you save on costs at inference time by not having to pay a service provider, if you have a low usage then using a commercial model might actually lead to a cost savings. The cost benefits of open source models is seen when the requested volume is more than one million (see: Skanda Vivek’s great piece on LLM economics). The main cost associated with using open source is hosting and deploying these large models. When you have thousands of requests a day, paying a service provider often works out cheaper than paying at inference. In addition to cost, open source models, while less demanding than building from scratch, still require substantial lift. Similar domain expertise is needed to train, fine-tune, and host an open source LLM. There is also evidence to support that reproducibility is still an issue for open source LLMs. Without the proper expertise you risk wasting time and resources. A Practical Guide to Optimization of Foundation Models The steps to deployment will differ depending on your use case and model choice. Commercial models, for example, do not allow for fine tuning. Instead, you opt for prompt engineering and context retrieval to optimize your LLM. When using an open source or your own LLM, you’ll likely use a combination of these techniques to refine the model’s performance. Prompt Engineering — Shaping Performance with the Right Instructions Prompt engineering is a new field in AI where engineers focus on crafting prompts to feed the model, with prompts defined as the set of instructions the LLM will use to perform the task. Prompts play a huge role in optimizing and controlling the performance of LLMs. Prompt engineering allows developers to provide the LLM with context to perform a specific task. A good prompt will assist the model in providing high quality responses that are relevant and accurate. This allows you to harness the power of LLMs without needing extensive fine-tuning. Since prompts are at the center of guiding model output towards the desired direction, prompt engineering is a crucial step in deploying an LLM. An effective prompt will allow your LLM to correctly and reliably respond to your query. You want your prompt to be clear and include specific instructions. Some best practices for prompts include: Write clear and specific instructions Use delimiters to indicate specific pieces of the prompt Outline the structure of the desired output Use guidelines to check if certain conditions are met Leverage few-shot prompting to give successful examples of completing the task Prompt engineering is an iterative process, it is unlikely that you will get the desired o\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `scrape_text` with `{'url': 'https://aws.amazon.com/blogs/machine-learning/learn-how-to-build-and-deploy-tool-using-llm-agents-using-aws-sagemaker-jumpstart-foundation-models/'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mLearn how to build and deploy tool-using LLM agents using AWS SageMaker JumpStart Foundation Models | AWS Machine Learning Blog Skip to Main Content Click here to return to Amazon Web Services homepage Contact Us Support English My Account Sign In Create an AWS Account Products Solutions Pricing Documentation Learn Partner Network AWS Marketplace Customer Enablement Events Explore More Close عربي Bahasa Indonesia Deutsch English Español Français Italiano Português Tiếng Việt Türkçe Ρусский ไทย 日本語 한국어 中文 (简体) 中文 (繁體) Close My Profile Sign out of AWS Builder ID AWS Management Console Account Settings Billing & Cost Management Security Credentials AWS Personal Health Dashboard Close Support Center Expert Help Knowledge Center AWS Support Overview AWS re:Post Click here to return to Amazon Web Services homepage Get Started for Free Contact Us Products Solutions Pricing Introduction to AWS Getting Started Documentation Training and Certification Developer Center Customer Success Partner Network AWS Marketplace Support AWS re:Post Log into Console Download the Mobile App AWS Blog Home Blogs Editions Close Architecture AWS Cloud Operations & Migrations AWS for Games AWS Insights AWS Marketplace AWS News AWS Partner Network AWS Smart Business Big Data Business Intelligence Business Productivity Cloud Enterprise Strategy Cloud Financial Management Compute Contact Center Containers Database Desktop & Application Streaming Developer Tools DevOps Front-End Web & Mobile HPC Industries Integration & Automation Internet of Things Machine Learning Media Messaging & Targeting Microsoft Workloads on AWS .NET on AWS Networking & Content Delivery Open Source Public Sector Quantum Computing Robotics SAP Security Spatial Computing Startups Storage Supply Chain & Logistics Training & Certification Close 中国版 日本版 한국 에디션 기술 블로그 Edisi Bahasa Indonesia AWS Thai Blog Édition Française Deutsche Edition Edição em Português Edición en Español Версия на русском Türkçe Sürüm AWS Machine Learning Blog Learn how to build and deploy tool-using LLM agents using AWS SageMaker JumpStart Foundation Models by John Hwang | on 15 SEP 2023 | in Advanced (300) , Amazon SageMaker JumpStart , Technical How-to | Permalink | Comments | Share Large language model (LLM) agents are programs that extend the capabilities of standalone LLMs with 1) access to external tools (APIs, functions, webhooks, plugins, and so on), and 2) the ability to plan and execute tasks in a self-directed fashion. Often, LLMs need to interact with other software, databases, or APIs to accomplish complex tasks. For example, an administrative chatbot that schedules meetings would require access to employees’ calendars and email. With access to tools, LLM agents can become more powerful—at the cost of additional complexity. In this post, we introduce LLM agents and demonstrate how to build and deploy an e-commerce LLM agent using Amazon SageMaker JumpStart and AWS Lambda. The agent will use tools to provide new capabilities, such as answering questions about returns (“Is my return rtn001 processed?”) and providing updates about orders (“Could you tell me if order 123456 has shipped?”). These new capabilities require LLMs to fetch data from multiple data sources ( orders , returns ) and perform retrieval augmented generation (RAG). To power the LLM agent, we use a Flan-UL2 model deployed as a SageMaker endpoint and use data retrieval tools built with AWS Lambda. The agent can subsequently be integrated with Amazon Lex and used as a chatbot inside websites or AWS Connect . We conclude the post with items to consider before deploying LLM agents to production. For a fully managed experience for building LLM agents, AWS also provides the agents for Amazon Bedrock feature (in preview) . A brief overview of LLM agent architectures LLM agents are programs that use LLMs to decide when and how to use tools as necessary to complete complex tasks. With tools and task planning abilities, LLM agents can interact with outside systems and overcome traditional limitations of LLMs, such as knowledge cutoffs, hallucinations, and imprecise calculations. Tools can take a variety of forms, such as API calls, Python functions, or webhook-based plugins. For example, an LLM can use a “retrieval plugin” to fetch relevant context and perform RAG. So what does it mean for an LLM to pick tools and plan tasks? There are numerous approaches (such as ReAct , MRKL , Toolformer, HuggingGPT , and Transformer Agent s) to using LLMs with tools, and advancements are happening rapidly. But one simple way is to prompt an LLM with a list of tools and ask it to determine 1) if a tool is needed to satisfy the user query, and if so, 2) select the appropriate tool. Such a prompt typically looks like the following example and may include few-shot examples to improve the LLM’s reliability in picking the right tool. ‘’’\n",
      "Your task is to select a tool to answer a user question. You have access to the following tools.\n",
      "\n",
      "search: search for an answer in FAQs\n",
      "order: order items\n",
      "noop: no tool is needed\n",
      "\n",
      "{few shot examples}\n",
      "\n",
      "Question: {input}\n",
      "Tool:\n",
      "‘’’ More complex approaches involve using a specialized LLM that can directly decode “API calls” or “tool use,” such as GorillaLLM . Such finetuned LLMs are trained on API specification datasets to recognize and predict API calls based on instruction. Often, these LLMs require some metadata about available tools (descriptions, yaml, or JSON schema for their input parameters) in order to output tool invocations. This approach is taken by agents for Amazon Bedrock and OpenAI function calls . Note that LLMs generally need to be sufficiently large and complex in order to show tool selection ability. Assuming task planning and tool selection mechanisms are chosen, a typical LLM agent program works in the following sequence: User request – The program takes a user input such as “Where is my order 123456 ?” from some client application. Plan next action(s) and select tool(s) to use – Next, the program uses a prompt to have the LLM generate the next action, for example, “Look up the orders table using OrdersAPI .” The LLM is prompted to suggest a tool name such as OrdersAPI from a predefined list of available tools and their descriptions. Alternatively, the LLM could be instructed to directly generate an API call with input parameters such as OrdersAPI(12345) . Note that the next action may or may not involve using a tool or API. If not, the LLM would respond to user input without incorporating additional context from tools or simply return a canned response such as, “I cannot answer this question.” Parse tool request – Next, we need to parse out and validate the tool/action prediction suggested by the LLM. Validation is needed to ensure tool names, APIs, and request parameters aren’t hallucinated and that the tools are properly invoked according to specification. This parsing may require a separate LLM call. Invoke tool – Once valid tool name(s) and parameter(s) are ensured, we invoke the tool. This could be an HTTP request, function call, and so on. Parse output – The response from the tool may need additional processing. For example, an API call may result in a long JSON response, where only a subset of fields are of interest to the LLM. Extracting information in a clean, standardized format can help the LLM interpret the result more reliably. Interpret output – Given the output from the tool, the LLM is prompted again to make sense of it and decide whether it can generate the final answer back to the user or whether additional actions are required. Terminate or continue to step 2 – Either return a final answer or a default answer in the case of errors or timeouts. Different agent frameworks execute the previous program flow differently. For example, ReAct combines tool selection and final answer generation into a single prompt, as opposed to using separate prompts for tool selection and answer generation. Also, this logic can be run in a single pass or run in a while statement (the “agent loop”), which terminates when the final answer is generated, an exception is thrown, or timeout occurs. What remains constant is that agents use the LLM as the centerpiece to orchestrate planning and tool invocations until the task terminates. Next, we show how to implement a simple agent loop using AWS services. Solution overview For this blog post, we implement an e-commerce support LLM agent that provides two functionalities powered by tools: Return status retrieval tool – Answer questions about the status of returns such as, “What is happening to my return rtn001 ?” Order status retrieval tool – Track the status of orders such as, “What’s the status of my order 123456 ?” The agent effectively uses the LLM as a query router. Given a query (“What is the status of order 123456 ?”), select the appropriate retrieval tool to query across multiple data sources (that is, returns and orders). We accomplish query routing by having the LLM pick among multiple retrieval tools, which are responsible for interacting with a data source and fetching context. This extends the simple RAG pattern, which assumes a single data source. Both retrieval tools are Lambda functions that take an id ( orderId or returnId ) as input, fetches a JSON object from the data source, and converts the JSON into a human friendly representation string that’s suitable to be used by LLM. The data source in a real-world scenario could be a highly scalable NoSQL database such as DynamoDB , but this solution employs simple Python Dict with sample data for demo purposes. Additional functionalities can be added to the agent by adding Retrieval Tools and modifying prompts accordingly. This agent can be tested a standalone service that integrates with any UI over HTTP, which can be done easily with Amazon Lex . Here are some additional details about the key components: LLM inference endpoint – The core of an agent program is an LLM. We will use SageMaker JumpStar\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `scrape_text` with `{'url': 'https://huyenchip.com/2023/04/11/llm-engineering.html'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mBuilding LLM applications for production Building LLM applications for production | Chip Huyen Chip Huyen Blog Books List 100 MLOps Guide Tiếng Việt Building LLM applications for production Apr 11, 2023 • Chip Huyen [ Hacker News discussion , LinkedIn discussion , Twitter thread ] A question that I’ve been asked a lot recently is how large language models (LLMs) will change machine learning workflows. After working with several companies who are working with LLM applications and personally going down a rabbit hole building my applications, I realized two things: It’s easy to make something cool with LLMs, but very hard to make something production-ready with them. LLM limitations are exacerbated by a lack of engineering rigor in prompt engineering, partially due to the ambiguous nature of natural languages, and partially due to the nascent nature of the field. This post consists of three parts. Part 1 discusses the key challenges of productionizing LLM applications and the solutions that I’ve seen. Part 2 discusses how to compose multiple tasks with control flows (e.g. if statement, for loop) and incorporate tools (e.g. SQL executor, bash, web browsers, third-party APIs) for more complex and powerful applications. Part 3 covers some of the promising use cases that I’ve seen companies building on top of LLMs and how to construct them from smaller tasks. There has been so much written about LLMs, so feel free to skip any section you’re already familiar with. Table of contents Part I. Challenges of productionizing prompt engineering …….. The ambiguity of natural languages ………… Prompt evaluation ………… Prompt versioning ………… Prompt optimization …….. Cost and latency ………… Cost ………… Latency ………… The impossibility of cost + latency analysis for LLMs …….. Prompting vs. finetuning vs. alternatives ………… Prompt tuning ………… Finetuning with distillation …….. Embeddings + vector databases …….. Backward and forward compatibility Part 2. Task composability …….. Applications that consist of multiple tasks …….. Agents, tools, and control flows ………… Tools vs. plugins ………… Control flows: sequential, parallel, if, for loop ………… Control flow with LLM agents ………… Testing an agent Part 3. Promising use cases …….. AI assistant …….. Chatbot …….. Programming and gaming …….. Learning …….. Talk-to-your-data ………… Can LLMs do data analysis for me? …….. Search and recommendation …….. Sales …….. SEO Conclusion Part I. Challenges of productionizing prompt engineering The ambiguity of natural languages For most of the history of computers, engineers have written instructions in programming languages. Programming languages are “mostly” exact. Ambiguity causes frustration and even passionate hatred in developers (think dynamic typing in Python or JavaScript). In prompt engineering, instructions are written in natural languages, which are a lot more flexible than programming languages. This can make for a great user experience, but can lead to a pretty bad developer experience. The flexibility comes from two directions: how users define instructions, and how LLMs respond to these instructions. First, the flexibility in user-defined prompts leads to silent failures. If someone accidentally makes some changes in code, like adding a random character or removing a line, it’ll likely throw an error. However, if someone accidentally changes a prompt, it will still run but give very different outputs. While the flexibility in user-defined prompts is just an annoyance, the ambiguity in LLMs’ generated responses can be a dealbreaker. It leads to two problems: Ambiguous output format : downstream applications on top of LLMs expect outputs in a certain format so that they can parse. We can craft our prompts to be explicit about the output format, but there’s no guarantee that the outputs will always follow this format. Inconsistency in user experience : when using an application, users expect certain consistency. Imagine an insurance company giving you a different quote every time you check on their website. LLMs are stochastic – there’s no guarantee that an LLM will give you the same output for the same input every time. You can force an LLM to give the same response by setting temperature = 0 , which is, in general, a good practice. While it mostly solves the consistency problem , it doesn’t inspire trust in the system. Imagine a teacher who gives you consistent scores only if that teacher sits in one particular room. If that teacher sits in different rooms, that teacher’s scores for you will be wild. How to solve this ambiguity problem? This seems to be a problem that OpenAI is actively trying to mitigate. They have a notebook with tips on how to increase their models’ reliability. A couple of people who’ve worked with LLMs for years told me that they just accepted this ambiguity and built their workflows around that. It’s a different mindset compared to developing deterministic programs, but not something impossible to get used to. This ambiguity can be mitigated by applying as much engineering rigor as possible. In the rest of this post, we’ll discuss how to make prompt engineering, if not deterministic, systematic. Prompt evaluation A common technique for prompt engineering is to provide in the prompt a few examples and hope that the LLM will generalize from these examples (fewshot learners). As an example, consider trying to give a text a controversy score – it was a fun project that I did to find the correlation between a tweet’s popularity and its controversialness. Here is the shortened prompt with 4 fewshot examples: Example: controversy scorer Given a text, give it a controversy score from 0 to 10.\n",
      "\n",
      "Examples:\n",
      "\n",
      "1 + 1 = 2\n",
      "Controversy score: 0\n",
      "\n",
      "Starting April 15th, only verified accounts on Twitter will be eligible to be in For You recommendations\n",
      "Controversy score: 5\n",
      "\n",
      "Everyone has the right to own and use guns\n",
      "Controversy score: 9\n",
      "\n",
      "Immigration should be completely banned to protect our country\n",
      "Controversy score: 10\n",
      "\n",
      "The response should follow the format:\n",
      "\n",
      "Controversy score: { score }\n",
      "Reason: { reason }\n",
      "\n",
      "Here is the text. When doing fewshot learning, two questions to keep in mind: Whether the LLM understands the examples given in the prompt . One way to evaluate this is to input the same examples and see if the model outputs the expected scores. If the model doesn’t perform well on the same examples given in the prompt, it is likely because the prompt isn’t clear – you might want to rewrite the prompt or break the task into smaller tasks (and combine them together, discussed in detail in Part II of this post). Whether the LLM overfits to these fewshot examples. You can evaluate your model on separate examples. One thing I’ve also found useful is to ask models to give examples for which it would give a certain label. For example, I can ask the model to give me examples of texts for which it’d give a score of 4. Then I’d input these examples into the LLM to see if it’ll indeed output 4. from llm import OpenAILLM\n",
      "\n",
      "def eval_prompt(examples_file, eval_file):\n",
      "    prompt = get_prompt(examples_file)\n",
      "    model = OpenAILLM(prompt=prompt, temperature=0)\n",
      "    compute_rmse(model, examples_file)\n",
      "    compute_rmse(model, eval_file)\n",
      "eval_prompt(\"fewshot_examples.txt\", \"eval_examples.txt\") Prompt versioning Small changes to a prompt can lead to very different results. It’s essential to version and track the performance of each prompt. You can use git to version each prompt and its performance, but I wouldn’t be surprised if there will be tools like MLflow or Weights & Biases for prompt experiments. Prompt optimization There have been many papers + blog posts written on how to optimize prompts. I agree with Lilian Weng in her helpful blog post that most papers on prompt engineering are tricks that can be explained in a few sentences. OpenAI has a great notebook that explains many tips with examples . Here are some of them: Prompt the model to explain or explain step-by-step how it arrives at an answer, a technique known as Chain-of-Thought or COT (Wei et al., 2022). Tradeoff : COT can increase both latency and cost due to the increased number of output tokens [see Cost and latency section] Generate many outputs for the same input. Pick the final output by either the majority vote  (also known as self-consistency technique by Wang et al., 2023) or you can ask your LLM to pick the best one. In OpenAI API, you can generate multiple responses for the same input by passing in the argument n (not an ideal API design if you ask me). Break one big prompt into smaller, simpler prompts. Many tools promise to auto-optimize your prompts – they are quite expensive and usually just apply these tricks. One nice thing about these tools is that they’re no code, which makes them appealing to non-coders. Cost and latency Cost The more explicit detail and examples you put into the prompt, the better the model performance (hopefully), and the more expensive your inference will cost. OpenAI API charges for both the input and output tokens. Depending on the task, a simple prompt might be anything between 300 - 1000 tokens. If you want to include more context, e.g. adding your own documents or info retrieved from the Internet to the prompt, it can easily go up to 10k tokens for the prompt alone. The cost with long prompts isn’t in experimentation but in inference. Experimentation-wise, prompt engineering is a cheap and fast way get something up and running . For example, even if you use GPT-4 with the following setting, your experimentation cost will still be just over $300. The traditional ML cost of collecting data and training models is usually much higher and takes much longer. Prompt: 10k tokens ($0.06/1k tokens) Output: 200 tokens ($0.12/1k tokens) Evaluate on 20 examples Experiment with 25 different versions of prompts The cost of LLMOps is in inference. If you use GPT-4 with 10k tokens in input and 200 tokens in output, it’ll be $0.624 / prediction. If you use GPT-3.5-turbo with 4k tok\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `scrape_text` with `{'url': 'https://www.seldon.io/deploying-large-language-models-in-production'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mFailed to retrieve the webpage: Status code 403\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `scrape_text` with `{'url': 'https://python.langchain.com/docs/guides/deployments/'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mDeployment | 🦜️🔗 Langchain Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations Guides API More Versioning Changelog Developer's guide Templates Cookbooks Tutorials YouTube videos 🦜️🔗 LangSmith LangSmith Docs LangServe GitHub Templates GitHub Templates Hub LangChain Hub JS/TS Docs Chat Search Debugging Deployment LangChain Templates Evaluation Fallbacks Run LLMs locally Model comparison Privacy Pydantic compatibility Safety Deployment On this page Deployment In today's fast-paced technological landscape, the use of Large Language Models (LLMs) is rapidly expanding. As a result, it is crucial for developers to understand how to effectively deploy these models in production environments. LLM interfaces typically fall into two categories: Case 1: Utilizing External LLM Providers (OpenAI, Anthropic, etc.) In this scenario, most of the computational burden is handled by the LLM providers, while LangChain simplifies the implementation of business logic around these services. This approach includes features such as prompt templating, chat message generation, caching, vector embedding database creation, preprocessing, etc. Case 2: Self-hosted Open-Source Models Alternatively, developers can opt to use smaller, yet comparably capable, self-hosted open-source LLM models. This approach can significantly decrease costs, latency, and privacy concerns associated with transferring data to external LLM providers. Regardless of the framework that forms the backbone of your product, deploying LLM applications comes with its own set of challenges. It's vital to understand the trade-offs and key considerations when evaluating serving frameworks. Outline ​ This guide aims to provide a comprehensive overview of the requirements for deploying LLMs in a production setting, focusing on: Designing a Robust LLM Application Service Maintaining Cost-Efficiency Ensuring Rapid Iteration Understanding these components is crucial when assessing serving systems. LangChain integrates with several open-source projects designed to tackle these issues, providing a robust framework for productionizing your LLM applications. Some notable frameworks include: Ray Serve BentoML OpenLLM Modal Jina These links will provide further information on each ecosystem, assisting you in finding the best fit for your LLM deployment needs. Designing a Robust LLM Application Service ​ When deploying an LLM service in production, it's imperative to provide a seamless user experience free from outages. Achieving 24/7 service availability involves creating and maintaining several sub-systems surrounding your application. Monitoring ​ Monitoring forms an integral part of any system running in a production environment. In the context of LLMs, it is essential to monitor both performance and quality metrics. Performance Metrics: These metrics provide insights into the efficiency and capacity of your model. Here are some key examples: Query per second (QPS): This measures the number of queries your model processes in a second, offering insights into its utilization. Latency: This metric quantifies the delay from when your client sends a request to when they receive a response. Tokens Per Second (TPS): This represents the number of tokens your model can generate in a second. Quality Metrics: These metrics are typically customized according to the business use-case. For instance, how does the output of your system compare to a baseline, such as a previous version? Although these metrics can be calculated offline, you need to log the necessary data to use them later. Fault tolerance ​ Your application may encounter errors such as exceptions in your model inference or business logic code, causing failures and disrupting traffic. Other potential issues could arise from the machine running your application, such as unexpected hardware breakdowns or loss of spot-instances during high-demand periods. One way to mitigate these risks is by increasing redundancy through replica scaling and implementing recovery mechanisms for failed replicas. However, model replicas aren't the only potential points of failure. It's essential to build resilience against various failures that could occur at any point in your stack. Zero down time upgrade ​ System upgrades are often necessary but can result in service disruptions if not handled correctly. One way to prevent downtime during upgrades is by implementing a smooth transition process from the old version to the new one. Ideally, the new version of your LLM service is deployed, and traffic gradually shifts from the old to the new version, maintaining a constant QPS throughout the process. Load balancing ​ Load balancing, in simple terms, is a technique to distribute work evenly across multiple computers, servers, or other resources to optimize the utilization of the system, maximize throughput, minimize response time, and avoid overload of any single resource. Think of it as a traffic officer directing cars (requests) to different roads (servers) so that no single road becomes too congested. There are several strategies for load balancing. For example, one common method is the Round Robin strategy, where each request is sent to the next server in line, cycling back to the first when all servers have received a request. This works well when all servers are equally capable. However, if some servers are more powerful than others, you might use a Weighted Round Robin or Least Connections strategy, where more requests are sent to the more powerful servers, or to those currently handling the fewest active requests. Let's imagine you're running a LLM chain. If your application becomes popular, you could have hundreds or even thousands of users asking questions at the same time. If one server gets too busy (high load), the load balancer would direct new requests to another server that is less busy. This way, all your users get a timely response and the system remains stable. Maintaining Cost-Efficiency and Scalability ​ Deploying LLM services can be costly, especially when you're handling a large volume of user interactions. Charges by LLM providers are usually based on tokens used, making a chat system inference on these models potentially expensive. However, several strategies can help manage these costs without compromising the quality of the service. Self-hosting models ​ Several smaller and open-source LLMs are emerging to tackle the issue of reliance on LLM providers. Self-hosting allows you to maintain similar quality to LLM provider models while managing costs. The challenge lies in building a reliable, high-performing LLM serving system on your own machines. Resource Management and Auto-Scaling ​ Computational logic within your application requires precise resource allocation. For instance, if part of your traffic is served by an OpenAI endpoint and another part by a self-hosted model, it's crucial to allocate suitable resources for each. Auto-scaling—adjusting resource allocation based on traffic—can significantly impact the cost of running your application. This strategy requires a balance between cost and responsiveness, ensuring neither resource over-provisioning nor compromised application responsiveness. Utilizing Spot Instances ​ On platforms like AWS, spot instances offer substantial cost savings, typically priced at about a third of on-demand instances. The trade-off is a higher crash rate, necessitating a robust fault-tolerance mechanism for effective use. Independent Scaling ​ When self-hosting your models, you should consider independent scaling. For example, if you have two translation models, one fine-tuned for French and another for Spanish, incoming requests might necessitate different scaling requirements for each. Batching requests ​ In the context of Large Language Models, batching requests can enhance efficiency by better utilizing your GPU resources. GPUs are inherently parallel processors, designed to handle multiple tasks simultaneously. If you send individual requests to the model, the GPU might not be fully utilized as it's only working on a single task at a time. On the other hand, by batching requests together, you're allowing the GPU to work on multiple tasks at once, maximizing its utilization and improving inference speed. This not only leads to cost savings but can also improve the overall latency of your LLM service. In summary, managing costs while scaling your LLM services requires a strategic approach. Utilizing self-hosting models, managing resources effectively, employing auto-scaling, using spot instances, independently scaling models, and batching requests are key strategies to consider. Open-source libraries such as Ray Serve and BentoML are designed to deal with these complexities. Ensuring Rapid Iteration ​ The LLM landscape is evolving at an unprecedented pace, with new libraries and model architectures being introduced constantly. Consequently, it's crucial to avoid tying yourself to a solution specific to one particular framework. This is especially relevant in serving, where changes to your infrastructure can be time-consuming, expensive, and risky. Strive for infrastructure that is not locked into any specific machine learning library or framework, but instead offers a general-purpose, scalable serving layer. Here are some aspects where flexibility plays a key role: Model composition ​ Deploying systems like LangChain demands the ability to piece together different models and connect them via logic. Take the example of building a natural language input SQL query engine. Querying an LLM and obtaining the SQL command is only part of the system. You need to extract metadata from the connected database, construct a prompt for the LLM, run the SQL query on an engine, collect and feed back the response to the LLM as the query runs, and present the results to the user. This demonstrates the need to seamlessly integrate various complex components built in Python into a dynamic chain of logical \u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `web_search` with `{'query': 'best practices to deploy LLM agents to production'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m['https://medium.com/@aigeek_/best-practices-for-deploying-large-language-models-llms-in-production-fdc5bf240d6a', 'https://www.qwak.com/post/best-practices-when-deploying-llms-in-production', 'https://www.linkedin.com/pulse/deploying-llms-production-anatomy-llm-applications-xenonstack', 'https://arize.com/blog-course/large-language-model-llm-deployment/', 'https://aws.amazon.com/blogs/machine-learning/learn-how-to-build-and-deploy-tool-using-llm-agents-using-aws-sagemaker-jumpstart-foundation-models/', 'https://huyenchip.com/2023/04/11/llm-engineering.html', 'https://www.seldon.io/deploying-large-language-models-in-production', 'https://python.langchain.com/docs/guides/deployments/', 'https://www.reddit.com/r/LangChain/comments/14g7qui/discussion_biggest_roadblocks_to_deploy_llms_to/', 'https://www.youtube.com/watch?v=t-dMfo2pONs']\u001b[0m\u001b[32;1m\u001b[1;3mHere are some resources that provide best practices for deploying LLM agents to production:\n",
      "\n",
      "1. [Best Practices for Deploying Large Language Models (LLMs) in Production](https://medium.com/@aigeek_/best-practices-for-deploying-large-language-models-llms-in-production-fdc5bf240d6a) - This Medium article discusses the challenges and considerations for deploying LLMs in production and provides best practices for prompt engineering, cost management, and monitoring.\n",
      "\n",
      "2. [Best Practices When Deploying LLMs in Production](https://www.qwak.com/post/best-practices-when-deploying-llms-in-production) - This blog post from Qwak provides insights into deploying LLMs in production, including considerations for privacy, security, and legal issues.\n",
      "\n",
      "3. [Deploying LLMs in Production: The Anatomy of LLM Applications](https://www.linkedin.com/pulse/deploying-llms-production-anatomy-llm-applications-xenonstack) - This LinkedIn article explores the deployment of LLM applications in production and discusses the components and reference architecture of LLM applications.\n",
      "\n",
      "4. [Large Language Model (LLM) Deployment](https://arize.com/blog-course/large-language-model-llm-deployment/) - This blog post from Arize AI covers best practices for deploying LLMs in production, including designing a robust LLM application service, maintaining cost-efficiency, and ensuring rapid iteration.\n",
      "\n",
      "5. [Learn How to Build and Deploy Tool-Using LLM Agents Using AWS SageMaker JumpStart Foundation Models](https://aws.amazon.com/blogs/machine-learning/learn-how-to-build-and-deploy-tool-using-llm-agents-using-aws-sagemaker-jumpstart-foundation-models/) - This AWS blog post provides a tutorial on building and deploying LLM agents using AWS SageMaker JumpStart and AWS Lambda.\n",
      "\n",
      "Please note that some of the links may require a subscription or access to specific platforms.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What are the current best practices to deploy LLM agents to production?',\n",
       " 'output': 'Here are some resources that provide best practices for deploying LLM agents to production:\\n\\n1. [Best Practices for Deploying Large Language Models (LLMs) in Production](https://medium.com/@aigeek_/best-practices-for-deploying-large-language-models-llms-in-production-fdc5bf240d6a) - This Medium article discusses the challenges and considerations for deploying LLMs in production and provides best practices for prompt engineering, cost management, and monitoring.\\n\\n2. [Best Practices When Deploying LLMs in Production](https://www.qwak.com/post/best-practices-when-deploying-llms-in-production) - This blog post from Qwak provides insights into deploying LLMs in production, including considerations for privacy, security, and legal issues.\\n\\n3. [Deploying LLMs in Production: The Anatomy of LLM Applications](https://www.linkedin.com/pulse/deploying-llms-production-anatomy-llm-applications-xenonstack) - This LinkedIn article explores the deployment of LLM applications in production and discusses the components and reference architecture of LLM applications.\\n\\n4. [Large Language Model (LLM) Deployment](https://arize.com/blog-course/large-language-model-llm-deployment/) - This blog post from Arize AI covers best practices for deploying LLMs in production, including designing a robust LLM application service, maintaining cost-efficiency, and ensuring rapid iteration.\\n\\n5. [Learn How to Build and Deploy Tool-Using LLM Agents Using AWS SageMaker JumpStart Foundation Models](https://aws.amazon.com/blogs/machine-learning/learn-how-to-build-and-deploy-tool-using-llm-agents-using-aws-sagemaker-jumpstart-foundation-models/) - This AWS blog post provides a tutorial on building and deploying LLM agents using AWS SageMaker JumpStart and AWS Lambda.\\n\\nPlease note that some of the links may require a subscription or access to specific platforms.'}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\": \"What are the current best practices to deploy LLM agents to production?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We encounter a few issues when we try to just add the scrape function to the agent without any other consideration. \n",
    "How about we make this better by improving the inner chain of the agent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oreilly-langchain",
   "language": "python",
   "name": "oreilly-langchain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
